author,content,date,publication,summary,title,url
Karl Bode,"Google has found itself under fire for plans to limit the effectiveness of popular ad blocking extensions in Chrome. While Google says the changes are necessary to protect the “user experience” and improve extension security, developers and consumer advocates say the company’s real motive is money and control.  As it stands, the Chrome web store currently offers users a wide variety of ad blocking extensions that can help curtail the volume and nosiness of online advertising. From Adblock to Ghostery , such extensions make it harder for ad networks to build a detailed profile of your online activities or serve you behavioral ads based on your daily browsing habits.  Last year, Google began hinting at some changes to Chrome’s extension system as part of its Manifest V3 proposal . Under these changes, Google said it would be modifying permissions and other key aspects of Chrome’s extensions system. The extension development community didn’t respond well, and said the changes would harm many popular user tools.  Currently, many Chrome adblock extensions use Chrome's webRequest API, letting users block ads before they even reach the browser. But Google’s proposal would require extensions use the declarativeNetRequest API , which leaves it to the browser to decide what gets blocked based on a list of predetermined rules. While some extensions, like AdBlock, already use the latter, developers say the overall result will be tools that simply don’t work quite as well overall .  In the wake of ongoing backlash to the proposal, Chrome software security engineer Chris Palmer took to Twitter this week to claim the move was intended to help improve the end-user browsing experience, and paid enterprise users would be exempt from the changes.   Chrome security leader Justin Schuh also said the changes were driven by privacy and security concerns.   Adblock developers, however, aren’t buying it.  uBlock Origin developer Raymond Hill, for example, argued this week that if user experience was the goal, there were other solutions that wouldn’t hamstring existing extensions.  “Web pages load slow because of bloat, not because of the blocking ability of the webRequest API—at least for well crafted extensions,” Hill said.  Hill said that Google’s motivation here had little to do with the end user experience, and far more to do with protecting advertising revenues from the rising popularity of adblock extensions.   “In order for Google Chrome to reach its current user base, it had to support content blockers—these are the top most popular extensions for any browser,” he said. “Google strategy has been to find the optimal point between the two goals of growing the user base of Google Chrome and preventing content blockers from harming its business.”  Hill argues that the blocking ability of the webRequest API caused Google to yield some control of content blocking to third-party developers. Now that Chrome’s market share is greater, the company’s in a better position to “shift the optimal point between the two goals which benefits Google's primary business,” Hill said.  Consumer advocates are similarly unimpressed, noting that the changes could also harm the effectiveness of some parental control, privacy, and security extensions.  “This is a very bad decision on Google's part,” Justin Brookman, Director of Consumer Privacy and Technology Policy at Consumer Reports told Motherboard in an email.  Brookman noted that millions of users rely on extensions like uBlock, Disconnect, and Ghostery to limit cross-site tracking and block malicious code from third-party servers, and that pushing these extensions to use a different API with lesser functionality would only weaken them.  “It's hard to escape the suspicion that this is driven primarily by a desire to protect third-party tracking and ad revenue, where Google is the overwhelming market leader,” he said. “Notably, the move will insulate the largest ad blocker AdBlockPlus, who Google pays to whitelist their ads and tracking behavior.”  That concern has long been mirrored by the Electronic Frontier Foundation. The group frequently argues that Chrome’s ad tracker blocking technology has lagged behind other browsers because Google, whose online ad market share currently hovers around 37 percent , doesn’t want to hamstring the profitability of tracker-driven, behaviorally-targeted ads.  The EFF’s Privacy Badger extension is one of the ad blocking tools that would be impacted, and its development team has also spoken out against the changes. In an email, an EFF spokesperson argued that Google’s move would stifle developer innovation in the browser space and hamper user security and privacy.  The group also wasn’t particularly sold on Google’s justification for the move.  “Google's claim that these new limitations are needed to improve performance is at odds with the state of the internet,” the organization said. “Sites today are bloated with trackers that consume data and slow down the user experience. Tracker blockers have improved the performance and user experience of many sites and the user experience. Why not let independent developers innovate where the Chrome team isn't?   The EFF says it was “particularly worrisome” that Google is going ahead with these changes despite all of the criticisms it's received from the developer community, adding that “security extensions should not be a privilege reserved only for enterprise users.”  While Google has responded to criticism by saying the proposal was subject to change, it hasn’t yet backed off the proposal, which would be implemented this fall at the earliest. Should Google stick to its guns in the face of widespread criticism, it’s pretty clear that more than a few Chrome users will soon be on the market for a different browser.",,Vice,Google says the changes will improve performance and security. Ad block developers and consumer advocates say Google is simply protecting its ad dominance. ,Google Struggles to Justify Why It's Restricting Ad Blockers in Chrome,https://www.vice.com/en_us/article/evy53j/google-struggles-to-justify-making-chrome-ad-blockers-worse
Michael Byrne,"Despite the ever-increasing capabilities of web pages most of us probably still tend to think of them as things that live within browsers and that do typical old-school website stuff, much of which consists of just sitting there displaying some combination of links, text, and images. The growth and development of JavaScript and its myriad extensions has, however, heralded in a chaotic new age—the era of the web application. A URL now might reveal what amounts to a fully realized computer program. A canonical example would be Google Maps. As web apps have progressed to this point, developers have been trying to break web applications loose from the web. JavaScript applications are able to ditch the browser thanks to the Node.js platform, a runtime environment that allows JavaScript to run within a normal computer operating system rather than merely a browser. It's commonly used to build JS-based web servers in place of PHP. Read more: In Defense of JavaScript (the Hard Way) Nativefier is based on the Node.js runtime, but it goes a few steps further, allowing for quick and automated conversions from regular old web sites to stand-alone desktop applications. You could right now make this site into a piece of software. In fact, I just did. Nativefier does this thanks to a framework called Electron , which enables programmers to write desktop applications in JavaScript, HTML, and CSS. Nativefier first automatically creates a template of the target website and feeds this into Electron, which then ""wraps"" the template with all of the required stuff to make an .exe or .app file, using a browser window to render the HTML. Nativefier is plenty user-friendly: it runs from the Node.js command line simply as ""nativefier 'somewebsite.com'."" (There are some additional options that can be specified, but that's all explained in Nativefier's brief documentation.) World, meet PornHub.exe.",,Vice,Including this one.,Nativefier Will Instantly Convert Any Web Page Into a Desktop Program,https://www.vice.com/en_us/article/53dby3/nativefier-will-instantly-convert-any-web-page-into-a-desktop-program
 ,"In this version of Snake , the playing field is the game’s code itself, made with some very creative programming.  That’s why the game, made by a programmer named Taylor Conor, is called Quinesnake. A quine is a program that functionally accepts no input and does nothing but print a copy of its own code; in computer science this is often called a “self-replicating program.” The source code of the program is represented as data within the program itself and printed at execution. Quines, then, are normally not terribly interesting.  You may be asking, then, if a quine is functionally useless and accepts no input, how can quinesnake be a quine? The program does allow you to play a game of snake, after all. The truth is that Conor’s program is using a bit of trickery here. He explains that quinesnake “runs a game loop to accept keyboard control input, and highlights parts of the text as it continuously prints it to render the snake and the food.”  Think of the code generating the snake game as a program within the quine — it’s an event loop that watches for input from your keyboard. So the quine runs to print its code, including that of the embedded snake game, and then stops doing anything. The quine function itself takes no input after execution. Trippy, right?  If you want to play it, the game’s project files are available on Github . The Github page will explain how to run it, and you use the WASD controls to play.",,Vice,This Game of 'Snake' Takes Place Inside its Own Source Code,Games,https://www.vice.com/en_us/article/kz4azm/this-game-of-snake-takes-place-inside-its-own-source-code
Michael Byrne,"From outside the programming world, programming languages mostly just look like programming languages—a garble of mathematical symbols dotted here and there with isolated English words. Any structure or organization to the code mostly lies within the meanings of these symbols and words within a particular programming language or family of programming languages. A non-programmer is likely to just see a mess, whether they're looking at Javascript, Java, or Brainfuck . If the esoteric programming language (esolang) Asciidots looks like a mess, it is at least a very different-looking and even aesthetically pleasing mess. Simply, its mechanics and syntax are based on Ascii art. Before going any deeper, let's quickly look at an example Asciidots program. What the program above does is pretty simple. It takes a variable and increments it by 1 every time the code executes. So, if we started with a variable x with a value of 0 and then ran the program three times we'd wind up with an x that's equal to 3. How it does this is probably not obvious. Asciidots is a unique sort of programming language known as a dataflow language. In this sort of language, we can imagine units of data (like our variable x ) following a data go-kart track that's interrupted in different places with pit stops that change the value of the data go-kart that's following the track around. One pit stop might add 1 to the variable, while another might chop it in half. At some points, the track might even split, with the data go-kart picking one fork depending on its current value. If, say, it's greater than 2 it might go left; otherwise, it goes right. Other examples of dataflow programming languages include LabVIEW, Pure Data, Max/MSP, Verilog , and Simulink. They're often used to simulate electronic hardware systems. Pure Data and Max/MSP are generally used for music composition and performance. In Asciidots, the aforementioned go-kart track is represented by lines (|,-,/,\). The program below just directs a unit of data from start to end without actually doing anything to it. Most of the other non-line symbols are mathematical operators, but there are also symbols that direct the program to request input from the user, set values, print values, and change the direction of the unit of data. That unit is itself represented by a single dot. So, the dots in a program represent the entry points for new units of data. Let's look at another program. So, here we see two dots entering the track. Each one is set to a value using a hash symbol followed by the desired value. The two data units then meet at a minus symbol, which does its work and then spits out a result, which then travels to the dollar sign at the top. Its meaning is to print any values that it receives. Here's a whole game coded in Asciidots. Remember that the dollar sign means to print whatever values it receives, which, in this case, are instructions to the user. Under the hood, Asciidots is a Python program. An Asciidots program is just fed into that underlying program and digested into normal Python code, which is then executed. A lot of languages are like that, including Python itself, which is usually translated into C code. I kinda doubt Asciidots will transcend its ""esolang"" status, but esolangs often have the benefit of forcing programmers to look at their work from a new perspective. I think Asciidots at least accomplishes that. And, of course, it looks really cool.",,Vice,"Beats Java, at least.",Asciidots Is the Coolest-Looking Programming Language,https://www.vice.com/en_us/article/a33dvb/asciidots-is-the-coolest-looking-programming-language
Michael Byrne,"Confession? I don't write green code. I mean, it might be green code just by coincidence, but I've never really thought too much about the relative energy consumption demanded by this design pattern or algorithm versus some other. Sadly, this is true even when I'm working with actual hardware and low-level software, such as that written in plain C for embedded devices (in my case, for an Arduino board or other microcontroller platform). What's more, I don't think the green code idea has ever come up in my years of computer science classes. I'm hardly the exception, according to a paper presented this week at the 38th International Conference on Software Engineering. In interviews and surveys conducted with 464 software engineers from a range of disciplines—including mobile, data center, embedded, and traditional software development—researchers found that where green coding most matters, its practice is rare. ""Despite its increasing popularity as a research topic, little is known about practitioners' perspectives on green software engineering,"" the paper notes. ""Even basic questions such as 'What types of software commonly have requirements about energy usage?', 'How does the importance of reducing energy usage compare to other requirements?', and 'How do developers find and correct energy usage issues?' do not have clear answers.""  Green software development is as it sounds. In their own words, the researchers behind the new paper, a team drawn from IBM, Google, Microsoft, and the University of Delaware, were looking specifically for answers relating to how software engineers think about battery life/energy usage when they write requirements, design, construct, test, and maintain their software. The researchers presumed that, of the different flavors of developer surveyed, those writing software relating to mobile devices, data centers, and embedded systems would be most likely to use green practices. These are, after all, the domains where energy use is most critical. ""Based on our interviews, we initially theorized that practitioners with experience in mobile ('battery life is very important, especially in mobile devices'), data center ('any watt that we can save is either a watt we don't have to pay for, or it's a watt that we can send to another server'), and embedded ('maximum power usage is limited so energy has a big influence on not only hardware but also software') would more often have requirements or goals about energy usage than traditional practitioners ('we always have access to power, so energy isn't the highest priority').""  This turned out to be accurate for only mobile developers, who used green practices more than any other group, with 53 percent reporting that they ""almost always"" or ""often"" wrote applications with energy usage requirements. For embedded and data center developers, 75 percent and 86 percent of those surveyed rarely or never programmed according to energy usage requirements, respectively. As to why this is, the researchers were able to get some insight from more qualitative interviews with study participants. The reasons given aren't actually all that surprising. In the words of one data center program manager: Our main concern is marketshare and that means user experience is a priority. We can be more efficient to try to cut costs, but since we don't charge by energy used this doesn't make us more attractive to users. So we tend to focus on other things like performance or reliability. Embedded systems developers, meanwhile, offered several reasons for disregarding energy usage. One of these is that, while embedded systems usually involve programming at hardware levels, the systems involved very often have access to non-battery power. Another reason given was that developers often rely on hardware and not software to provide energy efficiency. And finally: ""Ensuring the deterministic, real-time behaviour of our embedded device is more important than saving energy."" The paper has several suggestions as to how to generally improve this not-so-green development climate. One example comes in the subfield of software testing and debugging, where developers would like tools that can help detect energy issues and to determine if the amount of energy being consumed to complete a process is reasonable for the amount of work being done. Generally, the developers interviewed were at least interested in green engineering practices and said they would use whatever help they're offered (debugging tools, example code, documentation, etc.). So, it's less a situation of willful neglect than it is relying on business-as-usual defaults, which is fortunately something a whole lot easier to correct.",,Vice,"But they're at least into the idea, according to a new survey.",Programmers Aren't Writing Green Code Where It's Most Needed,https://www.vice.com/en_us/article/z433w5/programmers-arent-writing-green-code-where-its-most-needed
Caroline Haskins,"Jonathan Cornelissen, co-founder of data science and coding learning platform DataCamp, is taking an ""indefinite leave of absence without pay"" as Chief Executive Officer of the company after allegations of sexual misconduct prompted DataCamp teachers to boycott their own classes , according to an apology Cornelissen published Thursday and a blog published Wednesday by DataCamp.  In his apology, Cornelissen said that DataCamp would be establishing an Instructor Advisory Board to “determine the best path forward for the company,” and that a independent third party would be auditing the company’s “environment and culture, including its response to the incident.” According to DataCamp's blog, any possibility of Cornelissen returning to the company ""will be based on the findings of the independent third party review.""  As reported by Motherboard earlier this month, multiple DataCamp teachers urged students to boycott their courses in response to the company failing to respond to allegations of sexual misconduct against one of its executives. The instructors could not take down their online courses, on account of their contracts.  Despite its disciplinary action toward Cornelissen, DataCamp’s delayed response to Cornelissen’s behavior remain a concern for some academics. On April 18, data scientist Nate George, tweeted that he was developing a course for DataCamp, but was ceasing development “until the leadership makes a big change in their behavior.” Today, he asked, “Will it be enough?”   The alleged incident, according to a blog published by DataCamp’s Leadership Team on April 4, took place in October 2017. The blog says, “at an informal employee gathering at a bar after a week-long company offsite, one of DataCamp’s executives danced inappropriately and made uninvited physical contact with another employee while on the dance floor.”  The blog—which was in response to an open letter published by DataCamp instructors on April 3, condemning the “untenable situation” of sexual misconduct within the company—did not outline any further disciplinary actions for the then-unnamed executive. Rather, it said that he had to undergo “sensitivity training,” and “personal coaching,” and received a “strong warning” about sexual misconduct.  In his apology letter, Cornelissen apologized for the lack of transparency regarding his alleged sexual misconduct. “I have failed you twice,” he wrote. “First in my behavior and second in my failure to speak clearly and unequivocally to you in a timely manner.” Correction April 26: A previous version of this article said that Cornelissen resigned, and the article was changed to reflect the fact that he is taking an indefinite leave of absence without pay.",,Vice,DataCamp co-founder and CEO Jonathan Cornelissen stepped down after sexual misconduct allegations promoted teachers to boycott their own classes.,DataCamp CEO Steps Down After Sexual Misconduct Allegations Prompt Backlash,https://www.vice.com/en_us/article/8xzn3v/datacamp-ceo-steps-down-after-sexual-misconduct-allegations-prompt-backlash
Matthew Gault,"Video game graphics are so good these days it’s hard to imagine they could improve. Can anything look better than raytraced lighting generating the reflection of a house fire in a puddle of water in Battlefield V ? Can visuals get better than a shaft of light illuminating dust particles in an abandoned warehouse in Metro Exodus ? Those games are beautiful, yes, but none of them have dynamic fracture animation for bread.  Just look at this CGI bread:   I wanted to eat that computer generated bread so bad that I had to make some toast before sitting down to write this. The tech behind that bread is CD-MPM, or continuum damage material point methods for dynamic fracture animation. It’s a new method of animating dynamic fracture and it’s the product of hard work from a team out of the University of Pennsylvania .  The team developed CD-MPM as a whole new way to model fractures of every kind—everything from breaking bread to wrecking a car. “Dynamic fracture is ubiquitous to everyday life, but the mechanics underlying these intricate material changes are anything but simple,” they said in their research paper.  That's cool... but check out at how good they made this cookie look!   The team will present its research on fancy fracturing at SIGGRAPH 2019— an annual computer graphics conference in California . This year, several of the presentations are using food to demonstrate their tech. Which looks delicious. An adaptive variational finite difference framework for efficient symmetric octree viscosity (this white paper is not yet online) allows programmers to realistically melt chocolate bunnies. An implicit material point method for simulating non-equilibrated viscoelastic and elastoplastic solids sounds complicated but looks amazing when you see it simulating melted ice cream spreading across a bowl of cherries . These are amazingly complicated computer science papers that are often hard for the layperson to understand but the result is incredible looking CGI.  It all looks delicious and it all points to an era of game changing video game food.   Ignis whipped up some amazing dishes in Final Fantasy XV . The palicos grilled feasts fit for a warrior in Monster Hunter World . All that food looked incredible, but you could always tell it was fake, a computer simulation. SIGGRAPH 2019 and the researchers presenting there hint at a future where the simulated food will soon be indistinguishable from what we can generate with computers.  We are not ready.  Also, they can use the same method to realistically tear the limbs off of a gel armadillo.",,Vice,"Yes, I will absolutely get this video game bread.",We Are Not Prepared for the Next Generation of CGI Food,https://www.vice.com/en_us/article/gy4yw7/we-are-not-prepared-for-the-next-generation-of-cgi-food
Karl Bode,"Adobe is warning some owners of its Creative Cloud software applications that they’re no longer allowed to use older versions of the software. It’s yet another example of how in the modern era, you increasingly don’t actually own the things you’ve spent your hard-earned money on.  Adobe this week began sending some users of its Lightroom Classic, Photoshop, Premiere, Animate, and Media Director programs a letter warning them that they were no longer legally authorized to use the software they may have thought they owned.  “We have recently discontinued certain older versions of Creative Cloud applications and and a result, under the terms of our agreement, you are no longer licensed to use them,” Adobe said in the email. “Please be aware that should you continue to use the discontinued version(s), you may be at risk of potential claims of infringement by third parties.”  Users were less than enthusiastic about the sudden restrictions.    The company didn’t inform users why they needed to discontinue use of the software, but the company’s Twitter account indicated the issue stems from “ongoing litigation.” AppleInsider , which first reported the notices, pointed to a copyright lawsuit filed last year by Dolby Labs.   In a controversial move, Adobe pivoted away from the standard software model to the cloud-based subscription model in 2013, resulting in notably higher revenues (and higher prices for customers). Dolby’s lawsuit accused Adobe of copyright violations related to how the licensing costs Adobe paid to Dolby would be calculated under this new model.  In a statement to Motherboard, Adobe confirmed the letter's authenticity, but wouldn’t provide any additional detail beyond what was included in the notices.  It’s yet another example of how the products we buy in the modern era can lose functionality or stop working entirely on a lark. Be it a game console that loses features with a firmware update or entertainment products that just suddenly disappear , it’s a problem that’s increasing popping up in the always online era.  Dylan Gilbert, a copyright expert with consumer group Public Knowledge, said in this instance users aren’t likely to have much in the way of legal recourse to the sudden shift.  “Unless Adobe has violated the terms of its licensing agreement by this sudden discontinuance of support for an earlier software version, which is unlikely, these impacted users have to just grin and bear it,” Gilbert said.  Gilbert noted that consumers now live in a world in which consumers almost never actually own anything that contains software. In this new reality, end users are forced to agree to “take it or leave it” end user license agreements (EULAs), in which the licensor can change its terms of service without notice.  “Even if Adobe is fully in the right here with regard to the Dolby dispute, it has the power to force its customers to upgrade to newer more expensive versions at its whim, which illustrates the undue power and influence of EULAs over the lives of consumers,” Gilbert said. “We should be able to own the things we buy.”  Activist, author, and copyright expert Cory Doctorow agreed, telling Motherboard in an email that this kind of thinking has increasingly permeated countless sectors, including DRM-based media, software as a service, and even client-server games.  Both Doctorow and Gilbert noted that this kind of shifting landscape can often be particularly problematic for artists and creators, who often don’t want to risk ongoing projects by suddenly jumping to new versions of software that may contain unforeseen bugs.  “When your tools are designed to treat you as a mere tenant, rather than an owner, you're subject to the whims, machinations, and unforeseeable risks of the landlord from whom you rent,” Doctorow noted. “And your legal rights are likely defined by a ‘contract’ that you clicked through a million years ago, which says that you agree that you don't have any legal rights.”  It’s a comical, lopsided arrangement that copyright experts say isn’t changing anytime soon, leaving consumers with only one real option: when possible, don’t buy products from companies with a history of pulling the carpet out from beneath your feet.",,Vice,"""You are no longer licensed to use the software,"" Adobe told them. ",Adobe Tells Users They Can Get Sued for Using Old Versions of Photoshop,https://www.vice.com/en_us/article/a3xk3p/adobe-tells-users-they-can-get-sued-for-using-old-versions-of-photoshop
Samantha Cole,"Using a new algorithm from Nvidia, you can face-swap your pet’s sweet mug onto other breeds to create monstrous versions of themselves.  PetSwap is a project that uses Few-shot UNsupervised Image-to-image Translation, or FUNIT—an image translation AI algorithm. FUNIT uses a framework built on generative adversarial networks, or GANs, the same framework that Nvidia has used to generate people who don’t exist and train autonomous vehicles on driving conditions that don’t exist. It’s also similar to how deepfakes are made , which were first inspired by Nvidia’s GANs algorithms.  “Humans are remarkably good at generalization,” the researchers wrote in their paper about PetSwap, which is available on the arXiv preprint server. “When given a picture of a previously unseen exotic animal, say, we can form a vivid mental picture of the same animal in a different pose, especially when we have encountered (images of) similar but different animals in that pose before.”   For example, our brains can easily imagine what a standing dog might look like when it’s lying down, based on what we’ve seen other animals do. But machine learning algorithms can’t. With FUNIT, researchers are trying to fill that imagination gap. Having an algorithm assume what something might look like based on other examples would allow it to use a smaller dataset of training images.   Nvidia made a demo available for the public to test out its algorithm, so I chose to faceswap my roommate’s cat Cookies, a creature of unknown breed and gender. By drawing a tight rectangle around Cookie’s cross-eyed face and hitting “translate,” the algorithm gave me several mutant animals in an attempt to match the cat face to new species’ bodies. The results are unsettling.   Having failed with Cookie’s perfect face, of course I had to try it on my own. The algorithm isn’t built to translate human faces, but it was worth a shot.  It looks like I’m midway through an Animorphs transformation. I’m a pretty cute mongoose, though.  Having an algorithm that could generate new images based on previous knowledge, rather than huge amounts of data, would make training a faster, less onerous task for researchers developing machine vision—such as self-driving cars, helper robots, other autonomous tech. But for now, the FUNIT algorithm is still just a fun way to deepfake your pet.",,Vice,"Using similar algorithms to deepfakes, Nvidia developed a new way to turn your pets into tigers, wolves, and other animals. ",Turn Your Pet Into Another Species With This AI Tool,https://www.vice.com/en_us/article/nea737/face-swap-deepfake-pets-into-another-species-ai-tool-nvidia
Samantha Cole,"In a recent YouTube video, Joe Rogan says he wants to form a chimp hockey team, claims that he’s trapped in a robot, and muses over life being a simulation. The voice isn’t actually the host of The Joe Rogan Experience , though. It’s really an AI-generated voice that sounds uncannily like the comedian and podcaster, Jersey accent and all.   According to the video description, machine learning engineers at AI startup Dessa created a computer-generated dupe of Rogan’s voice using audio of him speaking. Machine learning tools learn patterns—like the particulars of a person’s voice—in large amounts of data. This makes sense for Rogan, who releases several episodes of his podcast every week, each of which may run two hours long or more.  In a blog post about the work, the company writes that the engineers—Hashiam Kadhim, Joe Palermo, and Rayhane Mama—generated the speech with text inputs, and that this approach “would be capable of producing a replica of anyone’s voice, provided that sufficient data is available.”  A spokesperson for Dessa told me in an email that it’s trained on audio data from Rogan, and “what you are hearing is the result of a text only input.”  This is similar to how the algorithm that made Jordan Peterson rap like Eminem works—that approach required at least six hours of audio to train on. Programs like Lyrebird and Modulate.ai also need a little audio to train the algorithm, even if just a few minutes, to replicate a real voice accurately.  Even so, the quality of Dessa’s Rogan dupe stands out from the crowd. . In the announcement blog, the Dessa researchers say they won’t release details about how the algorithm works, including any models, source code or datasets, at this time. But they promise to post a “technical overview” within the coming days.  Since the advent of deepfakes —algorithmically-generated face swap videos—we’ve seen more startups trying to make their own fake personas while simultaneously emphasizing the societal implications that these fakes could have. Instead of releasing the details of the project, Dessa outlined some of the consequences of ultra-realistic audio, including scammers and spam calls, harassment, and impersonating a politician. On the plus side, accurate fakes could improve accessibility tech and language translation, they wrote.  In case you think you’re not easily duped by fakes, the researchers set up a quiz to test how well you can discern the real Rogan from fake Rogan. It’s shocking how close they match, the only differences being slight inflections in the voices. I guessed correctly on seven out of eight of the examples, and the one I flubbed will haunt me.",,Vice,Startup Dessa created a Joe Rogan voice that sounds very much like the real thing. ,This AI-Generated Joe Rogan Voice Sounds So Real It's Scary,https://www.vice.com/en_us/article/597yba/ai-generated-fake-joe-rogan-voice-dessa
Michael Byrne,"IRC in 2016 seems so innocent. Internet relay chat (IRC) is a fundamentally primitive technology, a precursor to social networks, and, especially, chat services like Slack and Hipchat. Slack, in particular, has been quietly decimating IRC as mass chats and chat-supported collaborations make the jump from the relative anarchy of IRC to what is perhaps a new group-chat standard (and is based on a fancy newish network protocol called WebSockets ). A quick poll of open-source projects (the first 10 or so that came to mind) finds that Slack is just about ubiquitous where public chats are in use by contributors. The heart of an IRC channel is a server. On this server exists a daemon, which is basically a background computer program that waits around for something in particular to happen. Here, the daemon waits for incoming network connections on certain designated ports. For every incoming IRC connection, the IRC server maintains a TCP connection with some client, who can read messages that live on the server and also add messages. There are also commands that can be entered by clients for things like requesting channel invites or finding users by name. IRC networks expand by adding additional servers, and, with them, clients. Every server in an IRC network shares the same global state. So, a client can post a message to one server, and that server will make sure it's mirrored to the other servers. Freenode, the largest IRC network, employs a couple dozen servers, which then serve some 90,000 to 100,000 averages users across 35,000 to 45,000 channels. Here's the basic architecture: IRC is an application layer protocol. This layer is the top of the network technology stack and it's where regular end-users interface with networks. Other protocols at this level include HTTP (where webpage-formatted content is transferred), SMPT (email), FTP (file transfer), SSH (remote command shells), and DNS (domain name resolution). These are all just message formats corresponding to different network-application functions. The format for IRC, which was first formalized by the IEFT in 1993 after several years of increasing popularity, is lightweight: message target (""to:""), IRC channel, IRC server, and then the message itself. IRC is basically text messaging over the internet, but usually featuring multiple recipients. The maximum size of an IRC message is 512 characters. This has to include not just the message to be displayed to other users, but the meta information as well, including the nick (client nickname) along with any commands and command parameters. The actual message payload, the thing to be communicated to other users (read: content), is prefaced by a command (PRIVMSG) and is transported in the form of a parameter to that command. (Note that many of the IETF specs for IRC are routinely violated by various IRC networks; protocols are really just things that clients and servers agree upon.) 0.0) The first chat At its 1988 inception, IRC was intended to be an extension of a BBS (bulletin board system) at the University of Finland called OuluBox. Its creator, Jarkko Oikarinen (the ""WiZ""), basically wanted a real-time BBS, featuring a real-time Usenet-style discussion board service. The chat feature of Oikarinen's future-BBS was the first thing to be implemented, which is a good thing because he would soon have to nuke the whole real-time BBS idea. It was too unrealistic. Still, IRC was in place and so BBS users could at least chat in real-time. At first, this was only among universities in Finland, but soon enough servers at the University of Denver and Oregon State University were connected as well. By the end of 1988, IRC had spread across the internet at-large and encompassed 40 servers, according to an IRC history by cURL creator and IETF member Daniel Stenberg. Over the next several years, IRC would split several times, resulting in not just a single IRC but several, including Anarchy net, EFnet, IRCnet, Dalnet, and others. In 1998, Freenode launched as the Open Projects Net. It was to be the IRC network supporting the open-source movement and it remains as such. 1.0) Pick a client A chat client is how a user and-or administrator interacts with IRC. There are a whole lot of them. I picked CIRC, which is a Chrome app. LimeChat (Mac) and HexChat (Windows) both come recommended as free standalone clients. 1.1) Pick a nick Open up your client and set your nickname (""nick"") like this: /nick  /nick is the command, followed by whatever your nickname is going to be. 2.0) Pick a network Freenode is generally supposed to be for ""on-topic"" chatting—that is, chatting about open-source projects. There's a whole lot of flexibility in there, but for now let's set up our channel at IRCnet, which is about as OG as IRC gets. In our chat client, all we need to do is issue the /server command along with the hostname of the server. Like this: /server irc.us.ircnet.net The hostname is one of several US IRCnet servers in the United States. It's the first one I found when Googling. If your nick is already taken on the server, it will be automatically tweaked. When I tried to connect with ""mike,"" the server changed it to ""_mike"". I changed my nick to ""byrneio"" and found no conflict. 3.0) Make your channel Now for the big anticlimax. Let's make a channel. We do this with the /join command, like so: \join  My channel is #byrneio. That's it. Really. 3.1) Guard your channel Part of what makes the IRCnet network interesting is that it doesn't support registration. Nicks and channels are first-come first-served. How does one actually administer a channel then? IRCnet offers this guidance : The short answer - with experience, resources, and patience. In other words, the old fashioned way. As a channel op, it is your job to run your channel. Generally I advise that you do not try to run a channel unless you already have at least 5 and preferably 10 trusted friends with 24/7 connections to give you a ""critical mass"". Then you guys can pass ops back and forth and not lose ops. So, you basically need to keep your channel active so that no one can drop in and grab it. It's possible to script IRC commands, so that would certainly help, though it's a bit beyond the scope of this tutorial. 4.0) The void If your new channel is feeling a bit lonely (as #byrneio is), type in the command /list . This will list every active IRCnet channel on the current server. Go out and make some new friends. More Hack This : Hack This: Programming with the Twitter Firehose  Hack This: Become a Command-Line Assassin  Hack This: Extract Image Metadata with Python",,Vice,Old-school chat still thrives.,Hack This: How to Start an IRC Channel,https://www.vice.com/en_us/article/3daj4n/hack-this-start-an-irc-chat
 ,"Here's an interesting notion: bands and tech startups are essentially the same thing—entrepreneurs. Or at least that's how Vinitha Watson and Anna Acquistapace see it with their Zoolabs space, a new creative incubator designed to nurture music and tech startups.  Based in Oakland, the two met at the California College of the Arts Design Strategy MBA program. Watson, a former Google employee, caught the startup buzz while in India building a Google satellite office, and decided she wanted to transport culture and build an organization. Acquistapace, who formerly produced films in Paris, and worked with artists in gallery settings, became interested in leveraging her design consultancy background into community building in a dedicated space. Thus, Zoo Labs was born, which also includes Didier Sylvain and Dario Slavazza.  ""We wanted to incubate a lot of artists, but we were coming up against a lot of unsuccessful models of giving support or understanding why things weren't being disseminated into the world,"" said Watson. ""It got me thinking about what artists need to bring their work out into the world.""  Watson and Acquistapace recently took some time to talk about the challenges of fusing music and tech in a single startup incubator. As music industry outsiders, they see a simple solution to the problem: like tech startups, musicians simply need the right environment and tools to better push their ideas out into the world. While Watson and Acquistapace are encouraged by the early efforts at Zoo Labs, only time will tell if the tech startup approach, where coding experience and better social media strategies, provides a real, measurable impact on bands' fortunes.  MOTHERBOARD: What's interesting about music startups is the pro-active approach to solving the large-scale problem plaguing the music industry; which is that the revenue model was forever ruined by the internet and piracy. It's really a defeatist attitude. It's great when music startups try to do something. Was that part of the motivation?  Watson: I really haven't encountered that defeatist attitude. We come from a design strategy background, where it's all about figuring out what the problem is, tinkering with it, and finding a solution that really affects the behavior chain and products that are out there and meaningful. When we started this project, Anna was interviewing people, and one of the greatest outcomes of her research is that musicians are entrepreneurs. That kind of helped frame things for us.  Musicians are really resilient. They find ways to continue their passion, or find ways to make things work. We were really curious about those aspects of what people are doing. When we got into it, we thought that maybe they just needed a little reframing and strategic tools, as well as a place to record. That's kind of how the Zoo Labs residency was born. And we have technology startups in our space, and they're not dissimilar. They just talk in different languages.   The Boston Boys at Zoo Labs first residency.  Acquistapace: For us, being in the Bay Area, which has an enormous entrepreneurial population, one of the really unique things that we offer is this perspective of entrepreneurship. In the 21st century, part of being a musician is building a company. They're using the same tools, like trying to get traction on social media. They're trying to pitch their ideas to record labels the same way that entrepreneurs are pitching their ideas. It's an interesting overlap.  In the Bay Area, do you feel that people in bands are actually making a living as tech developers?  Watson: I think it extends way beyond the Bay Area. I think that music is a dynamic way to think, and it uses more of the brain than most things, and so does programming. There is quite a crossover. One thing that our other programmer says a lot is that when he was programming he was being creative in a musical way.  Acquistapace: I think that musicians are dealing with some of the same things that startup entrepreneurs are dealing with. Because we're serving both of those communities, we've had these amazing overlaps where engineers, who have been coding until 3:00 a.m., will come out and run into musicians coming out of the recording studio. We've heard how nice it is to have people in the space at that hour. They're both passionate about their projects, and we're finding these experiences really interesting.  Any other notable experiences?  Acquistapace: We started a programming class here where we're teaching the community the Python programming language. It was really great to hear Chief Xcel from Blackalicious say, “Yeah, this is like learning an instrument.” These are really powerful crossovers that we are finding.  What projects have come out of the startup lab?  Acquistapace: We've had five companies in our startup lab so far. It's an early stage co-working space. We attract music tech companies because we have the recording studio. Musikara came through the startup lab, and they're working on a very technical music tech problem, which is collaborating online. There is a latency problem when you're trying to play music together across the internet. So, these are people interested in using software to provide tools to the music world.   Watson: They did this crazy prototype where they had a concert between Palo Alto, Oakland, and New York. You would think that it would sound a little weird, or you wouldn't feel like you were attending a concert. I closed my eyes during that prototype, and my hair rose on my arm. You were in the middle of it, but you know that these people were far away. That was so incredible. What they're doing is pretty cool.  That's an interesting problem to be tackling. What are other startups doing at Zoo Labs?  Acquistapace: We had a company in here called Mosey . They're a startup tech company, and they're in the traveling tourism sector. They've built a platform that allows you to make custom itineraries for friends in your city. So, it's like, “Your Perfect Four Hours in San Francisco.” They were in the startup lab for about nine months, and for them it was really great to be in a space that felt a little different with a different community. They loved crossing paths with musicians. For them, it added a cultural relevancy that was different and exciting.  You launched a Beat Lab in February. Can you talk about that?  Acquistapace: The Beat Lab is what we like to call a co-working space for music producers, which as far as we know doesn't have an existing model. We came up with the idea of making a custom-built music production station. It allows music producers to bring their computer and plug it in. It also has speakers, a patch bay, all of the cords and stuff, so that they can work on their beats and music at the Zoo in the live room of Studio A, which is the nicest, biggest studio. It allows them to have a monthly membership and come in and work on projects.  The impetus for this was that music producers are craving a community. They usually work in isolation. This gives them the opportunity to come into a nice space with a nice common area and a larger community, and work. They can record vocals, a grand piano, drums, and work on cutting and putting their songs together. For us, supporting producers also means we're supporting musicians and different musical projects.   Watson: For the producers, it's pretty cool because they have access to microphones that they might not have access to unless they book time in a professional recording studio. They have access to synthesizers. We're doing this at an affordable cost. We're really excited about it, but we'll see how it goes.  Zoo Labs also has a music residency. What is it?  Watson: The music residency is pretty cool. It's a two-week program, and we ask that teams of three to five apply. They can be a producer, a singer, a graphic designer, and a manager or something. We ask for strong teams to apply online. We then have a judging process that goes through on the application, and then we pick the top contender. We have criteria that we look for like group dynamic, creative depth, and if they're really well-rounded, etc.  With startups, it's really about the founders and the team dynamic that can either push a project to be a great product or crumble and not make it to the world. Those things are really important to us, so we do a pretty comprehensive screening of our residents. We then get them to West Oakland, and we then give them two weeks in the studio to record their latest project. On top of that we layer strategic business classes. At the end of the two weeks we ask for a minimum of three songs and a strategic plan of how they're going to disseminate themselves and their product out into the world.  What happens at the end of the residency?  Watson: At the end of that, they have something called a Release Day where they perform, and we have a private listening project, where people come in and check out what the group created in those two weeks. In the middle of that there are a lot of discussions of them as a business—what are they doing as a brand and how are they getting their product out? So, we're giving them space and time like a traditional art residency, but also an accelerator and business school. We don't give them any answers, but we give them tools. We're looking for innovators and those who want to work outside the traditional model.  Acquistapace: Right now that's the ultimate expression of our mission to bridge the creative world of music with the strategic business side of things. We're starting to talk about the entrepreneur class of musicians, and I think it's something that we've found really resonates with people. Musicians really want to understand how to be more effective leaders of their own careers and projects.",,Vice,Zoo Labs Brings Bands and Tech Startups Together Under One Roof,This story is over 5 years old,https://www.vice.com/en_us/article/ezvx5p/zoo-labs-brings-bands-and-tech-startups-together-under-one-roof
 ,"by Tobias Leingruber .  HTML , or Hyper Text Markup Language, is the basic code structure that create websites, and it consists of elements called tags. For example “ ” defines a new paragraph or “ ” will show a picture. As HTML is steadily improved by a committee, the World Wide Web Consortium, or W3C, and as the web prepares for a linguistic upgrade to HTML5, some features are destined for the dustbin.  Among the newly obsolete, few features are as hard to lose as the one known as “”. Invented in the 1990s to allow web designers to mix-up several HTML pages, the frameset technology heavily influenced web design and net.art projects throughout that decade. (See the Motherboard TV episode about JODI , for instance.)  But despite its love by millions of amateurs and hot-linkers over the years, the frameset became a subject of serious fear by usability designers. This year, the HTML5 work-group decided to pull the plug and exclude the  from future specs. Let it rest in peace forever in quirks mode among its siblings, the and tag. (The frameset leaves behind one child, the .)  I knew the frameset personally; we were good friends back in the late ‘90s, when I designed my first HTML pages. I met it and was very impressed by it’s powers. I used it for my 1999 Star Wars fan page to hotlink/steal/incorporate a Queen Amidala gallery into my layout. The frameset with the pretty Comic-Sans buttons is still standing, but the Amidala fan page (under Star Wars) is long gone .  Dear HTML frameset,  Speaking for the ones who still remember you, from back when you were high-tech: we will very much miss you. And before you’re whisked away to the graveyard of “deprecated” html tags , let’s all remember you by looking at two beautiful art pieces that made history – with your help of course:  Olia Lialina, My Boyfriend came back from the war  Dragan Espenschied, GRAVITY  yours,  TBX  Read more: Jakob Nielsen – Why Frames Suck , 1996  Official HTML specs dropping the Frameset  Olia Lialina – An observation on frames  Jason Teaque – How-to program HTML frames  Jason Teaque – Frames are dead. Long live frames Inventor of blink tag, it was a Schnapps Idea  MCLI – Dont use marquee! Long live quirks mode!  This piece was originally published on March 30.",,Vice,"Requiem for the HTML Frameset, 1996-2011",This story is over 5 years old,https://www.vice.com/en_us/article/ezzq5z/requiem-for-the-html-frameset-1996-2011
 ,"Back in December, you probably made all sorts of promises to yourself about eating healthier, getting in shape or finally finishing that one project of yours in the coming year. Now it’s the end of January, and whether or not you’re having doubts about any of those commitments, here’s something that everyone — and I mean everyone — should be adding to their list: Learn how to code.  I’m not merely suggesting that programming is a ‘handy’ skill to have — it goes far beyond that. With digital applications becoming an inescapable framework for the choices we make every single day, knowing how to code is already starting to mean the difference between using software, and being used by it.  In his insightful survival handbook, Program or Be Programmed , Douglas Rushkoff reveals the many dangers of a read-only digital culture — from becoming enslaved to the various pings, cell phone vibrations and electronic notifications of an always-on, time-independent machine network to regimenting our lives around the discrete choices offered to us by drop-down menus and app marketplaces, the call to program has never been louder.   Remember, those discrete choices do not represent reality, but a simulation of reality. Furthermore, they are simulations offered by entities like Facebook, Google and others, meaning that when we use that software, we are surrendering to the models that they have created, and building our lives around them. As a result, our output increasingly becomes a reflection of the software and networks we’re wired to, trapped within a shrinking box of machine-defined options and labels.  In the information era, ubiquitous digital devices are carrying people out of the realm of traditional computing and into highly controlled, special-use software platforms. Author and critic Cory Doctorow has called it the first signs of a coming war on general purpose computing where these platforms, along with overreaching and flawed copyright law, will seek to turn computers and networking back into a read-only medium.  It doesn’t have to be this way. In a sense, knowing how to program is like having a voice where most others can only listen — and if this war comes to fruition, a powerful weapon. If more of us could code, the software and networks we are building would be a more accurate representation of all of us, not merely our reflection in the search engine / social media looking glass.  Computer programming looks daunting to the average person, but it’s based in a few very easy to understand concepts. After all, a program is just a set of instructions that then gets compiled (translated) in a way machines can read. Everything in between is learned by doing it yourself, and there are an increasing number of resources that can help you do just that.  Codeacademy is one notable example, offering a full hands-on intro to programming Javascript that they can even mail to you in bi-weekly segments if you like. There’s also Google Code University , Lifehacker’s beginner’s guide , an online course at Stanford … the list goes on and on, and they’re all free.  The important part is, even if you develop a basic understanding of how code is written this year, you’re giving yourself an important advantage in dealing with the networks and machines that dominate every facet of our reality. It’s ultimately what’s going to determine who will be helping build our future networks, and who will be hanging back in the bleachers.",,Vice,"Everyone Should Learn How to Code This Year, and Here's Why",This story is over 5 years old,https://www.vice.com/en_us/article/kbb34n/why-everyone-should-learn-how-to-code-this-year
by,"Though Raven Kwok ‘s academic focus was photography, he spent the past year venturing into new media art, where his true interest lies. His passion for exploring different forms of expression at the intersection of computer programming and visual arts resulted in a series of clever, playful animations, ranging in form from an adorable amoeba that follows your cursor around to self-evolving shapes, all made using Processing. He’s also found ways to distort and augment typography that appears almost like moving, generative graffiti. What inspired this style of gray scale, geometric movement? We spoke with Kwok to find out more about his process and the visual works that just we can’t stop staring at. Creators Project: What was your intention when you began using Processing? Raven Kwok : My creative intention was very simple. Because it seemed interesting and fun, I wanted to explore the visual impact that algorithms and code can bring. I don’t create just to convey ideas. That’s why my works are mainly titled with their completion dates. Recently, that way of titling became too long for me, so I turned the year, month, and date into a hexadecimal code, and used those as the titles instead. What’s the main difference between Processing and other software? What possibilities does it open up for you? Processing is an open source programming environment geared toward people without a programming background. It’s a software that you can use to make your own software. Unlike other design software, it doesn’t have numerous function interfaces, time axes, rulers, and motion curves. Upon opening the application, the largest surface is a text editor, and you can basically create anything you want in this area. Many artists often run into issues, saying, “I need an effect but my current software can’t realize it.” They might look for plug-ins or other software on the internet, but for a Proccessing user, if the software’s prefabricated modules fail to realize the results you want, then you just write your own. Your videos look like geometric shapes that morph into live organisms. Do you see these forms more as cold, man-made programs or self-generating art? From a creative point of view, I am very aware of the active structures of these lifelike results. Every level of program variation is set according to my logic. So, from its essence, it doesn’t have the free ability to think and learn, therefore it doesn’t actually fit the true meaning of being generative. Every example of computer-generated art is simulated/analog, including my own. A lot of this discussion relates to the works of Christopher Langton about artificial life. Of course, through the viewer’s own imagination, my work can be anything. For me, the programs I wrote are not cold, but they are not gimmicks either. Can you tell us a little about how music you’ve used to score your work? Even though I cite the artists, and I’m not using their songs for commercial purposes, I still feel a little bad because of copyright. I usually use electronic or ambient music, but they are not genres that influence my work. For an old school thrash metal music fan like myself, listening to metal while writing code provides the optimal mood.",,Vice,Entertainment,This story is over 5 years old,https://www.vice.com/en_us/article/ezaek7/raven-kwok-creates-morphing-shapes-through-processing
by,"If you’e trying to inspire people to learn programming, then using a pair of LED glasses with 174 programmable LED lights isn’t a bad way to go about it. That’s what Technology Will Save Us , a company who produce and educate people on how to produce technology, have come up with to teach people the ways of computer programming.  They’ve launched a Kickstarter page to help the project and the idea is for people to buy these DIY glasses, called Bright Eyes, which are easy to assemble and then start to play around with them depending on how adventurous they feel. There are three different entry levels to get people involved, from super easy, to writing code, to the more hardcore level where you can hook the glasses up to an Arduino to make them responsive to sound or light, produce graphics, or interact with your Twitter feed. [via PSFK ] @stewart23rd",,Vice,Design,This story is over 5 years old,https://www.vice.com/en_us/article/qkz9wx/learn-to-program-with-these-led-shades
by,"Hello world. Dust off your monocles, because today we plunge into academia—specifically, into the field of digital studies. The line between computer programming and art is already a blurred one: artists are crafting their own codes to generated computerized visuals (see: Ursula Damm and Martin Schneider's Transits video for a particularly haunting example), and computer programmers (like Aaron Koblin and Casey Reas ) are increasingly regarded as artists. So the question is not whether computer code can be used for art, but whether it is a form of art in itself? Certain academics argue that code is a form of speech—and comes with the political power that any other language holds. I polled a few of my programmer friends about whether they agreed with this, and most replied with cocked eyebrows and a firm ""no."" One said, if anything, it was more like legal language than anything else—based on pure functionality. ""If you release something with ambiguities, you've released a buggy code,"" said one, nonchalantly. Another said that maybe he would allow an analogy to Esperanto —but that was it. ""It lacks the capacity for detailed meaning, for conveying more than just information in a direct or literal sense,"" he said. I wanted to find out why so many academics believe that computer code leaks beyond the boundaries of scientific practicality, and can carry artistic or political weight. So, I decided to turn to Geoff Cox —a researcher in Digital Aesthetics at Aarhus University's Digital Urban Living Center . Cox's book, Speaking Code , came out last Friday through MIT Press, and in it, he applies critical theories from the humanities to the study of computing and software development. The Creators Project: Hey Geoff! Your book Speaking Code is coming out today. And by arguing that programming is a kind of ""text,"" you weave in ideas about politics, art, and language. What's the main idea your book contributes to the field? Geoff Cox: Just one? Hopefully there are a few, but maybe the last line suffices: ""The book proposes coding practices that have not only a body but also a body politic."" The cover of Geoff Cox’s new book, Speaking Code. I noticed you bring in a lot of critical theorists my English major friends creamed themselves over—especially Althusser and Judith Butler . Why do you think it's okay to appropriate ideas from such a different field of study? In my view, it is exactly this kind of interdisciplinary work that leads to new understandings—not seeking some kind of hard truth, but opening up new modes of thinking. I don't think there's a straightforward transfer from one discourse to another, especially from humanities to computing, but work to be done across and between. I wanted to explore the ways language goes out of control—what Butler refers to excitable states—and how this creates new kinds of subjectivities that are open to critique. Similarly, Althusser's interpellation describes the paradoxes of language: humans are constituted through language, but are also able to modify it. But there are many different kinds of code, all of them with their own sets of rules and contexts. How can they all be read as the same language? Human and machine languages are quite different things, but the analogy between speaking and coding is commonplace. It's useful to open up ideas about interpretability, action, and freedom of use—think about how the discourses about freedom of speech and free software intersect. As Henry Giroux puts it, who speaks, under what conditions and on behalf of whom? From Recycling the National Curriculum (1996), a research seminar, launch event, and performance at the School of Education, Middlesex University, on the subject of art, education, and blank paper with Geoff Cox, Victoria de Rijke, Howard Hollands, and Sophie Weeks. Image source . In your book, you say that one of your concerns is the way coding produces ambiguities outside of its deterministic tendencies. Can you provide an example of how this happens? Yes, this is one of the most important aspects of this book. Despite the way that code largely determines its outcome through commands, this isn't entire the case. This gives some political hope in a world where our actions and speech seem ever more predetermined and controlled. Can you give an example of this indeterminacy? Let's look at Feedback.pl, a text editor written by Alex McLean . The programmer edits the code, which runs live while it is modified. However, the running code is also able to edit its own source code, so that the code is able to make fundamental self-modifications. It demonstrates the complexity of how programs operate together with and without the programmer, both relaying instructions and acting upon them in an uncertain relation. Do you have something against practicality? I am trying to foreground artistic practices that break out of the functionalism of normal programming, specifically by looking at layers of communication. My book uses some examples of code specially written by Alex McLean, to express some ideas through the logic of code. One of these is a very simple demonstration of how comments distinguish the voice of the program from that of the programmer: # This is the voice of the programmer echo “This is the voice of the program” When you called code ""both a script and performance,"" that reminded me of the way Kittler wrote that code is a text with ""the extraordinary capacity of reading and writing itself."" I refer to Kittler's statement that program code is a very particular kind of writing. Other commentators, such as Florian Cramer and Katherine Hayles , have described program code as writing that breaks down the distinction between writing and tool. Take the Fluxus performance score by La Monte Young called Composition 1961 No. I, January I. The instruction was to ""draw a straight line and follow it."" It's the same idea of how notation and execution can be collapsed into one thing. More importantly, I am drawing on speech theory to suggest that like speech, code says and does something at the same time. Video from Geoff Cox’s  ""Generator"" exhibition (2002-2003) I want to go back to what you said about how free speech and free software are related. The book tries to further develop the analogy to free speech, to draw out some of its many paradoxes. The free software foundation clarifies their use of the term ""freedom"" by saying, ""free as in free speech not as in free beer."" But what about the problems with free speech or the liberal illusion of free choice? Last question: what do you think about all the hoopla that's been made recently about the discovery of computer codes in string theory? Do you accept this as proof that we live in virtual reality? I don't know anything about it, but I guess it can also be seen as another example of biopolitical power, and the ways that code has become embedded in our lived experience. I think close attention needs to be paid to code and the narratives around it (ideas like 'the computational universe' and so on) to understand the processes at a deep level. Be skeptical of the ways these ideas are naturalized. @MichelleLHOOQ",,Vice,Entertainment,This story is over 5 years old,https://www.vice.com/en_us/article/8qmjkx/can-computer-code-be-used-for-artistic-and-political-expression
Michael Byrne,"At the Heidelberg Laureate Forum this past Monday, Vint Cerf, the Google vice president and co-inventor of the internet itself, offered some worried remarks about his progeny's imminent Internet of Things frontier. ""Sometimes I'm terrified by it,"" he told a news briefing there, according to Network World . ""It's a combination of appliances and software, and I'm always nervous about software—software has bugs."" So, we should probably start paying attention: IoT security is a real threat. It hardly needs spelling out. Connecting a ""thing,"" whether it's a home thermostat or a nuclear power plant, means opening that thing to outside threats. The IoT offers new levels of exposure and we need to be thinking about this, like, yesterday. That is, if it's not too late already—back in April, a NIST analyst suggested that the IoT may even be fundamentally indefensible. Cerf went on to say that he even avoids Google's company-provided employee massage chairs: ""I know they're run by software—I worry they will fold up on me,"" he fretted, hopefully with sarcasm. ""Who is liable when an appliance doesn't work the way it should, and what if that's a software question?"" Sure. Network security is a deep and abiding concern with respect to the IoT, which is fair. But Cerf's concern about software suddenly running all of our appliances and other assorted Things comes off as a bit goofy. Most whatever you can possible imagine that uses electricity is already run by software and this is nothing new or unique to IoT matters. Embedded systems were around well before PCs, as were embedded software bugs. I could make an argument that, in terms of bugs, embedded software networked via the IoT might even offer more safety in that the software can be updated much more frequently. I would also note that software, of the sort that you download from some site and install on your laptop, is like an entirely different world from embedded software, which is debugged so deeply, aggressively, and exhaustively that it would blow the mind of your average scripter—speaking as someone that does it, anyhow. So, worrying about network security threats and the IoT? Of course. ""Software running things?"" Get over it.",,Vice,The internet co-founder has some concerns.,Google VP Vint Cerf Is (Sometimes) Terrified of the Internet of Things,https://www.vice.com/en_us/article/ae3bmp/google-vp-vint-cerf-is-sometimes-terrified-of-the-internet-of-things-2
Rachel Pick,"Artist-coder David Li has created an awesome browser-based simulation of fluid dynamics called ""Vortex Spheres . "" The video above shows it in action, but you can play around with it yourself on Li's website , controlling the flow of the simulated liquid by toggling different quantities for variation and radius. There's also a ""drop"" button so you can watch the column of liquid collapse. It's worth checking out the rest of Li's site to get a look at his other projects, including this cool wave simulator and a MIDI keyboard-driven smoke simulator .",,Vice,"Go ahead, give yourself a break.",Play Around With This Fluid Dynamics Simulator,https://www.vice.com/en_us/article/539xxk/play-around-with-this-fluid-dynamics-simulator
Jason Koebler,"Some of the paintings you see above were painted by some of the most renowned artists in human history. The others were made by an artificial intelligence. Robotic brains have a ways to go before they match the masters in terms of pure creativity, but it seems they've gotten quite good at mimicking and remixing what they see. In a study published late last week by researchers from the University of Tubingen in Germany, researchers described an artificial intelligence neural network capable of lifting the ""style"" of an image and using that style to copy another image, which is why you see these waterfront houses look as though they were painted by Picasso, van Gogh, or Munch. As you might expect, the math is quite complex, but the basic idea is pretty simple. As the researchers explain, computers are getting very good at image recognition and reproduction. The neural network basically does two jobs, then: One layer analyzes the content of an image, while another analyzes its texture, or style. These functions can also be split to work across two images. ""We can manipulate both representations independently to produce new, perceptually meaningful images,"" Leon Gatys, lead author of the report, wrote in a paper published prior to peer review in arXiv . ""While the global arrangement of the original photograph is preserved, the colors and local structures that compose the global scenery are provided by the artwork. Effectively, this renders the photograph in the style of the artwork, such that the appearance of the synthesized image resembles the work of art, even though it shows the same content as the photograph."" As I mentioned, this was published prior to peer review, but the paper is currently being considered for publication in Nature Communications . While waiting for review, Gatys told me he is barred from speaking to the media about specifics of how the neural network works. At the moment, the code isn't open source like Google's Deep Dream program is. That's a bit of a bummer, as people are champing at the bit to play with the neural net in an attempt to turn their Snapchats into algorithm-driven masterpieces. A couple people on Reddit have tried to get in the same ballpark as Gatys's team, without all that much success . Without speaking directly to Gatys and the team, it's hard to know what they're planning for the future of this thing, but the paper provides a few hints. Gatys suggests that it may be possible to use the algorithm to create new types of images that can be used to study how humans interpret and perceive images, by ""designing novel stimuli that introduce two independent, perceptually meaningful sources of variation: the appearance and the content of an image."" He wrote that these images could then be used to study visual perception, neural networks, and the computational nature of biological vision. Or, you know, maybe he'll just use it to create sweet art that'll sell for millions alongside the classics.",,Vice,"A new neural network can mimic the ""style"" of any image. ",Artificial Intelligence Can Now Paint Like Art's Greatest Masters,https://www.vice.com/en_us/article/wnjve5/artificial-intelligence-can-now-paint-like-arts-greatest-masters
Rachel Pick,"A Dutch developer has written a script designed to catch potentially insensitive language. Alex—the script's fittingly androgynous name—will alert you if any input text contains offensive phrasing, and suggests neutral alternatives. For example, an input of ""businesswoman"" suggests ""entrepreneur"" or ""business executive"" as viable options instead. As the script's developer Titus Wormer told me, the idea was actually suggested to him by another coder, who was inspired in turn by this tweet from a programmer of color. Dunno how I feel about using the terms ""Master"" and ""Slave"" for describing various concepts in Software Engineering. #blackdeveloperproblems — Iheanyi Ekechukwu (@kwuchu) July 8, 2015 While it was designed with programming in mind, the tool will work with any text. ""I'm deeply interested in natural language, and am an avid open source contributor,"" Wormer said. ""For the last two years I've worked on getting computers to 'understand' human language."" He's written a script called Franc that detects more languages than Google Translate, and one called Retext that aims to make normal syntax processable for computers. Alex is designed to catch subtle errors—the kinds of things someone might type without thinking Given his past projects, Wormer was an ideal candidate to write this kind of script. ""Whether your own or someone else's writing, alex helps you find gender favouring, polarising, race related, religion inconsiderate, or other unequal phrasing,"" reads the site , where you can try it out in a demo box. However, it's worth noting that while ""cripple"" sets off the appropriate red flag, ""slut"" and ""whore"" do not. Typing ""She is a massive slut"" only gets a warning because the gendered pronoun ""she"" may be unnecessarily specific. Curious about this, I asked Wormer how he built the library of offensive phrasing, and he admits to not including words that were obviously offensive. Alex is designed to catch subtle errors—the kinds of things someone might type without thinking. ""I was actually thinking of another project ('gosh'), which would warn about profane words (such as 'asswipe' and whatnot) which don't really fit within alex,"" he said. However, alex does accept suggestions from the community, so you can help expand its bank of potentially offensive language if you'd like.",,Vice,"This script detects gendered, racist, and inconsiderate phrasing and offers neutral alternatives.","Meet Alex, the Javascript Tool to Make Your Code Less Offensive",https://www.vice.com/en_us/article/nzeawx/meet-alex-the-javascript-tool-to-make-your-code-less-offensive
Noémie Jennifer,"“You never know how an auction will go,” says Bonhams specialist Cassandra Hatton. “Some things don’t sell; some things go for double the estimate.” Hatton is the Director of the History of Science and Technology department, which she founded last year. The first sale of artifacts in this field, held in October 2014, saw an original Apple-1 go for over $900,000. Last April, a World War II era manuscript by Alan Turing, containing notes on the foundation of mathematics and computer science, went over the one million dollar mark. This Monday, September 21, 78 lots go on offer in the Bonhams New York saleroom, and Hatton is expecting sales to reach about $1.6 million in total. The auction will save some of the best for last, with rare pieces of computing history making up the three final lots: a prototype Kenbak-1 computer from 1971, dubbed “the world’s first personal computer”; another fully functional Apple-1 computer, the most expensive item at auction, with an estimate of $300,000–500,000; and the only existing Cray-4 supercomputer, which was electronics engineer Seymour Cray’s final project before he died in 1996.  The Apple-1 is composed solely of a motherboard—users were expected to add and build out the additional parts themselves. Two batches of the computer were produced: in 1976, Steve Jobs got the Byte Shop in Mountain View, California to place an order for 50 boards. He and Wozniak then took the cash from that sale and built out another 200 or so. “The boards that collectors want are the first 50, and you want one that is fully operational,” explains Hatton. The motherboard that goes on sale Monday meets both those criteria. The original owner used it once or twice, wasn’t happy with it, and decided to trade it in for something else at a computer shop in Florida. The store owner stuck it on a shelf and forgot all about it, until he saw the Apple-1 that sold at Bonhams last year. He immediately got on a plane to New York to talk to Hatton, motherboard in tow, and will most likely be taking home a hefty check next week.  Very rare George III mahogany and engraved brass orrery. Estimate $200,000-250,000. The objects on sale were culled over months of research and travel, according to Hatton. “In this field, I have two layers of collectors,” she adds. “I have more established collectors who are really only looking for the rarest, earliest, most important science books, manuscripts and objects, and don’t buy a lot of 20th century material. Then I have younger collectors just entering the market, who buy more recent technology.” This makes for an extremely varied set of items spanning several centuries.  A Rare Early 3-Rotor German Enigma I Enciphering Machines (aka Heeres Enigma), Berlin, early 1930s. Estimate $160,000-180,000. A signed letter from Charles Darwin, pointedly stating that he does not believe in the Bible, is expected to fetch a high price, as is the 1934 Nobel Prize medal awarded to George Minot for his life-saving work on anemia. The sale also includes a rare 18th century orrery (a mechanical model of the solar system) and a functioning Enigma machine used by the Nazis. One overlooked find is a small portrait of Ada Lovelace , the under-recognized 19th century English countess who wrote the first-ever computer algorithm. The World’s First Computer Programmer. Augusta Ada King, Countess of Lovelace (1815-1852). Estimate $12,000-18,000. Ambroise Paré (1510?-1590), Les Oeuvres. Paris: Gabriel Buon, 1579. Estimate $5,000-7,000.  White's Physiological Manikin. New York, James T. White & Co., Publishers, 1886. Estimate $800-1,200. Many other scientific artifacts and works on paper, dating back to the 16th century, are on sale. See them all here , or if you are in New York, check them out in person this Friday, Saturday and Sunday during public viewing hours . Related:  The Floppy Disk, Typewriter, Phonograph And More Dead Tech In The Museum of Obsolete Objects  Original Creators: Ada Lovelace  Dig Up Dead Tech At The Media Archaeology Lab",,Vice,"Bonhams New York is auctioning off an Apple-1, a Nazi Enigma machine, and a letter from Darwin stating he doesn’t believe in God.",$1.5 Million Worth of Tech History Goes to Auction,https://www.vice.com/en_us/article/4xqq89/1-5-million-worth-of-tech-history-goes-to-auction
Michael Byrne,"Among esoteric programming languages, Shakespeare is the precise opposite of Brainfuck. While the latter reduces syntax and structure to a few scraps of symbols , as far away from the adornments of English as possible, the prior reimagines code as lavish Shakespearean prose. In fact, it requires it. What could that possibly mean? Just what it says. A program written in Shakespeare looks like the text of a Shakespearean play, while also fulfilling all of the obligations of a Turing complete programming language; that is, given infinite memory and time, a Shakespeare program can be written that replicates the functionality of any program written in any other language. Unsurprisingly, it began as a class assignment. Late one February night, Kalle Hasselström and Jon Åslund, the language's authors, were sitting around with a programming assignment meant to be presented at nine the following morning. ""A few weeks earlier we had discovered a number of truly fascinating programming languages, such as Java2k, Sorted!, Brainfuck, and Malbolge, and we wanted to make our own,"" Hasselström and Åslund explain . ""We have no idea why, but that night we were also thinking about Shakespeare in general, and Shakespearian insults in particular and three hours later we had come up with this amazing idea: the Shakespeare Programming Language, SPL."" ... wherefore art thou, integer? To see how this works, consider a variable declaration, perhaps the most fundamental mechanic in coding. A variable declared within a Shakespeare program must have the name of a Shakespearean character, or it will be ignored by the compiler (the thing that translates human-readable programs, composed using higher-level programming languages, into assembly and then machine code). The syntax for this is simply the name of the variable (the character name), followed by a comma, and then whatever fluff the programmer wants to write and have ignored by the compiler. In a normal programming language, this bonus text would be considered a comment . So, a series of variable declarations might look like this. The Infamous Hello World Program. Romeo, a young man with a remarkable patience. Juliet, a likewise young woman of remarkable grace. Ophelia, a remarkable woman much in dispute with Hamlet. Hamlet, the flatterer of Andersen Insulting A/S. The compiler just cares about the names, which correspond to the integer data type (and only the integer data type). The stuff afterward that gets ignored could be used as documentation, I suppose, which is often the point of commenting code: explaining what this or that chunk of program is supposed to do or any other guidance from one programmer to another. Meanwhile, a program (or play) is composed of one or more acts, which are then composed of one or more scenes, which are then composed of one or more lines. These are meaningful divisions. Acts, scenes, and lines function as labels do in C—they can be jumped to from other places in the program using the ""goto"" statement. There's no goto in Shakespeare, however. Instead of, say, ""goto Act III""—how one would program a jump to a section of code labeled ""Act III"" in C—Shakespeare requires the programmer to type a variation of ""Let us return to scene III,"" where ""let us"" can be replaced by ""we shall"" or ""we must"" and ""return to"" can be replaced by ""proceed to."" It just depends on what sounds the most natural, given the context, for a Shakespearean play. Jumps like this form the basis of many programming control structures. Shakespeare comes with an if-then mechanic that's similar to how assembly languages handle if-thens. That is, it's a two-step process of comparison and then jumping: Juliet: Am I better than you? Hamlet: If so, let us proceed to scene III. ""Is X better than Y "" tests if X > Y . If the integer stored in X is bigger than the integer stored in Y , then the program jumps to the label ""scene III."" To push or not to push Variables in Shakespeare have an extra property that's pretty interesting. They're a bit more than plain integers and instead function as integer stacks. A stack is a crucial data structure in which lists of items are stored in a first-in-last-out fashion. For example, if I added the numbers 1, 2, and 3 to a stack in that order. The first item I could remove is 3, then 2, and then 1. The classic stack illustration is a stack of dinner plates—you can only grab the top one, and to access the bottom plate, all other plates must be removed. Stacks are manipulated with push and pop operations. In ASM/MASM assembly language, the following code snippet adds two items to the stack and then removes the top one, leaving one item. push x push y pop Pop doesn't have an argument because there's really just the one thing that can be popped. In Shakespeare, the above code would look like this (but with Shakespearean names): remember x remember y recall In the above, the language is constructed so that the programmer could write whatever Shakespearean thing they want following ""recall"" and, like the variable fluff mentioned above, it will be ignored by the compiler. The example given in the Shakespeare documentation is this, where ""me"" is a variable and ""your imminent death"" is meaningless to the program's functionality. Lady Macbeth: Remember me. Lady Macbeth: Recall your imminent death! Hello, World I thought about not copying the entire Shakespeare ""Hello, World"" program because it's really long, but the length and verbosity is also the point. Here 'tis: The Infamous Hello World Program. Romeo, a young man with a remarkable patience. Juliet, a likewise young woman of remarkable grace. Ophelia, a remarkable woman much in dispute with Hamlet. Hamlet, the flatterer of Andersen Insulting A/S. Act I: Hamlet's insults and flattery. Scene I: The insulting of Romeo. [Enter Hamlet and Romeo] Hamlet: You lying stupid fatherless big smelly half-witted coward! You are as stupid as the difference between a handsome rich brave hero and thyself! Speak your mind! You are as brave as the sum of your fat little stuffed misused dusty old rotten codpiece and a beautiful fair warm peaceful sunny summer's day. You are as healthy as the difference between the sum of the sweetest reddest rose and my father and yourself! Speak your mind! You are as cowardly as the sum of yourself and the difference between a big mighty proud kingdom and a horse. Speak your mind. Speak your mind! [Exit Romeo] Scene II: The praising of Juliet. [Enter Juliet] Hamlet: Thou art as sweet as the sum of the sum of Romeo and his horse and his black cat! Speak thy mind! [Exit Juliet] Scene III: The praising of Ophelia. [Enter Ophelia] Hamlet: Thou art as lovely as the product of a large rural town and my amazing bottomless embroidered purse. Speak thy mind! Thou art as loving as the product of the bluest clearest sweetest sky and the sum of a squirrel and a white horse. Thou art as beautiful as the difference between Juliet and thyself. Speak thy mind! [Exeunt Ophelia and Hamlet] Act II: Behind Hamlet's back. Scene I: Romeo and Juliet's conversation. [Enter Romeo and Juliet] Romeo: Speak your mind. You are as worried as the sum of yourself and the difference between my small smooth hamster and my nose. Speak your mind! Juliet: Speak YOUR mind! You are as bad as Hamlet! You are as small as the difference between the square of the difference between my little pony and your big hairy hound and the cube of your sorry little codpiece. Speak your mind! [Exit Romeo] Scene II: Juliet and Ophelia's conversation. [Enter Ophelia] Juliet: Thou art as good as the quotient between Romeo and the sum of a small furry animal and a leech. Speak your mind! Ophelia: Thou art as disgusting as the quotient between Romeo and twice the difference between a mistletoe and an oozing infected blister! Speak your mind! This is easily the longest ""Hello, World"" program I've ever seen. (Minus the compiler-ignored fluff, it'd be much more reasonable, but also not really in the spirit of the thing.) The guts Shakespeare doesn't have much for guts actually. While I used the term ""compiler"" above, that's not quite what it is. The language has a ""translator"" instead, which is just kind of a dictionary that takes SPL statements and outputs their C equivalent. The resulting C is then fed into any normal compiler, such as GCC (the Gnu C Compiler), and rendered as an executable program. I imagine this makes debugging a rather unpleasant task. Because the most recent Shakespeare release dates back to 2001, I can't vouch for its current stability, but you can still download the whole thing here . Read more Know Your Language .",,Vice,A programming language that forces coders to write like the bard. ,Know Your Language: Coding Toil and Trouble with Shakespeare,https://www.vice.com/en_us/article/jpgegd/know-your-language-coding-toil-and-trouble-with-shakespeare
Michael Byrne,"What's the difference between programming and scripting anyway? There are a billion variations of this question posted across the internet and the answer is not always as simple as it might seem. The popularity of Python, a language which is unusually adept at both programming and scripting, and JavaScript (for analogous reasons) certainty have a lot to do with the muddying, but the distinction persists because, well, there is a distinction. The programming vs. scripting difference is easy to see when we consider shell scripting. Classically, a shell script is a program (a collection of instructions or commands) intended to run within a Unix shell environment (by ""Unix"" assume that I mean actual Unix along with its many open-source analogs, e.g. Linux). There are a lot of shells actually and they can be based on a lot of different languages—Python, Java, C, etc.—but the general idea of a command line interpreter is more or less the same. The Shell If you haven't used a command line very much, it's worth taking a small amount of time to just poke around, whatever your operating system. Windows and OSX both have command line interfaces of varying capabilities. In Windows you can find it by searching ""cmd"" and in OSX it's called Terminal. (The Windows shell, which is not Unix-based but mimics it, is a lot less useful, for a variety of reasons.) The idea is that these are just non-graphical portals into your computer. A command line is a hyper-minimal version of an operating system, but with all of the same capabilities. This is what an operating system used to look like exclusively. The ancestor to all modern Unix systems came around 1970 and the introduction of Digital Equipment Corporation's PDP-11, considered to be the first microcomputer. Unix then looked like Unix now, almost 50 years later. Unix in 2015 even still comes with a ""rewind"" command, which is how a user might tell a computer to rewind its cassette tape memory. Within a shell (any shell for any operating system), programs are launched by, well, typing the name of the program. And shells also have a bunch of small bits of programs/utilities built in, like, ""ls"" (""dir"" in Windows), which will list all of the files in a particular folder (directory). Or maybe you've had to reset your IP address using ""ipconfig"" or ""ifconfig."" With ""cat"" I can create or display a small file and with ""grep"" I can search a file for a regular expression. These are all just computer programs/utilities, but they can only be run from the command line, which is all they require. Working with shells is fun because it's fast and immediate, for one thing. Rather than working with a piece of software, you're working with one aspect of the giant hydra that is an operating system. There's no waiting around for an independent program to load, nor does a shell command have to ""ask"" the operating system for resources. It is the operating system! Hello, World A shell script is a way of packaging a bunch of commands together, exploiting the immediacy and speed of the operating system and the terseness of shell commands to do more complicated tasks. Here is a ""Hello, World"" script. It's meant to be interpreted using the the super-popular Bash shell, but it could be run just as easily with another Unix or Unix-like shell. #!/bin/bash # My first script echo ""Hello World!"" This looks an awful lot like a computer program and, in a sense, it is. A critical difference, and what is usually used to demarcate between scripts and programs, is compilation. When I write some code in C++ or another typical programming language, this is the step in which a piece of software takes all of my code and converts it into machine-readable instructions. The result will be an executable file—a piece of software. To run the software, the operating system will load it all into memory and weave it into a queue of executing processes. These processes will all share the computer's processor(s). A script, however, doesn't have this step. When I write a shell script, I'm taking a bunch of pieces that are already just there as functions of the operating system or are other scripts. They're already compiled and, in a sense, you can think of them as already running. All the scripter is doing is taking these moving gears and lashing them together in useful ways. Scripting languages are often referred to as ""glue"" languages, because they take existing programs and stick them together. A script doesn't ""run"" so much as tell other things to run at such and such time in such and such order. The script also serves to ""pipeline"" data from unit to unit. If I run an ls command, for example, which will output a list of files within the current directory, I can use a single character (""|"") to direct that output to the input of another unit, which might write the names of those files into a text file. A shell script could theoretically be as elaborate as any program, but that sort of defeats the purpose. Scripting vs. shell scripting Not all scripts are shell scripts. There are entire independent scripting languages, such as Python and Lua, and they function in a similar way. The difference is that rather than using the operating system as an execution environment, a scripting language will have its own. Python code can either be packaged into complete programs (not so much scripts) or it can be used as a scripting language, in which small programs dictate the behavior of other programs. Because it's just taking other complete programs and stringing them together, a Python script doesn't have to be compiled. It's just glue. Image: null-byte Shell scripts are meant to be super-lightweight things, but a Python script can become very elaborate and the capabilities of the Python interpreter are relatively vast and ever-expanding. More scripting definitions I thought maybe it'd be helpful to offer up some definitions from around the internet because scripting really is one of those endlessly debated and pontificated-on things. Software engineer Nael Shawwa : So what is Scripting? I think it is when you use a tool or combination of tools to automate a task or series of tasks—regardless of the language it is written in. I can be lazy, I would script cooking breakfast if I could. Phoenix 750 : scripting languages (like Bash, Python, Ruby...) are mainly designed to automate certain tasks (especially Bash). a lot of hackers use Python to write exploits (though i prefer doing it in Metasploit). scripting languages like python can have an extensive amount of functionality, but scripting languages have one big downside: they are slow. that is because unlike programming languages, scripts are interpreted, meaning that the program executes command per command, rather than throwing all commands together and translating it to machine code. Herman Venter , MSDN Blogs: Programs are sets of instructions carried out directly by computer hardware, perhaps with the cooperation of a runtime and/or operating system. Scripts, on the other hand, are sets of instructions carried out by other programs, such as text editors, command line processors and most famously, Web Browsers. At a high enough level of abstraction, there is no conceptual difference between ""scripting an operating system"" and ""scripting a text editor"", but in practice the details matter and they are different enough to have a material affect the design of a historical scripting languages. Getting started Scripting is a great way to get started with programming, generally. With something like Python it's easy to do very cool and hackery things without much background. If you wanted to hack your Twitter, for example, there are loads of clever scripts for doing all kinds of stuff. Other obvious Python utilities might include web scraping, data syncing, command line Googling, HTML verification and maintenance, collecting data for/from any number of pipelines/APIs, live-coding beats, and on and on and on. Thanks to Python, you could be flooding Twitter and beyond with bots by dinnertime. Download Python 3 here . For shell scripting, you can screw around with your Windows command line, but with something like Cygwin , you can run a Linux-like environment on the same machine and have the same Linux-like capabilities. On a Mac, you're already running Unix and so shell scripting is more natural. Read more Know Your Language .",,Vice,"Shell scripts are the programs that tell other programs to do stuff, but that's just the start.",Know Your Language: The Ghost in the Shell Script,https://www.vice.com/en_us/article/z4mjzy/know-your-language-the-ghost-in-the-shell-script
 ,"Hands down, the single most disappointing fact about science fiction is that we don't yet have the technology to create the lightsaber. Second only to this is our global shortage of ray guns and phasers, stunning or otherwise. Thankfully, as long as we have electricity, we'll have the ability to simulate the aforementioned fantastic weaponry, and thanks to the good folks at Adafruit Technologies , provided a 3D printer and some basic knowledge of circuitry, we'll all be able to create our own light painting ray guns. The Instructable itself, viewable here , will guide you from printing the pieces themselves, through soldering and assembly, all the way to the FLIR imaging ( forward-looking infrared ) practices that will provide you the eye-popping results seen above. From Adafruit, a taste of what you'll be going through: Here's what the bare circuit looks like [above] . A bit different then the circuit diagram, right? You will need to ensure the MLX90614 temperature sensor is orientated the correct way in order to solder the connections properly. You can trim the terminals of the sensor. Be sure to use shrink tubing to secure the connections on the sensors.  Doesn't sound too bad.. As far as the 3D printing goes, one should expect a Makerbot to take about 12 hours to produce all the pieces: Then, it's basically a game of thread-the-needle: Sha-Zam! You've got your very own temperature controlled ray gun, perfect for FLIR light painting, Halloween costumes, and any situation that could use a Captain Kirk. Total estimated project time? 16-17 hours (12 of which are for 3D printing). It's a fantastic way to spend a weekend, and an even better way to crush the oncoming Klingon horde. Check out DIY FLIR Light Painting - Heat Map Photography for this project, and Adafruit Learning System for more. h/t 3DPrintingIndustry Related: Juicy Instructable Will Teach You How To Make Stained Fruit Glass Andy Warhol's Forgotten Floppy Disk Art Has Been Found Terrifying Lamp Will Record Your Conversations Then Live-Tweet 'Em New Exhibition Turns Gallery Into Full-Fledged Candy Factory",,Vice,New Instructable Teaches You How To 3D-Print Light Painting Rayguns,This story is over 5 years old,https://www.vice.com/en_us/article/wnp589/new-instructable-teaches-you-how-to-3d-print-light-painting-phasers
 ,"Wage Islands is a 3D map of the New York City that reveals the geographies of access throughout the city, showing access to NYC based on housing and wages. Designed and created by Ekene Ijeoma , the map floods with water to show how severe access to housing based on wages really is. A button moves the physical map up and down in the water and an LCD display shows the corresponding wages and watered areas. The interactive installation is on display at the Storefront for Art and Architecture in Downtown New York City, alongside a series of 30 drawings, titled Measure , by 30 international architects and artists. The Storefront states that the drawings seek to “find measures, resist measurement, and measure the immeasurable by presenting from the real to the fictional and from the functional to the symbolic.” The exhibit is thus a heady display of information turned into a healthy experiment in visualizing complex civic issues. These are the types of projects Ijeoma likes to take on. In 2013, he co-designed The Refugee Project , a temporal map that illuminates refugee migrations worldwide since 1975. Wage Islands is in line with that trajectory of using data to tell stories about social issues, though Ijeoma hopes that Wage Islands is more poetic: “I thought of Wage Islands at Re3Storyhack, a 'hackathon for storytellers with a conscience' where creatives and technologists projects were driven by non-profit organizations not corporations,” Ijeoma tells The Creators Project. “At Re3Storyhack I worked with Fast Food Forward who have been fighting to raise minimum wages for fast food workers to 15 dollars an hour. I thought, what if the fight was framed around access not just wages? I remember thinking of the post-Sandy New York Magazine cover image of Manhattan and how it showed the geographies of access to energy. I wanted to design a big data-driven interactive installation that could give you that same visceral feeling.” Ijeoma worked in collaboration with a handful architects, engineers, and programmers to make the installation come to life. The process itself could be considered a physical collective bargaining: “Richard Dunks and Juan Francisco Saldarriaga helped with the geographic data modeling. Ryan Whitby and Jordan Taithelped with the 3D modeling and laser cutting. I glued together around 500 laser cut parts for the model. What a puzzle!"" He explains, ""Gwylim Johnstone, Dallas Swindle, Eric Macneil, and Pepin Gelardi helped with the mechanical engineering, which was the most difficult. It took a lot of trial-and-error to stop water from leaking into the motors. Lastly, Jonathan Dahan and Jonathan Sparks helped with the Arduino wiring and programming."" Wage Islands becomes a beautiful example of creating for the common good. It elicits more dialogue around what we consider fair and just. New York City is ever changing, from gentrification and new labor industries. The map is critical of this changing tide. “There’s something about seeing the clear abstracted forms of the city coming out of the dark clouded water, and feeling the concreteness of the message, says Ijeoma. “Every time the ending is the same—inequality in wage and housing/access to the city is a big issue and one that needs more rhetoric.” Wage Islands from Ekene Ijeoma on Vimeo . Wage Islands is currently being exhibited as part of Measure at the Storefront for Art and Architecture, from now through September 19. Click here to learn more about the show. Related: Working-Class Artists Still Exist in Philadelphia [Exclusive] Artist Hides a $2,000 Gold Nugget Inside a Mansion Filled with Dirt Insta of the Week: Art Simulates Financial Crisis",,Vice,Turning New York's Salary Gap into an Interactive Sculpture,Entertainment,https://www.vice.com/en_us/article/mgppyx/turning-new-yorks-salary-gap-into-an-interactive-sculpture
by,"Art school assignments too easy? Your next-level teacher might be on Twitter. Meet Art Assignment Bot , a Python-scripted Twitter bot whose sole function is its namesake; once an hour, on the hour, it creates a randomly generated art assignment using words pulled from assignments art teachers have used for centuries. It's a process-in-motion that will continue until the Twitter servers are down for good, or until after the bot generates 90,345,024 assignment ideas (which would take approximately 10,000 years). Jeff Thompson , the artist, professor, and programmer behind Art Assignment Bot, is also responsible for Every Possible Photograph . This custom-encoded software is designed to generate every possible combination of pixels within a low-resolution grid. With an estimated “finishing date” 46 trevigintillion (that's 46 x 1072) years from now, it might be Thompson's longest running process, but Art Assignment Bot is a close second. Just this year, another project by Thompson, Computers on Law & Order , was commissioned by Rhizome. For it, Thompson combed through all 456 episodes of the iconic TV show, and took a screenshot of every computer that appears within the episodes. The result is 11,000 images that seem to chronicle an evolution of computer usage parallel to what actually occurred in life off-screen. We interviewed Thompson to get some more insight into what goes on in the mind of this modern code artist. What inspired Art Assignment Bot? Where's there anything in specific that prompted you to create the Twitter account? Jeff Thompson: Over the past year or so I’ve been making lots of bots, and doing algorithmic writing in general. Bots that ask “would you rather” questions ( @wouldratherbot ), that traverse an endless dungeon ( @dungeon_bot ), and a now-defunct bot that replies to random tweets with “Really?” Art Assignment Bot came out of a lot of these kinds of experiments. I see these bots as creatures or machines, ones that are suited to their ecosystem and produce surprising results or spur interaction between algorithmic text and a human reader. Art Assignment Bot in particular isn’t really all that complicated, but it’s able to produce surprising results and gets people to interact with it. I think that’s the mark of a good bot. Why did you select the specific thematic art-words (ex: intervention, exploitation, psycho-sexual, etc.) that the Twitter account uses? Were these taken from assignments you worked on in the past? The “topic” for the assignments are pulled from canonical art school projects and what I see as over-generic or overused art themes (home, family, memory). There are also topics from historical artworks. For example, “war” is based on Goya’s famous “Disasters of War” series. I also threw in some more contemporary curveballs, like ""glitch"" and what seems to be Twitter’s favorite: “cats on the internet.” In your mind, should art practice and study be a didactic process? Do you think assignments like the ones from @artassignmentbot actually help improve an artist's skill set? Why or why not? I see bots as a form of writing, and as pieces in of themselves. If the tweets engage someone, or make them laugh or think about their practice as an artist or how they look at art, that’s fantastic. I feel more connected to the projects of conceptual art and what Kenneth Goldsmith calls “uncreative writing”. That kind of work could perhaps be called didactic, but I like to think of it as requiring a more active engagement with *how* something is made than what it depicts. Can you further explain how Art Assignment Bot engages with and critiques traditional art school assignments? Do you have any personal experiences with art assignments like these? I didn’t really intend the bot to be a critique of art school (I really enjoyed my time there), though it does intentionally try to shake up some topics that I think are a bit over-used. For example, a photograph of a sunset is pretty boring, but these assignments are weird, surprising, and would be challenging to make:  Make a clay form of the history of sunsets, due on Sun, Jun 0. — Art Assignment Bot (@artassignbot) June 8, 2014  Build a videogame about sunsets, due in 27 seconds. — Art Assignment Bot (@artassignbot) February 3, 2014  Make a clay form of the history of sunsets, due on Sun, Jun 0. — Art Assignment Bot (@artassignbot) June 8, 2014  As a teacher, I think the bot does provide some interesting ideas. I would totally love to teach an entire semester dictated by bots and algorithmic systems. Has anyone actually completed the AAB-generated projects? Can you tell us about any responses to the Twitter account? Yes! There seem to be two categories to finished assignments: the quick-and-dirty Internet link and a few actually handmade responses. Here are two recent examples: @jordan_yearsley  @artassignbot that was easy pic.twitter.com/uOec2uJYv1 — marc yearsley.com (@marc_yearsley) June 10, 2014 @artassignbot  @evhan55 done pic.twitter.com/XcOTvkJhFc — Emily Eifler (@emilyeifler) June 9, 2014 What other projects are you currently working on? On the bot side, I have a little one that runs on my personal account that tweets periodically how many files my computer has open at any given moment. I’m interested in a diaristic relationship with technology, especially when it reveals sides we don’t consider. I have another little bot that will go live soon called Better Than Bot that makes random “this is better than that” statements. I’m hoping it sparks virulent arguments! Aside from that, I’m spending my summer on a few other projects: building a machine that counts grains of sand (to make a sculpture of all the 0s and 1s on my hard drive) and creating a computer-vision algorithm to let computers recognize other computers. For the code-based out there, Jeff Thompson has graciously made available the script of @ArtAssignBot . More of Thompson’s projects can be viewed on his website . Related: 2,800 Screens Create Internet-Enabled LCD Mosaic Kenneth Goldsmith Printed Out 33 GB Of The Internet In Support of Aaron Swartz God In The Machine: An Interview With Computers Club Founder Krist Wood",,Vice,Entertainment,This story is over 5 years old,https://www.vice.com/en_us/article/4xqvbm/the-art-assignment-bot-is-generating-the-prompt-for-your-next-masterpiece
by,"The supercut has become the Internet’s practice of choice for humiliating lazy, cliche-ridden, or repetitive media. We’ve been in love with the genre ever since someone lampooned the outrageously inaccurate portrayal of imaging tech in cop shows (""Zoom in on the reflection"" gets recycled a lot ), and they’ve only gotten funnier since. Entire YouTube channels have been founded on such videos. The creators invest hours and hours of painstaking research and editing into each supercut in return for sweet, sweet likes and views—and many have earned millions of them. However, one clever coder is changing the game—as they often do—by algorithmically streamlining the entire supercutting process.  Sam Lavigne is a computer artist and game designer who’s at the cutting edge of creative computing. He’s run a technological experimentation blog called ‘Work In Progress’ since the fall of 2013, created such crazy contraptions as a code to transform a camera into a musical instrument , and even made a fully-automated stop and frisk robot . Now he has written a code called Videogrep , an app offering to the supercut biz. And the results are phenomenal. Watching a cycle of speech lead-ups that never reach climax in ’Total Silence’ is awkward, frustrating, cringe-inducing, and perfect. We haven’t even been able to watch the whole thing at once—only a computer could craft something like that. Lavigne spoke to The Creators Project about Videogrep, the art of supercutting, and the limitations—and potential—of procedurally generated art.    The Creators Project: Where did you get the idea to create a super-cutting super-code?  Sam Lavigne: Lately, I've been interested in procedurally generated text—for example, I made a Python script that transforms literary texts into patent applications , and I've also been experimenting with procedurally generated cinema . I started to create a system using surveillance cameras that acts as a director and an editor, automating camera movement and determining when to cut. At some point, I decided I should try to combine those interests, and use some of the techniques from my text experiments to generate edited videos.  How does the program work? Videogrep is a python script that searches line by line through the text of subtitle files for a given phrase, finds the time stamp for each line of dialog that matches the search, and then uses the amazing MoviePy library to create clips and stitch a new video together. It can handle multiple video files. The basic search takes any regular expression, but you can also give it a hypernym (a semantic category, like ""liquid"" or ""tool"") or part-of-speech tags (shorthand for different grammatical categories). For example, ""NN"" is a noun and ""JJ"" is a verb, so you could search for all lines of dialog that contain a verb followed by an adjective. For real example see my TED supercut . Using the hypernym search, you might, for example, make a supercut of all references to ""body parts"" in Cronenberg movies (I haven't actually done this yet but probably should). Both of these features make use of Pattern , which is a great natural language processing library.  What was the most challenging part of creating it? How long does it take? Creating it wasn't actually that difficult, since Zulko's MoviePy library and the Pattern library do most of the heavy lifting. The harder part is figuring out what to make with it that's actually interesting. That is, what source material and search terms to use to make compelling supercuts. I spent a long time downloading different movies I love or hate, mass downloading videos from the White House Youtube channel and from TED, and then experimenting with different search phrases.  I experiment a lot, so the time varies. The TED talk supercut took a while because I had to write a script to mass download TED talks and their subtitle files, then did some analysis to find the most commonly used grammatical structures, and then just played around until something felt right. The ""Total Silence"" video on the other hand basically made itself.   Supercutting—a long, and tedious artform—has been a certain YouTube staple now, for years. How do you think your code affects the people who have been doing it 'the hard way' for so long? I think that well crafted video compositions still need to be put together by hand—Videogrep can't and isn't intended to replace a human editor. For example, you wouldn't be able to make something like this or this with it. The greatest strength and weakness of procedurally generated work is that the computer has no concept of meaning. When the computer speaks, it doesn't know what it's saying; when it edits, it doesn't really know what it's seeing. It's free from language, which means it's unrestrained in its ability to make wild associations. In this sense, I think Videogrep could be a strong tool to explore and experiment with video quickly, to help find surprising and exciting juxtapositions.  Eventually, of course, all art will be generated by computers, with the only uncertainty being if they will prefer to make poppy trash art, or obtuse high art.  Of course! Give me your wildest imaginings of what completely computer generated, autonomous art might be like. It's obviously impossible to see beyond the event horizon of the autonomous art machine singularity/apocalypse, but I would speculate that the advent of completely automatic art will give rise to automatic art history, wherein intelligent critique machines hold endless academic conferences, subtly perform character assassination on each other, and vie for rapidly vanishing procedurally generated tenure track positions.  And what are you working on now?  Right now I'm taking a pause from generating new videos to fix up some bugs in Videogrep. I'm also starting work on a collaborative novel about data entry jobs, using a methodology I'm calling ""Object Oriented Fiction"" or ""OOF"" for short.  For more crazy experiments beyond the normal realm of tech, check out Lavigne’s personal website here , his experimental blog here , and his Youtube and Vimeo channels here and here .   Image via  Related:  See A Supercut Of Wes Anderson's Symmetrical Center Shot  Supercut Film School: Q&A With Elusive, Anonymous Video Essayist kogonada  Twitter's ""Art Assignment Bot"" Is Generating Prompts For Your Next Masterpiece  Beautifully Simple Code Generates Realistic Terrain With Fractals",,Vice,Entertainment,This story is over 5 years old,https://www.vice.com/en_us/article/wnp59w/this-programmer-will-show-you-how-to-make-instant-movie-supercuts
by,"Image via The Simpsons’ hometown of Springfield, USA— which lies right between the borders of Ohio, Nevada, Maine, and Kentucky —now exists within your web browser (and no, we don't mean via web streaming). Thanks to the fine work of web developer Chris Pattle , Simpsons fans the world over can now download most of the major Simpsons characters in CSS . Hand-decoded in same programming language that tells most browsers how their content should look—the ""scaffolding"" of the Internet, if you will—the characters look just as real as if they had sprung from Matt Groening’s own pen tip. A simple breakdown shows how each character is actually composed of letters, numbers, and symbols cleverly arranged and manipulated into the familiar faces of our favorite dysfunctional family. Homer’s luscious locks, for instance, are made of an “M” and some ellipses. His ears? A rudimentary letter ""G,"" ( wink, wink ). In order to create his custom codes, Pattle closely examined the The Simpsons and cross-reference his knowledge of CSS with the faces of Springfield’s animated residents. Once he’d figured out which CSS characters would fit the faces of which characters, he hand-composed the code, down to the most minute details. Each face, alone, consists of hundreds of lines of code. In total, Homer is almost 540 lines, Bart, almost 600, and Marge clocks in at nearly a thousand. If you ask us, we always knew she was a more complex character than Homer— but now we have proof. Pattle explained more about the creative process behind this project over on his blog . h/t Reddit Find more of Pattle’s work on his personal website , or view his code directly on his Github . Related: The 8 Best Artist Recreations Of The Simpsons' Intro Couch Gag This Programmer Will Show You How To Make Instant Movie Supercuts We Talked To The Curator Organizing A Bill Murray-Themed Art Exhibition",,Vice,Design,This story is over 5 years old,https://www.vice.com/en_us/article/yp5d5k/see-the-simpsons-converted-into-computer-code
by,"In 2011, filmmaker Martha Fiennes premiered a new take on the Christ Nativity scene with a hi-tech cinema experiment that The Guardian once described as ""a moving Christmas story like no other."" The artwork, titled Nativity , is a pioneering motion-picture piece where all of the scene's details are constantly evolving in ultra-slow-motion—a generative film loop that can run for months without ever repeating its own imagery. Created in conjunction with producer Peter Muggleston and motion graphics experts MPC , Fiennes' visual piece was the first realized project of SLOImage — a custom-built video encoding software that allowed the team to stitch together separately composed shots of actors and props across a digitally-illustrated background. Groups of actors, the lighting, birds in the air, and stars in the sky all work together to create images, just like in a traditional film, but in Nativity , each component exists on its own plane, culled into frame by the program. Countless combinations of people, lights, and wildlife converge upon the screen in fresh ways that even the creators have never seen—and may never see again. After originally debuting in the Convent Garden piazza in London, the current iteration of the project will be on desplay through July 6th at the exhibition Building the Picture: Architecture in Italian Renaissance Painting at the National Gallery in London. T he randomized, perpetual, self-generating moving image, draws from many disparate elements to create Fiennes' exquisite take on the classic Renaissance scene. When it comes to the technical aspects behind the project, however, Nativity is anything but classical. Shot on a high speed digital camera at hundreds of frames per second, the actors have a smooth, dreamy quality of movement. All of the action was captured in perfect loops, so that each could be stitched into the ever-morphing tapestry. To ensure the exactitude of this incredibly complex procedure, the programmers behind the encoding software actually had to be on the set during the entire production, upholding the technological bounds and limitations for the cast and crew. The intricate design processes can be watched through a ""Making Of"" video by MPC here . ""It is an image that is alive,"" Fiennes told The Guardian. ""That brings together ideas, creativity and technology.” Endlessly complex, generative, and coded in a mystical beauty—it’s impossible to predict what will happen next in this digital artwork— Nativity is the zenith of the as-of-yet unparalleled SLOImage technology. Though undeniably influenced by iconic Renaissance paintings, Nativity was inspired by Fiennes' feeling limited in the moving image world. This work was a chance to ""challenge the conventions of moving image editing in which images and sequences are 'fixed' indefinitely,"" explains SLOImage on the work's about page. What fascinated me most,” Fiennes told The Guardian, “were all these Renaissance paintings of the Nativity...And what drew me in were things like the representation of women, the place of the mother figure, all of which have been lost in Christianity. The pull for me was to interpret anew the world of this iconic story by making use of this incredible technology.” The work may be a modification of a famous religious scene, but she notes its not meant to be controversial or sacrilegious. ""My aim was to create a contemplative piece of art,"" that pushes the boundaries of technology and moving images. In a way, choosing a famous ancient scene is the perfect subject to explore technology that brings art into the future. It may be summer, but when it comes to a stunning, multi-layered project like Nativity , we'll happily celebrate Christmas in July. Nativity is on view through July 6th at the National Gallery in London . Images courtesy of SLOImage For more on MPC visit the effects company's site here . And for more on the exhibition see here . Related: Crafting Time-Bending Aliens And Other VFX Challenges Behind ""Edge Of Tomorrow"" This Programmer Will Show You How To Make Instant Movie Supercuts Robot Film ""Construct"" Could Change Everything You Know About CGI Times Square Becomes A Cosmic Epicenter In Yorgo Alexopoulos’ ""Transits""",,Vice,Entertainment,This story is over 5 years old,https://www.vice.com/en_us/article/mgpenp/experience-the-nativity-scene-as-a-generative-ever-changing-digital-painting
 ,"Images via Experimental digital artist Kynd has been using code to create art since the late 2000s, and every year of that experience shines through in his elegant new procedural music videos, locus of everyday life  and  nemumel . The video set's abstract nature is a departure from some of his more concrete digital generations—such as Auto Portraits , a series of faces generated from Flickr combined pictures, and his real-time 3D charcoal drawing of Thom Yorke's head —but they still retain a feeling of structure that suits the atmosphere of the background music. The videos react to the latest ""sound sculptures"" from Sawako , a Japanese electronic artist working within experimental ambient music. ""The sound has been gripping and keeping me a little late ever since Sawako shared me the tracks,"" Kynd wrote on his site in response Sawako's latest effort, nu.it . The fruits of Kynd's passion are these responsive watercolor animation that ebb and flow along with the soundscapes behind them. Kynd coded the videos using OpenGL and OpenFrameworks , though they're so smooth that we could have mistaken them for hand-drawn artwork if we didn't know better. If this is how Kynd responds to relaxing ambient tracks, we can only imagine how he'd interpret a trap beat or ripping metal jam. Or, better, what he'd create after listening to  songs created by people's brainwaves . Ebb and flow in tune with some GIFs from Kynd's music videos below: Get lost in more of Kynd's programmed art creations on Vimeo , Twitter , and his website . Related: Blade Runner Recreated Using Frame-By-Frame Watercolor Paintings Music Visualizer Turns Your Favorite Tunes Into Bursting Geometric Shapes Listen To A Hacked Dot Matrix Printer Play 'The Macarena' Digital Cells Pulse And Grow In Max Cooper's New Music Video",,Vice,Watch Code-Generated Watercolors Flow Over Tranquil Tunes,This story is over 5 years old,https://www.vice.com/en_us/article/yp5vv7/watch-code-generated-watercolors-flow-over-tranquil-tunes
by,"Images courtesy of the artists Programming language and coding aren't as dry as just a bunch of 1's and 0's. Be it coding projects that transform the Mona Lisa into the color palette of other iconic artworks, or characters from The Simpsons as CSS , the creative boundaries of programming are as boundless as the imagination of the creator. At a new exhibition in Brooklyn, two coders are investigating new forms of communication between man and machine, as well as how images can be used as the foundation of programming language (rather than the other way around). Starting this Saturday, the Brooklyn-based Transfer Gallery will offer its space to A Bill Miller and Daniel Temkin , two new media artists who will use their new exhibition, Language and Code , to present new methods of “creative coding."" Showcasing impressive alternative visual experiences generated with original means of manipulating texts and images, as the two artists combine their most recent forays into contemporary communication systems and human/machine interactions into web-based works, the viewer will be immersed into the heart of a conceptual exploration into our nascent post-digital era. On one end, Daniel Temkin (who debuted his Glitchometry series at Transfer back in December ) will present his Light Pattern programming language, a set of programs written in a unique language that replaces traditional syntax and encoding with information from a kaleidoscopic series of digital photographs; according to color schemes and exposures. In other words, Temkin's piece uses images as its source code. Explains Temkin, “To make Light Pattern programs for my ' Hello, World ' series for example, I often shoot outside close to noon, considered the worst time of day to shoot (due to high contrast). This is the best time, since I have to take many different exposures to get a working program. It's the photos taken collectively that are important; no one image does anything in LP. ” He told The Creators Project, “I go out to shoot knowing that it's very likely my Light Pattern program will fail; most of them don't work. They don't print the correct thing or they just crash. I do everything I can to make a program that works, but then, once I've made this effort, I no longer really care whether it succeeds; it's about how this attempt shapes the images, not whether it actually works."" For the occasion, not only will Temkin present videos that testify to the originality and beauty of his new language, he will also reveal the Light Pattern device— a camera, arduino controller, and system of rotating filters— that automates his shooting process. On the other hand, through 8 abstract animations that fall in line with his previous work, A Bill Miller ( of Plink Flojd fame ) will offer gridCycles , a new digital textual environment. Developed in Javascript 3D library three.js and webGL to enable direct web browser access, this series of experimentations takes advantage of the technical and aesthetic specificities of the world wide web. Here, Miller revisits the origins of the image/text relationship, and imagines an evolution on the theme through new developments in web code. “This [...] presents a new challenge for me because the animation is entirely code-based.” Miller told us. “It only occurs in the browser and doesn't require rendering out an animation, compressing it, posting to a video hosting service and sharing socially. It also presents an opportunity to make an animated environment that is interactive at some level, even if a lot of it is predetermined.” The collaborative show is not only an open door to new forms of communication and exchange between humans and machines, it's also an open invitation to reexamine the relationship between text and images. Take a detour to Transfer this Saturday for the show's opening night , or visit the exhibition, ongoing through September 1. Related: I Became An Algorithm At The Museum Of Modern Art Error-Ridden Data Visualizations Become Beautiful Accidental Art Watch Code-Generated Watercolors Flow Over Tranquil Tunes",,Vice,Entertainment,This story is over 5 years old,https://www.vice.com/en_us/article/wnpdxq/new-exhibition-pushes-the-binary-boundaries-of-creative-coding
 ,"Hey, web developer dudes and dudettes: What's your favorite programming language? Is it CSS? Is it JavaScript? Is it PHP, HTML5, or something else? Why choose? A new programming language developed by researchers at Carnegie Mellon University is all of those and more—one of the world's first ""polyglot"" programming languages . Sound cool? It is, except its development is partially funded by the National Security Agency, so let's look at it with a skeptical eye. It's called Wyvern—named after a mythical dragon-like thing that only has two legs instead of four—and it's supposed to help programmers design apps and websites without having to rely on a whole bunch of different stylesheets and different amalgamations spread across different files: ""Web applications today are written as a poorly-coordinated mishmash of artifacts written in different languages, file formats, and technologies. For example, a web application may consist of JavaScript code on the client, HTML for structure, CSS for presentation, XML for AJAX-style communication, and a mixture of Java, plain text configuration files, and database software on the server,"" Jonathan Aldrich, the researcher developing the language, wrote . ""This diversity increases the cost of developers learning these technologies. It also means that ensuring system-wide safety and security properties in this setting is difficult."" That system-wide safety and security properties bit is important, and perhaps might explain why the project is backed by the NSA. See, beyond all its standard spying and data collection, the NSA also has to protect its own systems from hackers, and, ostensibly has some sort of obligation to help American companies do the same (especially if any one of a series of cybersecurity bills ends up being passed ). By confining everything you need for a web app or a mobile app or a webpage in one place, it'd theoretically be easier to lock it down, as Aldrich explained. He also notes that it's ""designed to help developers be highly productive when writing high-assurance applications."" A ""high-assurance application"" is code for ""one you really don't want to screw up."" It's usually used to refer to military code and applications that the armed forces uses for communications, missile systems, radar, medical devices, and that sort of thing. I could be totally wrong—the NSA does fund all sorts of things in all sorts of fields, and it may have other plans for it. But security seems the most obvious, at this point. In any case, Aldrich says that Wyvern can automatically tell what language a person is programming in, based solely on the type of data that's being manipulated. That means that if the language detects you're editing a database, for instance, it'll automatically assume you're using SQL. The language is working in a prototype mode at the moment, and, as with most new programmery stuff these days, it's all open source, so you can see how it works exactly over at GitHub . Please let us know if you can think of other reasons why the NSA might be interested in its development.",,Vice,The New NSA-Funded Code Rolls All Programming Languages Into One,This story is over 5 years old,https://www.vice.com/en_us/article/xywa8q/new-nsa-funded-programming-language-is-all-programming-languages-in-one
 ,"In 2009, NASA launched its very own TopCoder challenge in cooperation with researchers from Harvard Business School and London Business School. The competition, hosted by the Johnson Spaceflight Center's Space Life Sciences Directorate, invited the competitive software development community TopCoder to create algorithms to optimize medical kits for long-duration spaceflight. This first contest spawned the NASA Tournament Lab (NTL) , which now regularly hosts contests wherein the TopCoder community competes to solve problems. Coders get to win prizes and NASA gets to take advantage of crowdsourced expertise. It's win-win, and up next, NASA is crowdsourcing solutions to deep problems like asteroid tracking, deep-space networking, and astronaut health.  TopCoder , part of the global cloud consultancy company Appirio, is a company that administers contests in software architecture and development. Since its inception in 2001, TopCoder has provided a stable infrastructure for coding competitions that see members submit solutions to given problems. Currently, the community consists of some 630,000 data scientists, developers, and designers. For some, the competitions are a hobby and for others its a way of gaining experience in the field before entering the job market. But in any case it makes the community a wildly skilled and successful one that saves the space agency the cost of hiring in-house developers to tackle the same problems. ""As NASA continues to push the boundaries of human imagination and innovation, we have seen the value in utilizing a citizen-based professional crowd to complement our internal efforts and solve complex real-world challenges,"" said Jason Crusan, the director of NASA's Tournament Lab. ""Tapping into a diverse pool of the world's top technical talent has not only resulted in new and innovative ways to advance technologies to further space exploration, but has also led to a whole new way of thinking for NASA, and other government agencies, providing us with an additional set of on-demand tools to tackle complex projects."" Now, NASA is hoping to tap into that expertise to solve some of its biggest problems—like hunting for and tracking asteroids. The Asteroid Data Hunter challenge is part of the ongoing Asteroid Grand Challenge series running on TopCoder in cooperation with Planetary Resources Inc. (PRI). This particular challenge is tasking the TopCoder community with developing an algorithm that can find and validate asteroids, giving scientists a better picture of the rocks that are threatening our planet while cutting down on the number of falsely identified asteroids. Phase two of the challenge is launching tomorrow, with the aim of finding an algorithm that will increase the sensitivity of asteroid detection software. The ultimate goal of this contest is to develop a program that anyone can run on an everyday laptop, meaning scientists will be able to use the processing power of willing citizens to help create a detailed map of threats looming in space. Another problem NASA is turning to the TopCoder community to solve is part of the Disruption Tolerant Networking Challenge Series. The task is to make communication between ISS astronauts and Earth more efficient and seamless. Emails don't go straight from the ISS to an astronaut's family's home computer; the messages have to travel through a series of nodes, with information sent to one node being stored in that location until the next node comes into view, at which point it takes the next leap in its journey. Imagine sending information over an internet in a state of constant interruption, which sure sounds like much of America's busted web infrastructure, but be assured that it can get much, much worse. What this means is that, although the ISS orbits about 250 miles above the planet, an email travels some 44,000 miles between the station and the Earth-bound recipient as its routed around the planet. The TopCoder community is working on adapting an existing email system to take advantage of delay-tolerant networking by converting email into bundles instead of packets to simplify data sharing. Finally, the TopCoder community is taking on astronaut health with the ISS Food Intake Tracker (FIT) challenge . Microgravity does a number on the human body; not having to support their own body weight for weeks or months at a time causes astronauts to lose muscle mass and bone density. Exercise helps, but more important is diet. Each astronaut launches with a target value of nutrients and calories to consume each day, and it's up to every individual to pick foods that meet those needs. Where TopCoder comes in is helping NASA develop a user-friendly iPad app to help astronauts track their food to make sure they're meeting their nutritional needs. Prototypes going through preliminary testing at the Johnson Spaceflight Centre are using voice recognition software to call up information like personal profiles and meal plans. The goal is to have the first ISS FIT system launch in the spring of 2015, taking the guesswork out of maintaining a healthy diet in space. ""These data scientists [that make up the TopCoder community] have been able take the complex, data intensive problems facing NASA and other federal agencies, analyze it, and then turn that analysis into creating actual solutions to advance some of their most important initiatives,"" said Dr. Karim R. Lakhani, Principal Investigator of Harvard-NTL and Lumry Family Associate Professor of Business Administration. It will be interesting to see not only what the TopCoder community can do to solve these big problems, but what other challenges NASA will crowdsource solutions to in the future.",,Vice,NASA Is Contest-Sourcing Solutions to Its Deepest Problems,This story is over 5 years old,https://www.vice.com/en_us/article/mgb7nq/with-help-from-topcoder-nasa-is-contestsourcing-its-deepest-problems
 ,"In an analog experiment designed to tap into the aesthetic of demoscene art—where programmers use just a few lines of code to generate stunning visuals—animator grade die has extrapolated the simple act of opening a soda bottle into a surreal short called Fun With Fluids . In the 40-second clip, the artist attempts to push the art of dynamic splitscreen to its limits, multiplying 8mm footage of soda bottles across the screen and moving them about like the video version of a slider puzzle. grade die told The Creators Project that with Fun With Fluids he wanted to, ""let the visuals become analog, let the audio go gaga and answer following question: What would it be like if code was a drink? "" He answers this question by emulating the procedural demoscene style, but with completely vintage technology. Acompanied by music from Sascha Jatho , Fun With Fluids won second place in the animation category at the Evoke demoscene festival . Though we wish there was a Moxie shout out, the retro video definitely quenches our thirst. See some stills from the short below. Find more of grade die's animation explorations on his website and Vimeo channel . Related: 'The Motion Paradox': Kaleidoscopic VJ Visuals Mapped To Dark Electronic Beats New Exhibition Pushes The Binary Boundaries Of Creative Coding This Awesome Claymation Throwback Was Made From A Compilation Of Vines Help PES Make A Stop-Motion Sandwich Out Of Vintage Sports Gear",,Vice,Watch An Animator Discover What Code Would Be Like As A Drink,This story is over 5 years old,https://www.vice.com/en_us/article/qkwbnx/watch-an-animator-discover-what-code-would-be-like-as-a-drink
 ,"Michael Anderson's wood carvings are about as far from old-timey whittling as it gets. By programming a specialized computer numerical control machine (CNC), he cuts incredibly intricate geometric patterns into pieces of plywood. “I’m interested in using this technology to express art through form, texture and spatial experience,” Anderson told Instagram . The process itself is reminiscent of early, subtractive versions of 3D printing, which carved desired objects out of blocks of wood. Often naming his works after phonetic representations of the phenomena they represent (e.g. /ˈspīrəl/ , /ˈärˌɡīl/ , and /rəˌvərbəˈrāSH(ə)n/ ), Anderson's eye for finding patterns within the wood yields consistently mesmerizing results. “Each material expresses itself in a unique way,” Anderson explains of his newest efforts, which involve making sculptures from plaster and resin, as well as with a three-sided flip mill . “Experimenting with different materials allows me to understand the limitations of not only the hardware, but the material itself.” As a result, he's been able to transform his relatively flat wood-patterned carvings into elegant 3D cubes. Revealing layers, Michael Anderson, 2015. [vawr-teks], Michael Anderson, 2015 Liquidwood, Michael Anderson, 2014 (saɪˈmætɪks), Michael Anderson, 2015 /rəˌvərbəˈrāSH(ə)n/, Michael Anderson, 2015 Hybrid scripts resulting in layered valleys, Michael Anderson, 2014 /ˈärˌɡīl, Michael Anderson, 2014 More tool paths, Michael Anderson, 2014 Experimenting with a 3 sided flip mill, Michael Anderson, 2015 Sand, paint then repeat, Michael Anderson, 2015 Find more of Anderson's carvings on his Instagram feed and Behance page . H/t Instagram Related: Laser Cut Wood Sculpture Morphs into a Kaleidoscopic Portal A Single Block Of Wood Becomes An Absorbing Stop-Motion Animation '15000 Volts' Turns A Piece Of Wood Into An Electrifying Short Film Li Hongbo Carves Silhouettes From Butcher Knives",,Vice,Artist Uses Algorithms to Carve Hyperdetailed Patterns into Plywood,Entertainment,https://www.vice.com/en_us/article/4xq3pj/artist-uses-algorithms-to-carve-hyperdetailed-patterns-into-plywood
Michael Byrne,"A programmer sits down with some reasonably basic task. It might be implementing a list-sorting scheme, coding a basic version of Minesweeper, or diagnosing a memory leak. The timer starts, giving 10 or 25 or 45 minutes, and the resulting code is fed into an algorithm that applies a bunch of different criteria and returns a score. Job interview over. This is the basic idea behind a new instrument developed by a trio of Norwegian computer scientists. No longer would potential employers have to rely on ""proxy variables"" like work experience or education. The question is just whether or not a program decides your programming ability is good or bad. How quickly can you implement binary addition? Can you make a digital clock and display it graphically? Print, ""Hello, world."" ""We are interested in how skill can be measured directly from programming performance,"" Gunnar R. Bergersen and team write in the current edition of Computer . ""Consequently, our research question is, to what extent is it possible to construct a valid instrument for measuring programming skill? The implicit assumption was that the level of performance a programmer can reliably show across many tasks is a good indication of skill level."" Bergersen and his team crafted their instrument using long lists of questions and criteria that could be objectively applied to a given solution. Offered as simple ""yes"" or ""no"" questions, it becomes possible to automate this sort of evaluation. Why is recreating Minesweeper a more apt test of programming skill? One example: ""Is an abstract base class used in X?"" An abstract class in programming is a way of laying out data fields and routines/functions that aren't directly accessible to a user (that might be viewed as ""internal"" to a program), but the important thing is that it can be verified via a simple automated check. There's nothing subjective about it: y/n. The instrument itself was creating using data collected on 44 subjects/programmers set to work on the given tasks. This is where Bergersen and company came up with the tool's baseline scores. Said baseline was further validated using another 19 programmers. The Norwegian team admits that 65 is an uncomfortably small sample size. Rigorous quantitative evaluation isn't a new thing in the world by any means, in industry or academia. Setting aside general knowledge evaluations like the SAT or GRE, university computer science programs have been known to require similar albeit human-evaluated tests for admission, e.g. implementing this or that data structure from scratch. It's probably more reasonable in computer science and engineering than most anywhere else even, just given the ease with which one can totally just fake it based on the preexisting mountains of code existing a mere Google search away. Still, there's a philosophical angle. What task or collection of tasks is general enough to say someone is a good or bad programmer? As Bergersen notes, ""The universe of potential programming tasks is infinite."" It's easy to reduce programming to fundamentals, crafting binary trees and intuitive GUIs, but at the same time one difference between an acceptable programmer and a great program would seem to be offering creative or novel solutions rather than simply recalling this or that item of discrete knowledge. In that, the group's evaluation instrument seems more of a test of coding skill than programming skill, which is the difference between knowing syntax and standards and doing actual science or creatively solving problems, which one could argue is the only way to solve a problem. Why is recreating Minesweeper or implementing a clock using object-oriented design a more apt test of programming skill than completing an inductive correctness proof or demonstrating the big-O efficiency of an algorithm ? Maybe that's just idealism, but one wonders about the state of things should outside-the-box thinkers wind up weeded out of the mainstream. Knowing how to code is like knowing a foreign language. It doesn't matter much if you have nothing to say.",,Vice,A way of killing creativity in software engineering or of weeding out code-posers?,This Automated Tool for Judging Programming Ability Is Kind of Ominous,https://www.vice.com/en_us/article/4x3qkg/this-automated-tool-for-judging-programming-ability-is-kind-of-ominous
Jason Koebler,"​The best limit Texas Hold'Em poker player in the world is a robot. Given enough hands, it will never, ever lose, regardless of what its opponent does or which cards it is dealt. Poker being what it is, the robot, named Cepheus after a constellation in the northern hemisphere , will lose if it's dealt an inferior hand, but it will minimize its losses as best as is mathematically possible and will slowly but surely take your money by making the ""perfect"" decision in any given scenario. Heads-up limit Hold'Em, it can be said, has been ""solved."" Heads-up limit Hold'Em is a type of poker in which only certain amounts of money can be bet during certain times of the game. It's far less popular (and less complex) than no limit poker, in which bets are only limited by how much money a player has (meaning there are many more decisions in the game). Cepheus in action. Screengrab: University of Alberta And it was solved by computer scientists at the University of Alberta who don't actually play the game. That's because solving the game is more of a math problem than anything else. ""You can play one hand of poker and there are hands that Cepheus will fold, and it loses. That's poker. But poker is about how you do in the long run and, if you play long enough, Cepheus will never lose,"" Neil Birch, cocreator of the robot, told me. ""It doesn't make mistakes."" That's the key, of course. Birch and his colleagues essentially ""brute forced"" the game of limit poker, in which there are roughly 3 x 10^14 possible decisions. That, ​according to some estimates , is more possible permutations than hands of poker than have ever been played in human history. ""Poker is big enough that just to specify a strategy—to say how we should play each situation—is as big, maybe bigger than the total number of card games people have ever played,"" Birch said. Cepheus runs through a massive table of all of these possible permutations of the game—the table itself is 11 terabytes of data—and decides what the best move is. Image: University of Alberta In an analysis ​paper published in Science , Tuomas Sandholm of Carnegie Mellon wrote that ""it cannot be beaten with statistical significance in a lifetime."" Birch said that if he, someone who is very bad at poker, were to play against a professional poker player, the professional poker player could possibly end up winning more money than if Birch were to play against Cepheus. That's because human poker players are often trying to maximize on the mistakes of their opponents in doing so, that human player can end up winning big with larger bets, but could also miscalculate and end up losing. Cepheus, meanwhile, is just trying to make the mathematically logical play, every single hand, regardless of opponent and is unlikely to overly penalize other players for their mistakes with large bets. If two Cepheus machines play, the winner will be whoever ends up getting the best cards, over the time period the two play. ""No matter who it's playing, Cepheus will always be a little better,"" he said. ""When it's playing me, on the off chance that I might be a good player, it's careful to still not make mistakes."" In that sense, it's the ""perfect"" poker player, but it might not be the ""optimal"" one—meaning, someone who hammers his opponent's mistakes to maximize the money you can take from them. When I played against Cepheus, I won a couple hands, I lost a couple hands, and then I lost many, many more. I also had the sense I was being hustled. Still, it's probably not a bad way to make a living. Cepheus is a major breakthrough in artificial intelligence. It's one of the first times that a real ""imperfect information"" game has been solved—that is, the robot performs perfectly without knowing the cards its opponent has. It's a greater achievement, from an artificial intelligence and computing perspective, than creating a robot that is extremely good at checkers or chess, according to a paper published by Richard Carter of the University of Edinburgh in 2007. Sandholm, in his analysis, agreed: ""This is, to my knowledge, the largest imperfect-information game essentially solved to date, and the first one competitively played by humans that has now been essentially solved,"" Sandholm wrote. To do this, the overall game is chopped down into many more smaller sets of decisions—each choice to call, raise, or fold—and is checked against the table. Essentially, what the opponent does doesn't really matter—there is always a ""perfect"" way to play, Birch told me. ""It's a static, fixed strategy that guarantees that, no matter who the opponent is, it'll do at least as well as it would have done if the opponent had been perfect,"" he said. ""And the opponent, no matter how good they are, isn't going to be perfect."" One ""mistake"" that his team has mathematically proven is a mistake (which has long been suspected by professional poker players) is the idea of ""calling"" the blinds (which is a small bet at the beginning of each hand): ""The answer is, you never call the blinds. You fold or you raise. That is the mathematically correct thing to do,"" he said. Because the paper (it's ​also published in Science ) hasn't come out yet, Birch hasn't had the chance to get feedback from the massive online poker community, but he said that one professional poker player who reviewed the team's work said he wasn't surprised that a strategy like this exists. It has long been rumored that limit poker could be ""solved."" So, is online poker now dead? Destined to be crushed by robots? Not quite: No limit Texas Hold'Em—in which any amount of bet in any dollar amount can be made—is by far the most popular, and while robots can play that game quite well, we're no where close to solving it. Limit poker has roughly 3 x 10^14 permutations; no limit poker has 3 x 10^48, which is many orders of magnitude harder to solve. ""Directly solving that problem seems unlikely to be feasible in even the distant future,"" Birch said. ""That's not to say we're not working on it. We can't completely brute force the thing, so you have to add in specific poker knowledge. You can play very very well, but you lose that guarantee of being perfect."" You can play, and get crushed by, Cepheus here .",,Vice,"Researchers have 'solved' Texas Hold'Em and it will beat a human every time, given enough hands.",This Robot Is the Best Limit Texas Hold'Em Player in the World,https://www.vice.com/en_us/article/ypwq9g/this-robot-is-the-best-limit-texas-holdem-player-in-the-world
 ,"In September of 1997, the USS Yorktown , a US Navy warship, experienced ""an engineering local area network casualty"" during maneuvers off the coast of Virginia. The missile cruiser, a prototype for the Navy's PC-based Smart Ship program, had quite suddenly become dead in the water, with its propulsion systems rendered useless. Reportedly, the ship drifted helplessly for over two hours before the sailors managed to regain control. The enemy in this case was a single ""0."" The Yorktown 's Smart Ship setup consisted of 27 200-MHz Pentium Pro PC computers running Windows NT and connected via a high-speed fiber optic network. The goal was automation, with the computers replacing about 10 percent of the cruiser's sailors and, thus, saving the Navy some $2.8 million a year. The failure was due to a human crew member entering the number 0 into a database entry field. This led to the computer attempting to divide by 0 and crashing as the result of a subsequent buffer overrun, a computing error in which a system begins overwriting already-allocated memory slots, with all sorts of possibly undesirable outcomes, such as rendering a warship useless. The error spread throughout the network. Division by zero is an peculiar thing, but especially so within computing. If, within the execution of a given bit of software, a computer encounters an attempt to divide by 0, that 0 can act like a stick jabbed through the spokes of a bicycle: one thing stops executing (the wheel), then another thing that depends on the first thing (the bike frame itself) stops, and then, finally, the person on the bike just gets tossed off as the whole bike fails. So, hopefully the programmer has ensured that division by zero just isn't possible within their bit of software, but programming languages themselves often act as a backup just in case. So, if some Java program detects a zero in the wrong place, it will throw what's called an exception, which is bad but not as bad. Given an exception, rather than allowing the illegal zero to go through and break everything, the computer's whole current state of code execution is saved so that the problem can be fixed and execution can go on normally afterward, with nothing actually busted. If division by zero is encountered by a more stripped-down, ""fundamental"" (lower level) language like C or C++, the result is ""undefined behavior."" It all depends on the context of the division itself—what depends on that particular division and why?—and the result might be no problems at all or a foundering warship. What exactly is the problem with division by zero in the very first place? Our intuition tells us first off that division by 0 is pointless or absurd—dividing something into nothing just isn't a real concept. And yet we are allowed to divide by plenty of other things that might also seem absurd, like irrational numbers such as pi and the square root of 2. We can also legally divide by infinity or negative infinity. And then there are the imaginary numbers, a collection of numbers used in math that aren't allowed to exist in the real world of numbers and counting. There is no square root of -1, but we do it anyway. To see the problem, we need to restate what division even is and look at it through the lens of multiplication, which division can be viewed as a restatement or rearrangement of. So, if you had the division 10 / 5 = 2, algebraically that's the same thing as 10 = (5) * (2), right? The two equations say the exact same thing. Now try, 10 / 0 = x, where x is any value imaginable. That would mean that x * 0 is 10, but we know that 0 times anything is just 0. The result is a contradiction, which is the actual mathematical proof of division by zero being invalid. Division by zero has no meaning within the very definition of division. The proof can be restated in even more intuitive terms. Think about what division is, just the word ""division."" It's separating things from other things, a special case of subtraction. Imagine if you were given 10 dollar bills with the instructions that you are to take these 10 bills into the next room and distribute them evenly among the people in that room. You will give every person x number of bills, subtracting that number from the total, starting at 10. You will have finished the division task only when the bills have been divided, naturally. This seems like an easy enough thing to do. Or, rather, it would be easy if there were any people at all in the room. It's empty (and you don't count). How do you divide a number of things among no things? Indeed, when we say that division by zero is undefined, it's meant in the most literal sense. Division by zero has no meaning within the definition of division. It's fun to think about. As thought-dessert, consider one special case of sorts. Back when we were talking about computers and 0, something got left out. Zero is illegal usually only for the integer data type. That is, when we're dividing whole numbers with no leftover decimals or fractions. Decimal numbers, known as floating points, often have a sort of fudge built-in. So, if we're dealing with the floating point data type, the language will allow not only zeros but positive and negative zeros. It works out as such: division by positive 0 yields positive infinity, while division by negative zero yields negative infinity The point of this is to ensure that positive and negative signs are preserved when a program is dealing with very, very, very small non-zero numbers. It's possible for a number to be so small, with so many leading zeros, that the data type can't hold it all within its allocated memory and instead winds up storing just a bunch of zeros, with the numerical substance trimmed off. The result of division by such a small number would likewise be too big to store, so we get infinity. It kind of makes sense.",,Vice,Why We're Not Allowed To Divide By Zero,This story is over 5 years old,https://www.vice.com/en_us/article/3dk58b/why-were-not-allowed-to-divide-by-zero
 ,"All photos from CrossCountry Canada screenshots  This article originally appeared in VICE Canada  In today's video games, you can choose to assume the role of a high-powered military dynamo in the Call of Duty series to murder other soldiers in glorious HD, or you hop into the boots of an alien in Destiny to zoom across moonscapes with a jetpack and a space rifle. But in the glory days of the Canadian edutainment software industry in 1991, the name of the game was delivering potash from Saskatoon to Winnipeg in the most subdued and frustrating manner possible. If you grew up in Canada's elementary school system in the 90s, there's a good chance you sat in front of a text-command-driven truck driving simulator called CrossCountry Canada . The weird thing about the game is that most people who played it seemed to have fond memories of it. Sure, the concept makes a rudimentary flight simulator feel like Quake , but there was something oddly soothing and patriotic about taking the helm of an 18-wheeler and traveling across Canada bringing valuable goods from city to city. Given that I hadn't played CrossCountry Canada in about 15 years, I was excited to power up the emulated version on the Internet Archive, which—unless you've been living under an internet-free rock for the past few days—just published over 2,000 MS-DOS games that are free to play in your browser. I immediately felt a rush of nostalgia as I saw the CrossCountry Canada title screen pop up in my browser, and was excited to enter my name of choice: Thrillho . Once Thrillho was strapped into his 18-wheeler, I was presented with a map of Canada and a blinking command prompt. I wasn't sure what to do, but I knew I wanted to drive north to deliver some gold in Manitoba. I tried entering ""N"" to go north. Oh, of course my fictional MS-DOS truck isn't turned on. My bad. Now that my truck was on and spewing virtual pollution into a virtual Canada, I was ready to get moving. I tried ""drive north"" instead of just ""N,"" and the game kicked into gear. I was on the road in Winnipeg, feeling the wind in my hair and presumably cranking some sweet CanCon tunes on the road. But it wasn't long before the game threw a wrench in my plans and presented an unexpected challenge: My headlights were off, and I wasn't driving safely. Clearly the game wasn't about to let me disobey the rules of the road, so I acquiesced and fired up my headlights. Just like Future told me to do in his hit song , I turned on the lights. Unsurprisingly, turning on my headlights wasn't going to be the last challenge that CrossCountry Canada would throw my way. Far from it. I guess I was turning up too hard, cranking up the radio as I tore through Winnipeg: I got into an accident. Unfortunately, I couldn't figure out what the hell the game wanted me to do. It told me I had to get to a gas station to get my truck repaired, but I couldn't get to the gas station without getting my truck repaired. I was stuck in an infinite feedback loop. The game had beaten me. Frustrated, I shut down the game and got in touch with one of the game's programmers, a blues artist from Vancouver named Jimfre Bacal . Bacal's primary duty in the development of CrossCountry Canada was to port the original version from Mac to PC. He told me the game intentionally did not provide its users with the language needed to control the game, and that the developers caught a bit of flak for that. But I suppose trying to figure out how to play the damn game is where the edutainment comes from. Bacal said programming edutainment for ""little pipsqueaks"" was a ""garbage-heap of money"" in terms of salaries for programmers in the mid to late 80s. He clarified that point by saying: ""There was a 'newness' about educational software during that era and sales were low compared to today's video game sales. Big companies mostly shied away from school software and many small independent software companies competed against each other to fill the void. By the 90s, gaming software had become the big thing with the introduction of game boxes that supplanted desktop computers as the main target for game companies."" Despite Bacal's somewhat gruff comments about his experience coding CrossCountry Canada , it seems the nobility of making edutainment software for kids has left him with a strong impression about the possibilities of software development. Playing CrossCountry Canada even for a few minutes on the Internet Archive brought back some great memories of going on potash runs, via MS-DOS, through the Canadian landscape. And while he might use metaphors about garbage to talk about his pay at that time, the CrossCountry Canada experience provided a rare, oddly patriotic relief from the ennui of elementary school life on the ancient computers that populated Canadian school boards.  Follow Patrick McGuire on Twitter .",,Vice,Experience the Ennui of Canadian Truck Drivers with This Computer Game from 1991,tech,https://www.vice.com/en_us/article/ppmjk9/crosscountry-canada-is-now-free-to-play-online-384
 ,"Choreographer Huang Yi’ s dance partner is neither classically trained, naturally graceful, nor human. In the past few years, we've witnessed a fair share of dexterous robots and diva drones . Nevertheless, Huang’s performance differentiates itself with an good old-fashioned pull on the heartstrings. Known as KUKA, his robot isn't just a coworker—he's Huang’s childhood dream, realized with the help of German industrial engineering, and programmed by the artist himself. Next week, Huang and KUKA will perform their award winning duet of modern dance and mechanical engineering on the stage of 3-Legged Dog Art and Technology Center (3LD). Following the first official performance of Huang Yi & KUKA at Ars Electronic in 2013, the pair became a part of 3LD’s Artist Residency and 3LD/3D+ programs, and were able to expand their show into a full-length production. Throughout the performance, KUKA responds fluidly (by robot standards, at least) to Bach’s ""Parita for solo violin and original additions"" by Ryoichi Kurokawa. As the dance progresses, Huang and KUKA slip in and out of sync, mirroring the mutable relationship between man and machine. Huang Yi & KUKA opens on February 11 and runs until February 17. Learn more about the Huang, KUKA, and their daring performance on Huang Yi’s artist’s page and the 3LD’s website . Related:  Choreographed Robots Are Tap Dancing Their Way Through London  [Exclusive] Robots And Choreography Abound In Update To Ballet Masterpiece  Japanese Dance Company Choreographs Performance With Drones",,Vice,Human-Robot Choreography Marries Modern Dance and Machinery,Entertainment,https://www.vice.com/en_us/article/8qvxy3/human-robot-choreography-marries-modern-dance-and-machinery-5899ce3d80ec534e74c167b4
Jason Koebler,"​Yesterday, Barack Obama became the first pr​esident to ever ""write a computer program "" (at least publicly) as part of a White House event. But now that Obama has mastered JavaScript, can we expect him to fix any future bugs on Healthcare.gov? Not quite. Obama didn't actually write a computer program. Instead, ​he wrote one single line of JavaScript that moved Elsa , a cartoon character from the Disney movie Frozen, forward exactly 100 pixels. That's according to Hadi Partovi, CEO of Code.org, the group that hosted the hour-long basic coding le​sson for 20 elementary school students (and the President). The initiative challenges teache​rs around the country to teach their students coding skills . Here's what Obama's code looked like:  moveForward(100); ""Hello World!,"" it ain't. There's not all that much to analyze here, but Obama apparently has good coding instincts. The President typed the code himself after a short tutorial, Partovi wrote on the quest​ion-and-answer site Quora . Obama knew that JavaScript is a case-sensitive language. He also apparently knew that if you don't close your parentheses, you tend to screw things up. And if you leave out the semicolon, the computer doesn't know that the line of code is done. Or maybe he didn't know any of that, and is just good at following instructions. ""The President asked if he needs to type the F in upper-case, and he got the () and the ; right too, he was very precise and didn't make a typing mistake,"" Partovi wrote.  The code is ""not rocket science,"" Partovi wrote, but that's the point (Obama used an as-of-yet unreleased version of Patovi's coding tutorial, apparently). ""That's how computer science starts,"" he said. ""You don't write a fully-fledged game when you write your very first line of code."" He's right, of course. And it's probably worth getting children into the the idea that they can code. In general, more people should learn how to code, or at least know how it works. But as for the idea that everyone should learn how to code? Well, Obama should probably keep running the country for the time being, at least.",,Vice,President Obama became the first president to publicly code—here’s what he wrote.,An Analysis of Barack Obama’s First Line of JavaScript,https://www.vice.com/en_us/article/kbznd9/an-analysis-of-barack-obamas-first-line-of-javascript
Jason Koebler,"The latest thing you can watch on YouTube or Twitch, a site where you can watch other people play video games, isn't a League of Legends match or a Starcraft championship. Instead, it's a dude, sitting in his house in Russia, staring at a black screen, writing thousands of lines of Python as he tries to create a new search engine.  Welcome to the world of Watch People Code , a premise which couldn't possibly be any more straightforward and is exactly what it sounds like. It's an idea that hasn't exactly taken off yet, but that has at least gotten a bit of a following. The associated subreddit that it grew out of has garnered nearly 5,000 subscribers in less than a month, and at any given time, you can, well, watch someone code.  On a recent day, I watched a guy program a dartboard game on Twitch, a search engine on YouTube, and a program to connect League of Legends players with each other. At any given time, there's at least someone to watch code, and while I confess I didn't stick around long enough in any given feed to pick up any tips of my own, its boosters say it's a good way to improve your coding.      ""We're kind of puzzled that it took off this fast, but it's not boring. Streamers provide commentary about what they're doing, and there's always a lot of stuff going on in the chat,"" Alexander Putilin, the Russian programmer working on a search engine told me. ""And, as a streamer, I've actually learned from some of the viewers who asked me a question about what I was doing or who offered helpful advice.""        Putilin's search engine. Screengrab: Alexander Putilin   And maybe we shouldn't be so surprised it's taken off. When Twitch originally launched in 2011, the dominant narrative was this is so stupid—who would want to watch someone else play a video game, when you can just do it yourself? Well, Twitch is now the top livestreaming video site in the world, and more than 55 million people watch Twitch streams each month. The site was even home to a veritable cultural phenomenon, Twitch Plays Pokemon , in which thousands of people controlled a Pokemon master using keystrokes programmed in the site's chat box. Twitch sold for nearly a billion dollars to Amazon last year .  So if people like watching other gamers defeat a hard boss, maybe it's only logical that a programmer might enjoy watching a colleague solve a tough problem.  Putilin runs the Watch People Code subreddit and website with a friend and his girlfriend, and it's kind of taken on a life of its own already. His ""Building a Search Engine"" series now has 11 archived episodes dating back to early November. Some of them are short—a quick explainer is just two minutes. Others, such as a two hour session spent building the backbone of the engine or an hour spent only fixing bugs, require a bit more patience on the part of the viewer.  The plan, he says, is to teach people, but it's also to create a better search engine than the one reddit currently provides, which is notoriously terrible.      If this video isn't loading, the coder is offline. More here .  ""I wanted to demystify things for people,"" he said. ""But I also wanted to see if I could make an easier way to search reddit.""  Putilin has even organized his coding sessions into ""seasons"" and episodes, as you would a drama. So is there much entertainment value here? Not unless you're super into coding, as you'd expect.   A recent thread on reddit asked viewers why in the world they're spending their time watching someone else write a bunch of lines of computer code, and the responses were surprisingly varied: Some wanted to watch other people's setups, to see what they could change about their own; others hoped to learn by being immersed in something, rather than starting from the ground up; others were simply scouting for new projects. And then, a couple people said they did it simply because they thought it was fun.  ""There's a certain unpredictability to it—I don't know if I'd call it drama, but it's interesting to watch people struggle and think for 10 minutes and then figure out what they need to do,"" Putilin said. ""I think that's really awesome to watch.""",,Vice,'Watch People Code' is the weird internet's latest phenomenon.,Thousands of People Are Watching This Guy Code a Search Engine,https://www.vice.com/en_us/article/pgax4n/thousands-of-people-are-watching-this-guy-code-a-search-engine
Michael Byrne,"​The problem with code is that it's too good. This isn't to say that it solves its respective problems in the best and most efficient ways possible, or that it's guaranteed to achieve modular perfection. It's just that code lasts. The guiding principle of computer engineering is backwards compatibility and, for programming, this translates roughly into reusability. Quality or even just well-functioning code should never be rewritten. The logical result of this is what Rough Type's Nicholas Carr offers as ​""peak code."" It's detailed in ​a recent paper by a trio of Boston University economists working with Earth Institute founder Jeffrey Sachs, who modeled a theorized programming future in which programmers had made themselves redundant by being too good at, well, programming. The best code by the best programmer doesn't need to be rewritten, and so that task is eliminated from the marketplace—along with the programmer. ""Our simulated economy is bare bones,"" Sachs and co. write. ""It features two types of workers consuming two goods for two periods. Yet it admits a large range of dynamic outcomes, some of which are quite unpleasant."" This is the result of ""code accumulation,"" in which past programmers wind up competing with current and future programmers. It's easy enough to imagine them as the same people. These robots contain the stuff of humans—accumulated brain and saving power. The result, the economists explain, is a bust—or, at best, a boom-bust cycle—to occur at some point in the not-terribly-distant future. ""The combination of code and capital that produce goods constitutes, in effect, smart machines, aka robots,"" Sachs and his group write. ""And these robots contain the stuff of humans—accumulated brain and saving power. Take Junior—the reigning World Computer Chess Champion. Junior can beat every current and, possibly, every future human on the planet. Consequently, his old code has largely put new chess programmers out of business."" As the authors explain, the situation only grows worse the less proprietary software becomes, e.g. open-source is part of the problem. Which all adds up to something cynical as hell, but with some amount of truth to it. That said, the counterargument is found in Sachs' own chess example. Chess stays the same. The rules are the same, always. Programming chess bots is a highly idealized situation, in which the resulting software exists in a static environment. A chess game will be a chess game, tomorrow or a hundred years from now. Real-world code seldom occurs in static environments, however. And this is why ​software rot exists. The environment in which code operates is dynamic and will continue to be so. The result of this is a sort of decay. Code becomes less and less well-suited for emerging conditions and it develops bugs and deep inefficiencies. As rot advances, static software becomes error-prone and, indeed, ambles toward obsolescence. We can't program around every future. And in ideal worlds, we might extend Sachs argument to anything at all: bridges, recipes, art, architecture. What's to say that the situation is so different from that of an architect or civil engineer? Is there some similar anxiety in those fields? What if I make this bridge too good?  Is there something about code that sets it apart from the larger world of ""making things?"" If there is, it could only matter in an unchanging world. Mathematical logic is surely more resilient than cement and aesthetics, but resilience only goes so far in a dynamic world. The ground moves and concrete cracks. So it is with software.",,Vice,A group of economists offers a cynical take on the programming future.,Are We Headed for 'Peak Code'? Not Likely,https://www.vice.com/en_us/article/vvbxk8/are-we-headed-for-peak-code-not-likely
Zack Kotzer,"When Sony introduced the PlayStation 3 back in 2005 during an E3 keynote, the demonstration had a few staples. Space marines, explosions with dancing debris—the usual. However, one of the most memorable tech demos wasn't centered on any kind of violence at all, but something much more impressive: a rubber ducky. "" Here's my duck, stunningly rendered in real time ,"" said Phil Harrison, then-executive vice president of Sony Computer Entertainment Europe. ""We have the ability to render the water in great detail. As I jump the duck in and out you can see all the caustics and effects that are being rendered in real time on this fantastic HD display."" As Harrison spoke, the duck scooted about a tub, leaving behind a tiny trail of precious shockwaves, water splashing against the sides whenever the duck drifted by. Harrison then filled the tub with more ducks until it looks like a tiled bag of popcorn, but the audience really started to holler when two glass cups started pouring water back and forth. When video games want to dazzle you, they don't use dragons or giant bugs, because fantastical creatures don't need to prove themselves. Rather, realism is the spectacle—our oceans, rivers, puddles and swimming pools shimmering in real time. Water comes up so often in graphical discussion it's practically cliché. It's probably no coincidence then that Nintendo's splashy jet ski series Wave Race 64 came out only a month after the Nintendo 64 launched, and it's sequel Blue Storm on the same day as the GameCube. It's also not surprising that Crytek's first gaes, Far Cry and Crysis, took place on tropical islands with bright blue beaches. Water is the unlikely element that games have always used as a measure of performance because it's so hard to get it right. ""Often we use fluids in tech demos because it is so computationally demanding. But the result is something beautiful that anyone can appreciate,"" said Miles Macklin, a physics programmer who works at the graphics technology company NVIDIA on the PhysX team. ""One reason we find fluids so captivating is that they show detail and complexity at a wide range of length scales. For example, large vortices will dissipate into smaller vortices, and eventually turbulence; larger bodies of water will split into many smaller droplets, etc. Capturing these effects is difficult, but when it is done well we find it particularly visually pleasing,"" Macklin explained. It's not as if we don't appreciate other forces of nature and their graphical nuances. The drifts of snow in Killzone 3 or the sand dunes of Journey are stunning, but neither are as heralded or as often used—plus water covers about 71 percent of the world , so it's no surprised it's so often used. Uncharted even had a rendering engine dedicated specifically to the wet stuff—one that developers Naughty Dog took great pride in. ""Our water shader not only uses the same basic reflection and refraction techniques as other games, but uniquely models water flow effectively,"" wrote Neil Druckmann and Richard Lemarchand in a post mortem for Gamasutra . ""Color is computed not from textures but by using optical and physical principles, some based on water depth, which helps give our water a very realistic look. We did all of our water effects -- foam, water bubbles from churn, and silt -- in the shader, which also helped our water ""sell."" All in all, we were very happy that we not only caught up in terms of graphics technologies, but even helped raise the bar."" Indeed, when Uncharted came out, it felt as if its flooding chambers, misty waterfalls and the damp shirt of lead character Nathan Drake were the most discussed parts of one of the PlayStation's biggest blockbusters. Even with today's processing power, making water isn't as easy as turning on the tap. Those in the industry know that water is a frustrating material to program. It bubbles and splashes, it fills whatever it's poured in until it spills over. It rains, it soaks, and it has volume—which according to Macklin is one of the most troublesome qualities of all. ""The major challenge for water in games is performance,"" said Macklin. ""Good algorithms are well known for simulating water, but they are often too slow for real-time applications. Often the major computational cost is in maintaining fluid incompressibility—that is, the requirement that the fluid does not change its volume. This is a fundamental property of water. Depending on the technique used, if incompressibility is not maintained then you may notice mass loss, which appears as water unnaturally evaporating into thin air."" It's hard to say how much longer the race for realistic waves will continue. The goal for games in general now no longer seems to be photorealism, but rather, other kinds of ways to cause your jaw to drop—gameplay, originality, atmosphere. In fact, when trailers and tech demos come out nowadays, the reactions usually aren't stunned as much as skeptical. Hydrophobia tried to sell itself on its water rendering merits, but the gameplay itself was a wash. In fact, Macklin's says the most memorable water effects for him aren't the crashing waves of Assassin's Creed IV: Black Flag or Far Cry's tropical blue shores, but the heavily stylized waves of The Legend of Zelda: Wind Waker. The 2002 Nintendo GameCube game was cel-shaded and cartoony, sure, but it used inspired visual cues to convey a feeling of movement along the open seas. Realistic water, on the other hand, will likely always be difficult to render in real time. And just like the uncanny awkwardness of virtual faces, we tend to notice when water is off—even just a little. This story is part of The Building Blocks of Everything, a series of science and technology stories on the theme of materials. Check out more here: http://motherboard.tv/building-blocks-of-everything",,Vice,"Even with today’s processing power, having good virtual water isn’t as easy as turning on the tap. ",Rendering Realistic Water Is Still Game Development's Moby Dick,https://www.vice.com/en_us/article/z4m885/rendering-realistic-water-is-still-game-developments-moby-dick
Michael Byrne,"​GitHub is becoming an oddly popular place given its role as a vast storehouse of oft inscrutable computer code and technical information. It makes sense though, as programming itself is integrating itself into pop culture in wholly unexpected ways so too should one of its most crucial meeting grounds. Thus, it seems like an opportune time to talk about what ​Git actually is, as one of the most powerful shadow technologies in computer science. Allow me to rave for a moment. Git is awesome, one of the more profound and useful tools in programming even. Git is also weird and kind of confusing if one doesn't happen to be directly involved in code and the processes behind code production and the difficulties of managing potentially hundreds of files being worked on by just as many programmers and engineers. First of all, GitHub is not Git. I see this confusion a lot. GitHub is a place on the internet for storing the files and file structures organized according to the Git system. The Git system is a type of version control, surveilling your stuff for updates and deletions and making sure you or someone else on your project doesn't do anything stupid. It watches your files for changes and allows for even simple programming projects of one or two files to be shared among large amounts of people. It does this almost invisibly. In a sense it's just a way of saving things in an alternate universe. I use Git all the time without ""pushing"" (which is like uploading) my documents to GitHub. My Git repository (a collection of files usually pertaining to one project) in that case is just on my hard drive. The utility of Git is in branching and merging documents. If I have a project with a dozen files that are all interdependent on each other in crucial ways, making edits to any of those files becomes something of a risky business, particularly if other people happen to be working on those files too, and even more so if the project is ""live"" as a website. Last week, I was working on a kind of dump map thing that was a part of a website and I wanted to add a cool new feature, but the site couldn't be disrupted. I made a branch of the project, worked on it, tested and tried to break it a bunch of times, and then, once satisfied, I merged it with the master and replaced the files on the server. So, instead of working on the main ""master"" version, I check out a clone of the master version. I can screw around with this all I like without it interfering with the master documents and when it's totally ready, I (or we) can take my changes and merge them with the master project. So, in a sense it's just a way of saving things in an alternate universe. Linus Torvalds. Image: ​Alex Dawson/Flickr Git was created by Linus Torvalds, of Linux fame. He needed something reasonably quick that could handle the near-constant barrage of activity involved in working on something like Linux. It also needed to be free. Not being able to find it, he created Git. Like Linux, Torvalds named it after him​self ; in British slang, Git means one that is ""pig headed, think[s] they are always correct, argumentative,"" according to the Git WIki. The thing that really makes Git work is in its ability to detect changes to files—not just that a change has occurred, but where and what change occurred. A Git tracked file has its contents hashed using the SHA-​1 encryption algorithm , resulting in a 40 (hex) character blob. When this blob is different than the last blob Git observed, it registers the file as ""modified"" and the developer can proceed accordingly (committing the changes to the master or whatever). All of this seems really simple, and it manages to stay that simple as projects grow enormously. Thanks again, Linus.",,Vice,"First of all, it's not a website.",What Is Git? An Explainer,https://www.vice.com/en_us/article/d73e37/what-is-git-an-explainer
 ,"In programmer Will Gallia ’s Space Filling Portraits , complex mathematics become minimalistic portraits that even the most left-brained of us can enjoy. Using a Roland DXY 980 pen plotter custom-programmed with OpenFrameworks, Gallia depicts five mathematicians' portraits out of the space-filling curves they invented. In simple colors on plain white paper, the programmer “takes a raster image, posterizes it using K-means [clustering, a signal processing technique], then plots the path of a space-filling curve in different pens for the shades.” Gallia’s illustrations honor mathematical theorists David Hilbert, Claude Shannon, John Von Neumann, Bill Gosper, and, in a tri-colored tribute, Giuseppe Peano , the father of space-filling curves. Watch as Gallia and his Roland fill in the beards and bushy eyebrows of these famous math minds, below: Claude Shannon John Von Neumann Bill Gosper Giuseppe Peano David Hilbert A portfolio of more of Gallia’s pen plotter work can be found on his Work by Roland Tumblr page. Related: Man vs. Machine 'Connect Four' Games Become Beautiful Line Drawings  ""Art Of Fraud"" Showcases The Simple Infographic Beauty Of Malware Visualizing the Infinite Beauty Of Pi And Other Numbers",,Vice,Machine Draws Famous Mathematicians with the Math They Invented,Entertainment,https://www.vice.com/en_us/article/53wye5/machine-draws-famous-mathematicians-with-the-math-they-invented
Beckett Mufson,"A massive, 25 lb. portrait of Albert Einsten made from 2200 dice could be yours, thanks to redditor and programmer Joshie196 . ""A year or two ago, I wanted to make a program that converted any images into dice. I went ahead and made a neat program, but left it at that,"" he says in a Reddit thread . ""About 2 months ago...I decided I wanted to actually try and make one of the mosaics that it produced. I ordered some dice from China and happily waited, and the rest you can see."" The Einstein quote that inspired this project, ""As I have said so many times, God doesn't play dice with the world,"" will be inscribed on the final project, which Joshie196 says he may be interested in turning into a real business, if people are interested in buying them. While he preferred not to release his program to the web, Joshie196 recommends Mosaicify for people who want to make similar artworks. Check out his process in the images below. Via Reddit Related: Transform Any Image Into An Emoji Mosaic Watch 24 Hours of News Morph Into Stunning Video Mosaic Meet The Scientist Growing Celebrity Portraits Out Of Bacteria",,Vice,A redditor combined 2200 black and white cubes into one mathematical super genius.,This Exists: Giant Einstein Portrait Made of Dice,https://www.vice.com/en_us/article/bmyvkw/this-exists-giant-einstein-portrait-made-of-dice
Michael Byrne,"Tearing up the internet right now is some dude's wan extended dick joke in the form of a Github repository called ​DICSS . It's a bunch of stupid attempted puns smashed together in the form of a JavaScript project promising ""directly injected CSS."" Lulz include attempts at fourth-grade boy humor like ... ( ({})<=8 ) { // yeah, you get it... } and ... DICSS keeps on growing! Some popular DICSS add-ons include COCSS (cross-origin DICSS), SECSS (security enhanced DICSS) *DICSS-Pics (DICSS plugin for working with images) Circum-Sizer (calculate the circumference of a rounded element in CSS) It goes on like that across several files. In ​a comment thread attached to the project, a GH user pointed out, ""You guys seem to think this is funny. But it's really not. It's harmful to the developer community."" And so a familiar battle ensued (is ensuing) between the internet's legion of man-boys and those who man-boys like to call ""SJWs"" (for ""social justice warriors""). The SJWs seem to think that Github should be a neutral platform for programmers to collaborate on projects and remotely store their git repositories, while the man-boys think it should be a safe place to be all sweaty and uncomfortable to be around. Can you even believe it? Some commenters have suggested that the project makes light of sexual violence. Its read-me file contains the line, ""And please, never force someone to use DICSS if they're not willing,"" which is an amazing indicator of just how tone-deaf and deluded this whole crowd is. A line meant to disclaim the creepiness of the joke that winds up being one of the creepiest lines in the whole thing! That's impressive. The standard retort in said comment thread is that said line proves without a doubt that the joke is in fact very female-friendly. Because putting someone in any kind of situation or circumstance where it might be wise to reassure them that ""hey it's cool I'm not going to rape you"" is totally cool so long as you say the magic words. Whatever. It's creepy-dude humor, which is Family Guy humor: winks, leers, barf. And, yeah, this is not dissonant with the pre-existing tone with the developer world—the JavaScript/front-end developer world in particular (sorry bros!). Business Insider UK interviewed the project's creator, Randy Hunt (user ""letsgetrandy"") ​because someone had to . Hunt brushed off the criticism in the usual way: ""People want to be offended. It's reverse privilege."" Reverse. Privilege.",,Vice,Gross.,This Weekend's Big Stupid Thing on the Internet Is a GitHub Repository,https://www.vice.com/en_us/article/xywm9w/this-weekends-big-stupid-thing-on-the-internet-is-a-github-repository
 ,"INVESTIGATION and SERVICE and TIME from CSPAN Five on Vimeo . ""SERVICE, SERVICE, SERVICE, INVESTIGATION, SERVICE, TIME, INVESTIGATION..."" Sometimes the use of strong, simple language becomes so favored a tool in politicians' repertoires that it seems like buzz-worthy keywords are the only ones coming out of their mouths. As C-SPAN 5 , the automated C-SPAN re-editing program by Stupid Hackathon founder Sam Lavigne (see This Invention Lets Your Pizza Tweet Every Time You Take a Bite ) proves, they are the only words coming out of politicians' mouths. Programmed in Python using audiogrep and moviepy , C-SPAN 5 takes hour-long blocks of C-SPAN programming, transcribes the audio, and creates a new, 1-2 minute supercut of its actors', anchors', analysts', and experts' most-used words. Via automation, these results are then published online and announced on the @CSPANFive Twitter. The program is only nine days old but has already racked up a cache of 23 different videos. Iran Nuclear Negotiations-393038_1.mp4.edited.mp4.1426915063.mp4 from CSPAN Five on Vimeo . ""Generally speaking, I'm interested in exploring techniques for shrinking down large inventories, lists, and bodies of texts,"" Lavigne tells The Creators Project. ""Reductions, especially programmatic ones, allow us to make unexpected discoveries and can potentially reveal hidden truths about corpora."" In this case, the truth is not-so-hidden: it's almost as if C-SPAN's figures are performing acting exercises wherein the directive is to repeat the same word differently every time. ""I think this is particularly worthwhile when dealing with governmental corpora, which are frequently illegible due to how massive they can be,"" Lavigne continues. ""Releasing information, even in the name of transparency, can itself be an obfuscating strategy. So, programmatic reduction is a kind of counter-strategy."" Watch more footage of politicians repeating themselves on Vimeo , and stay tuned to @CSPANFive for an update every time a new video is published. Click here for more from Sam Lavigne. Related: [Longreads] Additivism: 3D Printing's Call to Action A Beautiful Love Story Between Two Glitch Art Twitter Bots This Sculpture Simulates Politics with Angry, Indecipherable Beeping",,Vice,This C-SPAN Bot Proves Politicians Say the Same Things,Entertainment,https://www.vice.com/en_us/article/gvw5yb/this-c-span-bot-proves-politicians-say-the-same-things
Brian Merchant,"If Instagram were to translate your pictures into words, what might they say? That's (sort of) the question that self-professed language hacker Ross Goodwin has tackled in his latest provocation, ​word.camera . It is, as the name implies, an app that translates any image you take or upload into a pageful of words. I've ​written about Goodwin's work before —he programmed an algorithm that turns text into novels, and we had him ""translate"" the ​CIA torture repor​t into fiction . The results were interesting, if not entirely comprehensible. So last week, when he emailed me with news of his latest experiment, I was immediately piqued. Goodwin's word.camera offers a snapshot into how machines can be taught to interpret, and even describe in human terms, the real-world environment. I ​tried the app with a selfie in the office ; the results were enjoyably beguiling. The first paragraph went on about my European-ness (fair enough, I am of European descent), before veering into a soliloquy about voting, politics, sin, and crime. Word.camera did seem to know that the photo was a portrait, that I was indoors, that I was wearing clothes, and that I was seated in front of a window. The rest was sort of like a horoscope—there were lines about the significance of music and protest, two things that are important to me, personally—you could read into it whatever you like, and perhaps find some seed of truth. (""The music is important to man, and the protest was made from a formal and solemn declaration of objection,"" the machine wrote, and it's hard to disagree!) Try it yourself, ​here . To learn more about how the algorithm works, and what it might portend for the future of machine-human interactions, I sent Goodwin a few questions about his program. When we think about the type of artificial intelligence we'll have in the future, we think of a robot that can describe and interact with its environment with natural language  Motherboard: ​ What's the background here? ​ Ross Goodwin: I recently received a grant from the Future of Storytelling Initiative at NYU, which is funded by Google, to produce a computer generated screenplay. For the past few months, I have been thinking about how to generate text that's more cohesive and realistically descriptive, meaning that it would transition between related topics in a logical fashion and describe a scene that could realistically exist (no ""colorless green ideas sleeping furiously"") in order to making filming the screenplay possible. After playing with the Clarifai API, which uses convolutional neural networks (a very smart machine learning algorithm) to tag images, it occurred to me that including photographs in my input corpus, rather than relying on text alone, could provide those qualities. word.camera is my first attempt at producing that type of generative text. At the moment, the results are not nearly as grammatically correct as I would like them to be, and I'm working on that.  What is the benefit of having a program that generates stories about pictures? ​ This project is about augmenting our creativity and presenting images in a different format, but it's also about creative applications of artificial intelligence technology. I think that when we think about the type of artificial intelligence we'll have in the future, based on what we've read in science fiction novels, we think of a robot that can describe and interact with its environment with natural language. It shouldn't just avoid obstacles and recognize human faces. It should notice the dead pigeon on the sidewalk and make a comment about mortality (or perhaps its lack thereof); it should make witty jokes about the crazy party hat you're wearing; it should notice that your haircut makes you look European. I think that creating the type of AI we imagine in our wildest sci-fi fantasies is not only an engineering problem, but also a design problem that requires a creative approach.  How does it work? What are the stories ""about,"" and how does the algorithm 'know' what to write? ​ It's generous for you to call them ""stories."" I don't really know what to call them. The algorithm is extracting tags from the images using Clarifai's convolutional neural networks, then blowing up those tags into paragraphs using ConceptNet (a lexical relations database developed at MIT) and a flexible template system. It knows what to write because it sees concepts in the image and relates those concepts to other concepts in the ConceptNet database. The template system enables the code to build sentences that appear to connect those concepts together.  The story generated from a picture of my face is all about European-ness. What gives? ​ Your face was algorithmically matched by a convolutional neural network to a large number of other images that were tagged by humans as ""european"". The internal states of the neural network are not human readable, so it's impossible to say for certain what features the algorithm is detecting. If I had to guess, I'd say it's because you have white skin, but I don't know for sure.  Do you have a favorite image-generated story yet? ​ Right now ​it's this one :  Any specific plans for this? ​ I don't have any concrete plans at the moment. My intent was to make an unusual camera that anyone could use on their phone or computer. But I would love to install it in a gallery or museum as a photobooth. (Any gallery/museum owners in New York City who are interested in this should feel free to contact me. My contact information is ​on my website .) I also want to make a talking surveillance camera that moves around and describes what it sees. I have access to some high end programmable surveillance cameras at the moment, and this seems like a good application for them.",,Vice,"If Instagram were to translate your pictures into words, what might they say? This app will tell you.",This App Translates Your Photos Into Stories,https://www.vice.com/en_us/article/8qx7q3/the-app-that-translates-your-pictures-into-words
Michael Byrne,"Python already has a reputation as a classy do-anything programming language—or programming cult , depending on who you ask—with an emphasis on easily readable or at least pretty code and intuitive syntax and behavior. Anecdotally, it's probably the most frequent ""favorite"" language I encounter. I think xkcd sums it up well enough: Image: ​xkcd.com ​ Anyhow, the annual Python conference PyCon is underway this week in Montreal and you can find the full schedule here —it is indeed full, with sessions ranging from ""Data Science in Advertising: Or a future when we love ads"" to ""The Ethical Consequences Of Our Collective Activities"" to ""'Words, words, words': Reading Shakespeare with Python."" I'm excited and, as a diehard fan of the archaic and pointlessly tedious (also: curly braces), I don't even really use Python—though that resistance is crumbling fast. Here's one PyCom session worth sharing (of many), and it illustrates a couple of important things simultaneously: one, simulating a brain is really hard and probably impossible given any conceivable technology and, two, Python rules and its simplicity/intuitiveness do well in unraveling neurocomplexity. With a half-dozen lines of code, you or I could make and observe a neuron—yes, our very own neuron—as it monitors incoming currents and sends impulses.  The talk comes courtesy of Trevor Bekolay, a computational neuroscientist at the University of Waterloo. Bekolay introduces several Python tools that enable the simulation of neurons, such as Brian and an adaptation of the classic Neuron simulator software , but, mostly, the Nenga Neural Simulator project, which Bekolay is a developer on. The key difference with Nengo is that it allows a natural and intuitive scaling up of neurological processes. ""At the University of Waterloo, we have been working on a new approach to scaling up brain simulations, and a new Python package to support that approach,"" Bekolay writes in the presentation's abstract . ""Instead of focusing on biologically accurate neurons, we focus on how to connect neurons together such that they can compute interesting functions."" From a computer science perspective, the Nenga approach is pretty interesting. It follows something like the code-compiler-machine instruction process that allows programming languages, even elegant high-level languages like Python, to be translated into the gritty not-so-elegant assembly and machine instructions that actually tell computer hardware what to do. Nenga offers a set of functional (corresponding to functions of the brain) constructions that can be called simply via Python and which correspond to the holy mess that is actual neurology, which might look a bit like a tangle of machine instructions. And so it becomes possible to put together simple-seeming commands and objects in interesting, hopefully relevant ways. As Bekolay notes, this is a potentially powerful tool in the development of new forms of artificial intelligence.",,Vice,An early hit from this week’s PyCon 2015.,How to Make a Human Brain in Python,https://www.vice.com/en_us/article/mgb43p/how-to-make-a-human-brain-in-python
 ,"PI, Alka Cappellazzo. Courtesy the artist A version of this article originally appeared on The Creators Project Italy.  The pattern is the repetition of a graphic, a structure, an ornamental design. For creative coder Alka Cappellazzo , repetition of geometric shapes is a form of expression, whether it is static or a pattern of hypnotic animated shapes that seem to pierce the screen along the z-axis. ""My intention is to explore the nature of the pattern through programming,"" Cappellazzo tells The Creators Project.  She generates her patterns in Processing, building complex and magnetic images from simple geometric bases. ""Through the use of the 'random' and 'noise' functions,"" she explains, ""the results I get are always different and interesting.""  Dimensions, Alka Cappellazzo  On her Do you see a pattern here? Tumblr, you can also find the codes which bring the works to life. ""In my work, code is always visible. For me it is essential to show the nature of the creative process. Making the source code public allows for study and collaboration between artists, programmers and enthusiasts. Art is knowledge, it is not good to build jealously, but to share so that it can live and spread.""    Tune, Alka Cappellazzo  To learn more about Alka Cappellazzo, visit her Tumblr and portfolio on OpenProcessing .  Related:  A Man Is Using Math To Create An Impossible 4D Video Game  Phillip Stearns Glitches Meat for a Year-Long Creative Code Experiment  Say ""Hello World"" to the World's First Algorithm Auction",,Vice,Lose Yourself in a Coded World of Patterns and Geometry,Entertainment,https://www.vice.com/en_us/article/mgp3vp/lose-yourself-in-a-coded-world-of-patterns-and-geometry
Ava Kofman at VICE,"This article originally appeared on VICE. When most people think about the future of wearable technology, they picture Apple Watches and FitBit bracelets that are more affordable and wornas commonly as underwear. However, Becky Stern pictures a jacket whose zipper shuts off the annoying TVs in her favorite bar, a GPS dog harness that track's her dog's run, or a skirt with embedded LED sensors that sparkles as she moves. Then she sews and programs them, and shares for free what she's made and how-to make it in detailed step-by-step tutorials online. Though Stern has been sewing, tinkering, photographing, filming and editing things since at least the age of eight, she didn't combine all of her skills until attending Parsons Design and Tech program. At Parsons she made one of her first electronic craft projects: a set of plush steaks embedded with LEDs . (They symbolized the ""radiation process most American beef goes through during processing."") In graduate school, Stern started producing video tutorials for MAKE —a passion project that eventually turned into a full-time gig. Since dropping out of grad school, Stern's work has been exhibited internationally, featured in books, and viewed hundreds of thousands of time online. She's become something of a celebrity on YouTube, hosting weekly wearable show-and-tells , uploading new videos of inventions, and answering questions as the go-to guru for hacking fashion. Stern puts her expertise to use as the director of wearable electronics at Adafruit , a New York-based factory that manufactures smaller components and shares tutorials so that people can design and program their own open-source gadgets. Wearable computing projects promoted by Stern are turning crafting hobbyists into coders, strengthening the DIY-arm of the wearable revolution. Taking a break from her work designing the future, Stern spoke with me about fashion-friendly tech, data privacy, and LED tiaras.  VICE: You make projects that are useful (the Citi Bike Helmet ), playful (the beating heart headband ), and just plain fun (the firewalker LED sneakers ). Your work feels different from other wearables. How would you describe your process, especially given your DIY approach?  Becky Stern: My work in wearables started long before the first Fitbit. It doesn't aim to be mass-produced, but rather radically customized through the interpretation of instructions I publish. Wearables are so personal that I think it's heavy­ handed to suppose that one [product] will work the same way for everyone. Also, my work is rarely about the final device as much as it is about the design and building of it. Adafruit's wearables hope to reach enthusiastic novices in both craft and tech, and we hope to give those novices confidence, knowledge, and inspiration to build their own technology. This gives folks more agency over all the tech they use, encouraging more critical engagement about the devices we use every day. What's cooler to a little girl: the light up shoes her parent bought her or the accelerometer­laden magic wand that activates color changes on her LED tiara that her parent made with her? DIY philosophy encourages free information sharing and open source. The consumers of the technology are presumed to be interested in becoming builders of that technology, which is very different from the ""we create, you consume"" mentality of Apple and other mass market companies in the wearable space.  Your work is about invention, but it's also about sharing and documenting your process through tutorials. Why make art in online spaces? Why share these tutorials? I get a strange satisfaction from getting ideas and processes out of my head/hands and onto the internet. I've never been able to fully explain my motivation to finish projects and put them online. Readers often give back their own ideas and suggestions; sharing creates an open exchange that's very helpful when I'm stuck or out of my league.  What becomes possible when coding turns into a mainstream hobby? When coding knowledge becomes commonplace, the gap between technology maker and technology user will begin to close. Consumers will have more critical understanding of their devices and online networks, which will undoubtedly change their behavior, whether it decreases unintentional privacy violations or fundamentally changes tech company's product development and marketing campaigns.  Intel announced a collaboration with Fossil on tech­driven accessories , and Google Glass is working with the makers of Ray Ban and Oakley sunglasses to develop more fashionable eyewear . What do you think of tech companies' forays into fashion? These collaborations are inevitable as tech and fashion companies finally learn how to relate to one another. They hardly speak the same language, have vastly different value systems, not to mention business models. You better believe that if there is an activity, there will be a wearable for it soon. I'm sure in the coming years we'll see brilliant ideas executed poorly, and idiotic ideas made mega-popular through great business decisions. I think tech is moving quickly towards the seasonal replacement pattern that fashion has long established, and I find that a bit depressing for the planet, since circuit boards and batteries are far harder to reuse or recycle than out-­of-style garments.  Could DIY wearables help us circumvent some of the problems of data ownership and privacy sidestepped by larger companies? I like the idea that shiny metallic EMF-blocking clothing and accessories could become in vogue because they represent data privacy—this is a technological factor that informs a visual one. Building your own wearables definitely makes you understand more about what the big guys are up to. If you learn how to code your own GPS logger, you become painfully aware that your phone is probably logging your every move and uploading that info to a server somewhere, or at least that it could be doing that without your knowledge or consent. I wouldn't say DIY wearables provide an alternative to the big guys, though, as smartphones and other ""mandatory"" devices of the modern era can't easily be DIY-ed at the level of compact reliability necessary for staying as connected as we like.  How do you get people to want to wear your products? Fortunately I don't have to try to make people want to wear something, I get to try to get them to want to make something. Whether they wear it is often irrelevant to the goal of teaching and helping folks have a good time learning tech and crafts. Creating new use cases that are compelling enough to get people to buy products is a challenge for wearable tech companies: The scenario has to be more compelling than a slick marketing campaign, or the product will fizzle after the first users get tired of it. Motivations for putting something on your body range wildly and are very personal. I think it's easier to put something on your body if you made it yourself than if you bought it—you've already invested a part of yourself in it.  See Becky's products on her website . Related: This Armband Lets DJs Control the Lightshow by Dancing Nightmare Bracelets Created with a New 3D Printing Design Method And Now There's A Wearable Tomato-Dispensing Robot Now You Can Wear Your Aura on Your Sleeve",,Vice,"As coding goes mainstream, Becky Stern is encouraging crafters to make their own wearable tech.",Forget the Apple Watch and Make Your Own Wearable Technology,https://www.vice.com/en_us/article/53wybd/forget-the-apple-watch-and-make-your-own-wearable-technology
Michael Byrne,"Daniel Shiffman, an associate professor with NYU's Interactive Telecommunications Program , got me into a lot of stuff I didn't think I'd care about with his textbook, The Nature of Code . It's about modeling natural systems with programming, generally, but in the process it makes a lot of possibilities in game development, artificial intelligence, the absolutely exploding world of creative coding, and beyond suddenly really clear and accessible. You should pick it up—or download it for a donation . Shiffman is a great explainer and over the past week or so he's been going all out in making tutorial videos for the p5 project, a JavaScript library based off of the immensely popular creative coding language Processing. It's not a strict Processing port—that already exists as Processing.js —but rather a reinterpretation of the language intended to make it even more accessible to artists, designers, teachers, and people just getting started with code. The p5 project itself is being led by artist and programmer Lauren McCarthy . (I even did a short documentation project on p5 for a web development class a while back, but have since burned the evidence.) Part of what I like about Shiffman's approach, and what's probably a benefit of learning a language straight from the source, is that he doesn't ignore the fundamentals or offer shortcuts. This is a route to learn some programming rather than learn how to make a cool thing with some code you found. That's part of the larger point of Processing itself: a starter language, a stepping off point to something more like Java or Python or C++, though it seems like a lot of coders are perfectly content to stick with Processing. Anyhow, the videos can all be found here , starting with the intro above. Together they probably amount to a full course on the language. Have fun, learn some programming, make something pretty.",,Vice,"Learning p5, a web-based interpretation of Processing, just got even easier to learn with a new series of tutorial videos. ",Now Would Be a Great Time To Get Into Creative JavaScript,https://www.vice.com/en_us/article/ezvjm4/this-would-be-a-great-time-to-get-into-creative-javascripting
Michael Byrne,"George Boole gets left out of most timelines of computer history. Babbage, Lovelace, Turing—sure, they're all there. Only rarely does the grandfather of digital logic get his due. Perhaps it's because his creation just seems like nature itself, a fundamental feature of information and so of the universe. Boole is a discoverer among inventors. Computer scientists get acquainted with Boole and his namesake early on. Boolean or ""bool"" is a data type built into most programming languages. It can hold one of two values: true or false. Nothing else. It could be a user-defined variable holding that true or false, or it could be some function or sub-routine, a collection of code that we can ask to spit out either a true or false result. Booleans are especially helpful as flags, which are sort of switches that might serve to tell a program to start or stop doing something. George Boole. Image: Getty But there is so much more to Boole's new world than a high-level programming data type. Boolean logic is digital logic. A system of mathematics based on trues and falses is one of yeses and nos and, ultimately, of 1s and 0s. There and not there. This is computing. Formal logic itself existed before Boole, of course. The founder of the idea is usually taken to be Aristotle. It wasn't until much later, in the 1600s, that Gottfried Wilhelm Leibniz took things even further, arguing that logical thought could be reduced to symbols and therefore could be examined as a computational system. If we can solve geometric problems using symbols and numbers, Leibniz believed everything could so be reduced. Leibniz forms a bridge of sorts between Aristotle and, centuries later, Boole. Boole's idea was, similarly, that if logical arguments and the relations between them can be reduced to symbols and rules, as in proper algebra, then a whole universe of thought could be computed mathematically. The result is what became known as Boolean algebra: a set of symbols and connectives allowing for the simplification of logical expressions. Formal truth-finding. As plus and minus signs are fundamental to arithmetic, Boolean algebra has the operations AND, OR, and NOT (to start). Each one of these relations receives one or two Boolean values (trues and-or falses) and spits back a single true or false in response. Computing ""true AND true"" results in true, for example, while NOT simply returns the opposite or inverse of the input: true becomes false and false become true. All of this translates immediately to switches and gates within a physical computer. A NOT gate takes a 1 bit and outputs a 0; an OR gate takes in a 0 and a 1, and outputs a 1. The relationships are all defined in truth tables, which you can see below. Image: SparkFun You can see Boole's accomplishment in most any circuit involving computation: An elevator control circuit. Image: Cameron Steiger/Penn State In his paper ""The Mathematical Analysis of Logic,"" Boole wrote that, ""What may be the final estimate of the value of the system, I have neither the wish nor the right to anticipate. The estimation of a theory is not simply determined by its truth. It also depends upon the importance of its subject, and the extent of its applications; beyond which something must still be left to the arbitrariness of human Opinion."" The applications of Boolean logic wouldn't been realized until almost a century later. At MIT in 1937, the mathematician and engineer Claude Shannon proposed using Boole's system as the basis for electrical switches in circuits, as seen in the diagram above. This idea is what became known as information theory and, soon enough, it became the basis of computing. For several reasons, Boolean algebra breaks down in quantum computing , but also not completely. Many of the algebraic rules of quantum logic remain the same, but there are some pretty ugly differences. Boole won't soon be obsolete, but his system may wind up looking quite different.",,Vice,"Almost a century before computers, Boole realized the universe can be computed. ","Happy 200th to George Boole, the Grandfather of Digital Logic",https://www.vice.com/en_us/article/aekw8j/happy-200th-to-george-boole-the-grandfather-of-digital-logic
Michael Byrne,"The thought of dealing with the ""null"" construct within a computer program causes me intense physical pain. Should null appear unexpectedly during the course of software execution, it has the potential to royally fuck things up. An example might be, say, if a software or website user's name happened to be ""Null."" It happens and apparently doesn't make life too easy for those bearing said surname either. A technology journalist named Christopher Null happens to have this misfortune, and, indeed, life as a Null isn't especially easy: The computing world would mostly like to think of Null (and the other Nulls of the world) as not really existing. This causes problems. ""But there's a dark side to being a Null, and you coders out there are way ahead of me on this,"" he writes this week for Wired . ""For those of you unwise in the ways of programming, the problem is that 'null' is one of those famously 'reserved' text strings in many programming languages. Making matters worse is that software programs frequently use 'null' specifically to ensure that a data field is not empty, so it's often rejected as input in a web form."" We can imagine it going like this: A ""Mr. Null"" enters his last name into a login field on some website and clicks submit. The software will take ""null"" and assign it to a temporary container (a variable) as it packages up a request to send to some server (where the name and password will be checked against some records). Somewhere in the course of things, ideally before sending the request, the program needs to check to make sure the user actually put something into the login box and didn't just hit the submit button for an empty field. It might do this by checking for ""null."" Within the PHP language, which is likely to be the thing validating a website form's input, there is a function known as ""empty()."" Simply put, a programmer gives empty() a variable, and the function returns a value of true or false, depending on if the variable is empty or not. To do this, it checks for a half dozen or so possibilities for the variable's value, including an empty string (""""), a 0 (several variations of 0, actually, depending on the data type), a ""false"" value, and, finally, a null value. These things all mean that the variable does not have a value or that it has a value that is ""undefined."" Sort of the same thing. Null has meaning—a whole lot of it, actually—but its whole purpose in life is to indicate no value. So, Mr. Null may be prompted to re-enter his name because, to the program, he has not yet done so. This could happen in any number of situations because there are null checks buried in any number of programming frameworks and functions, sometimes quite deeply. (In programming, it's often the case that a programmer will be referencing chunks of code that have already been written and are just hanging out in a file somewhere and don't need to be worried about, usually.) The problem is a bit more than syntactical. It's also philosophical. In programming, we're constantly creating variables, which can be viewed as names we associate with specific values. Or, rather, they can be viewed as names we intend to associate with specific values. Variables can be created without being assigned, which is really the whole point. The program's logic is agnostic about the ultimate values of its variables, so long as they fall within some prescribed ranges and types. They wouldn't be much use otherwise. But this whole business of unassigned variables means that we need to always check to make sure that we're computing with real stuff and that the variables did eventually get some appropriate value, which is not always the case: bugs, bad data, dudes named Null, etc. So, we have to have a way of saying that a variable is not currently defined. Enter ""null."" Null is clearly a value. It has meaning—a whole lot of it, actually—but its whole purpose in life is to indicate no value. Almost invariably, trying to pass a null value along to some unit of code as if it were something will return an error. This is the source of the aforementioned physical pain: null errors. At least Mr. Null doesn't have to debug himself.",,Vice,What it means to be an undefined value in programming.,When a 'Null' Value Happens to Be a Person,https://www.vice.com/en_us/article/qkjdxm/when-a-null-value-happens-to-be-a-person
Michael Byrne,"Via its research blog , Google announced on Monday that it was releasing the second generation of its machine learning framework as an open-source library called TensorFlow. ""The most important thing about TensorFlow is that it's yours,"" write Google Technical Lead Rajat Monga and Google Senior Fellow Jeff Dean. ""We've open-sourced TensorFlow as a standalone library and associated tools, tutorials, and examples with the Apache 2.0 license so you're free to use TensorFlow at your institution (no matter where you work)."" The whole thing is ready, waiting, and, based on some skimming around, quite accessible; the project comes well equipped with documentation and tutorials. As of now, it features APIs for both Python (more support) and C/C++ (better performance, less support), but as the TensorFlow project page notes, it's hoped that eager open-sourcers will get to work on building front-ends for Java, JavaScript, Go, and the rest of the programming language hoard. Via the C++ API, it's possible to integrate the framework into Android projects (Ctrl + F ""iOS"" not found). Here's some background, courtesy of Monga and Dean: Deep Learning has had a huge impact on computer science, making it possible to explore new frontiers of research and to develop amazingly useful products that millions of people use every day. Our internal deep learning infrastructure DistBelief , developed in 2011, has allowed Googlers to build ever larger neural networks and scale training to thousands of cores in our datacenters. We've used it to demonstrate that concepts like ""cat"" can be learned from unlabeled YouTube images, to improve speech recognition in the Google app by 25%, and to build image search in Google Photos . DistBelief also trained the Inception model that won Imagenet's Large Scale Visual Recognition Challenge in 2014 , and drove our experiments in automated image captioning as well as DeepDream . While DistBelief was very successful, it had some limitations. It was narrowly targeted to neural networks, it was difficult to configure, and it was tightly coupled to Google's internal infrastructure -- making it nearly impossible to share research code externally. This isn't the first open-source machine learning library by any stretch. Even just for Python there's a pretty long list . As Cade Metz points out at Wired , some large part of the significance of the TensorFlow release is that the system lets programmers use hardware like Google uses hardware. In particular, this means running software on graphics-processing units (GPUs), or at least the possibility of it. GPUs are conventionally intended for playing games and handling, well, graphics. This is because graphics tasks fit a certain computing paradigm that most other software tasks don't, at least historically. This is parallel programming, in which many computations can be done at once on different units of data. In graphics applications, software is constantly updating and manipulating potentially millions of pixels and those calculations are unique in that that they don't usually need to wait around for other computations to finish. They're independent. The software telling this page to do what it's doing is, by contrast, sequential. It spends a lot of time hanging around waiting for different things to happen. This is not so good for parallel programming and GPUs. That's all a GPU is and it happens that machine learning applications can exploit this because they're likewise needing to calculate many, many small pieces in parallel. That's how machine learning learns: examining vast numbers of raw data points. So, TensorFlow could be made to exploit the GPU in your gaming laptop naturally, which could prove to be a big cool thing for making machine learning even more general than it currently is. That's part of what makes it interesting to me. Machine learning and its AI parent are still widely interpreted as being esoteric and science-fiction-y fields, when they really are just natural ways of handling a lot of data. Machine learning is used in applications (usually pretty banal stuff) ranging from market analysis to image/speech recognition to spam filters. Will TensorFlow open up machine learning even more? That would be cool, at least.",,Vice,"There are a lot of open-source machine learning tools out there, but Google offers something extra: accessibility. ",Google Offers Up Its Entire Machine Learning Library as Open-Source Software,https://www.vice.com/en_us/article/8q8avx/google-offers-up-its-entire-machine-learning-library-as-open-source
Rachel Pick,"The Hour of Code is an annual campaign designed to get kids across the globe interested in coding, and this year, Microsoft and Minecraft developer Mojang (which Microsoft acquired in 2014 ) have joined the initiative by designing a fun Minecraft tutorial that helps teach children the fundamentals of coding. It says it's for ages ""6 and up,"" but I confess: I'm 27, and I had a lot of fun with it. The tutorial, which is already up and running , is broken down into 14 increasingly complicated ""puzzles."" You drag and drop blocks representing commands into your workspace, and press ""Run"" to see your Minecraft character navigate its world—chopping down trees and shearing sheep. After you complete a puzzle, you can view the code you wrote by seeing the blocks translated into Javascript. Image: Code.org Full disclosure: I have never played Minecraft , nor have I ever written in Javascript, so the novelty it had for a newbie like myself probably played a part in my enjoyment. But it was still bizarrely satisfying to solve these puzzles, even if they were designed for a child. And by showing kids how basic typed commands can translate into on-screen actions, I'd wager that the appeal will be pretty universal. The Hour of Code campaign doesn't officially kick off until Computer Science Education week, which begins December 7, but the tool is already up and ready for use by kids and educators. Code.org CEO Hadi Partovi believes Minecraft 's simple, universal appeal will reach a broad range of children, saying "" Minecraft is a special game that girls and boys alike often can't be pried away from. This year's Minecraft tutorial will empower millions of learners around the world...and will inspire them to impact the world by creating their own technology or apps."" If we really want a more diverse workforce in the tech sphere, we need to reach a broader range of students at a young age, so coding becomes just another way for them to build, play, and create. So initiatives like the Hour of Code are worth paying attention to and supporting —plus, I had a lot of fun shearing sheep.",,Vice,Minecraft teamed up with Code.org to design a fun educational tool and hopefully inspire the next generation of coders.,"Minecraft's Coding Tutorial Is Made for Kids, But I'm 27 and I Loved It",https://www.vice.com/en_us/article/8q89av/minecrafts-coding-tutorial-is-made-for-kids-but-im-27-and-i-loved-it
Michael Byrne,"Your average scripter likely isn't writing a whole lot of proofs or going through the rigors of formal program verification, generally. Which is fine because your average scripter also isn't writing software for jet airliners or nuclear power plants or robotic surgeons. But somebody is—and the odds are pretty good that your life has been in their hands very recently. How do you know they're not a complete hack? Well, you don't really. Which prompts the question: How is this sort of code tested? It was a short blog post written by Gene Spafford, a professor of computer science at Purdue University, that inspired this particular asking of the question. It relays an anecdote about the origins of fly-by-wire cockpits—in which aircraft flight surfaces and whatnot are manipulated electronically rather than directly—in the late-1980s. We're used to it now, but back then it was a huge leap: direct control was being taken away from pilots and put into the hands of software designed to communicate pilot commands to mechanical actuators. A computer could now crash a plane. That fact put the people behind those computers into a hell of a position. Spafford writes: In the late 1980s, around the time the Airbus A340 was introduced (1991), those of us working in software engineering/safety used to exchange a (probably apocryphal) story. The story was about how the fly-by-wire avionics software on major commercial airliners was tested. According to the story, Airbus engineers employed the latest and greatest formal methods, and provided model checking and formal proofs of all of their avionics code. Meanwhile, according to the story, Boeing performed extensive design review and testing, and made all their software engineers fly on the first test flights. The general upshot of the story was that most of us (it seemed) felt more comfortable flying on Boeing aircraft. (It would be interesting to see if that would still be the majority opinion in the software engineering community.) So, Boeing's programmers had this additional motivation of putting their lives in the hands of their own software on day-one. There would be no room for hotfixes at 30,000 feet—it just had to work. Which do you feel more comfortable with: the taking one's own medicine approach to software verification, or a bunch of formal proofs and simulations? I'd prefer both really, but the question of how this stuff is tested is interesting in itself. A 2011 Stack Exchange thread provides a lot of good insight into the process and its evolution. For one thing, the Boeing approach is going out of style or has mostly gone out of style, according to SE poster Uri Dekel (handle: Uri), a Google software engineer. ""There's a serious move towards formal verification rather than random functional testing,"" he writes. ""Government agencies like NASA and some defense organizations are spending more and more on these technologies. They're still a PITA [pain in the ass] for the average programmer, but they're often more effective at testing critical systems."" Scott Whitlock, a software engineer who maintains the home automation-focused software library FluentDwelling, goes deeper, explaining the design requirements for safety relays, which are systems in industrial applications designed to monitor equipment warning signals and shut down the implicated equipment if necessary. They include, according to Whitlock, ""Two redundant processors, typically sourced from different vendors, based on different designs. The code running on each processor has to be developed by two teams working in isolated conditions. The output of both processors has to agree or else the safety relay faults."" ""Now once you've designed your safety system, the logic you write to control the machine itself can be very seat-of-your-pants,"" Whitelock adds. ""Programmers frequently crash machines causing thousands of dollars of damage, but at least nobody's going to be injured."" Image: NASA How about manned spacecraft? In terms of criticality, a spacecraft is like an airliner crossed with a life-support system crossed with the hardest math problems in aeronautics. ""This software never crashes,"" wrote Fast Company's Charles Fishman in ""They Write the Right Stuff."" ""It never needs to be re-booted. This software is bug-free. It is perfect, as perfect as human beings have achieved. Consider these stats: the last three versions of the program—each 420,000 lines long-had just one error each. The last 11 versions of this software had a total of 17 errors. Commercial programs of equivalent complexity would have 5,000 errors."" How do NASA's programmers pull that off? Well, mostly because they have to. The smallest bug will put billions of dollars and lives at risk. All in the brightest of public spotlights. ""Before every flight, Ted Keller, the senior technical manager of the on-board shuttle group, flies to Florida where he signs a document certifying that the software will not endanger the shuttle,"" Fishman explains (the article predates the shuttle's demise, but still). ""If Keller can't go, a formal line of succession dictates who can sign in his place."" As for the code itself, its perfection came as the result of basically the opposite of every trope normally assigned to ""coder."" Creativity in the shuttle group was discouraged; shifts were nine-to-five; code hotshots and superstars were not tolerated; over half of the team consisted of women; debugging barely existed because mistakes were the rarest of occurrences. Programming was the product not of coders and engineers, but of the Process. ""The process is so pervasive, it gets the blame for any error,"" Fishman wrote, ""if there is a flaw in the software, there must be something wrong with the way its being written, something that can be corrected."" The software is perfect with no margin of error. More: The development group is supposed to deliver completely error-free code, so perfect that the testers find no flaws at all. The testing group is supposed to pummel away at the code with flight scenarios and simulations that reveal as many flaws as possible. The result is what Tom Peterson calls ""a friendly adversarial relationship."" ""They're in competition for who's going to find the errors,"" says Keller. ""Sometimes they fight like cats and dogs. The developers want to catch all their own errors. The verifiers get mad, 'Hey, give it up! You're taking away from our time to test the software!'"" So, what do you think, future astronaut? The programmer that's flown into orbit under control of their own software, or the programmer reared in NASA's shuttle group?",,Vice,"Ideally, it's so good that it barely needs to be in the first place.",How Is Critical 'Life or Death' Software Tested?,https://www.vice.com/en_us/article/9akjve/how-is-critical-life-or-death-software-tested
Michael Byrne,"Computers exist in Minecraft, of course. You can build your own virtualized machine with a mod called OpenComputer (and probably others) and it will sure enough be a ""real"" computer. Different computers with different capabilities can be crafted from the mod's arsenal of modular components: graphics cards, network cards, redstone cards (redstone being the Minecraft version of electricity), sensors, holograms, robots, and more. It's also possible to program Minecraft computers with the real-world programming language Lua. Unsurprisingly, one player (known as ""giannoug"" at hashbang.gr ) decided to take this capability to its wonderfully absurd conclusion and create a program that can control some real lights in the real-world: ""breaking the fourth wall,"" in giannoug's words. Giannoug was able to accomplish this is less than an hour, in part thanks to some wi-fi-enabled light bulbs that just happened to be pre-hacked such that they could be controlled via TCP, a communications protocol that allows network nodes to exchange streams of data (vs. packets of data, as in IP). For the bulbs in question, that stream consisted of bytes corresponding to three RGB colors, the desired brightness of a fourth, white bulb, a mode switch (color or white), and that's it. It looks like this: 0x56 RED GREEN BLUE WHITE MODE 0xAA ""OpenComputer works with redstone, so we can easily read the binary state of a Minecraft lever, either ON or OFF and toggle the WHITE byte to turn on or off the WiFi bulb,"" giannoug explains. ""This is the computer I build in-game. It has 6 monitors that get combined to a big one, a disk drive and a redstone lever connected to the right side with redstone dust."" The final piece is a program to put it all together, which is where OpenComputer's Lua capability came in handy. I won't paste it all here, but the code is pretty short and doesn't look all that different from how you'd program the same system with an IRL computer.",,Vice,Minecraft 'breaks the fourth wall.',Program a Virtual Computer in Minecraft To Control Stuff in the Real-World,https://www.vice.com/en_us/article/yp3zbk/program-a-virtual-computer-in-minecraft-to-control-stuff-in-the-real-world
DJ Pangburn,"IO Sounds - Another Love EP from Derelicht on Vimeo . As an established denizen of the UK underground, iO Sounds explores the darker, more minimal side of house and garage. With a sonic palette that is both futuristic and familiar in a retro way, the producer crafts subtly impressionistic dance floor music. For part of his new Another Love EP , out now on 20/20 Vision Recordings, the artist commissioned visual artist, programmer, and designer Joëlle to create a generative music platform that took three months to complete. Joëlle tells The Creators Project that the concept evolved from research into Jupiter's moon, Io. After discovering that Io was the most geologically active location in the solar system with over 400 volcanoes, she knew she had her conceptual inspiration. “This inspired thoughts around how the surfaces and textures of a planet’s moon might look and feel, and really informed the aesthetic of the video,” Joëlle said. “Rock, volcanic explosions, dust, gas, and energy were elements that I wanted to convey, and I wanted it to be quite gritty and slightly surreal.” To pull these texture off, Joëlle used Trapcode Mir v2 Beta and Trapcode Sound Keys in After Effects to create audio-reactive moons and landscapes. “I’m a huge fan of the Trapcode plugins as they really allow for fast iteration and experimentation,” she explains. “There’s also the unknown element as it’s somewhat generative and there are some unexpected results which can be awesome.” She then imported the renders into Quartz Composer and added some effects using a plugin by vade called Optical Flow . Using the videos as a data source, she next applied feedback, distortion, and blur with some of the parameters driven by the BPM using the EMIT Quartz Composer library by Rybotron. Joëlle then captured those results in real time using Syphon Recorder, and re-imported them into After Effects to make the final edit. The evolving orbs look rad online and may have taken months to create, but we'd wait months more to watch them warp and wiggle live in the background of an iO Sounds show. Check out some of our favorite moments from the video below: Click here to grab a copy of iO Sounds' Another Love EP. Related: Trip Through Tokyo in Rone's Super-Psychedelic Music Video for ""Acid Reflux"" Walk Down Watercolor Railroads in the Rotoscoped Music Video for Hugo's ""Hailstorms"" [Premiere] A Plastic Bag Creature Takes a Surreal Stop-Motion Journey in Dirt Bikes’ New Music Video",,Vice,"Rock, volcanic explosions, dust, gas, and energy inspired visual artist and programmer Joëlle to create this generative music platform. ","Audio-Reactive Moons Erupt in iO Sounds' ""Another Love EP"" Video",https://www.vice.com/en_us/article/gvwdxx/audio-reactive-moons-erupt-in-io-sounds-another-love-music-video
Michael Byrne,"Nomadic Code School presents itself as a cross between a beach resort vacation, wilderness adventure camp, and a learn-to-code incubator. For those future-scripters looking for something beyond the offerings of the hundreds (more?) of mostly frontend development-focused camps currently offered across the country, one will soon be able to learn their code in ""amazing places with amazing people."" NCS hasn't properly launched yet and is still in the sign-up-for-more-information phase. According to its website, NCS is currently scouting potential locations for its inaugural session. Matt Lea, the company's founder and CTO at Townsquare, told me that destinations under consideration include India, Turkey, and Italy. Three weeks in Vermont this is not. This is what you can expect learning-wise: We will start with the basics literally creating your first web page via a text editor. Next we will learn the fronted building a variety of browser based apps. Finally we will dig into the back end and build server side apps using NodeJS, ExpressJS, and MongoDB and learn how to host them on the Amazon Web Services. Would you believe ... I actually kind of like this? At the very least, I wish it was my idea. For one thing, I have no grounds to hate on Nomadic Code School, though that's kind of where I started because the pricetag is completely brutal. While a resident of a certifiably amazing place—literal minutes from the best whitewater kayaking, mountain climbing, skiing, and windsurfing in the continental US—I'm able to study computer engineering at a pretty great university and work for a pretty great company thanks entirely to this here internet. It's not bad! It's also not India, but whatever. I am a deep, lifelong believer in amazing places and NCS' offering of three week coding bootcamps at ""exotic locations throughout the world"" has a certain resonance. Hell, my second degree is in recreation management (which is sort of hard to explain, but we can just say that I'm well-qualified as a park ranger). I'm a recreator. I think a lot better here in my amazing place, certainly. I'm outside constantly doing outside kinds of things: hiking and climbing and kayaking. It for sure helps me to learn and problem solve, and there are a bazillion studies out there confirming that outdoor activity is good for mental health. A 2011 metastudy for example, found that exercising in natural environments led to ""greater feelings of revitalization and positive engagement, decreases in tension, confusion, anger, and depression, and increased energy."" To be sure, there's something not quite on about boutique coding retreats and expensive programming bootcamps in general. In a way they seem designed to reinforce the status quo within the industry of rewarding the rewarded, but that's not really on NCS. (Though it likely won't be cheap: ""These prices are subject to change,"" Lea said. ""Not including travel or food, but including housing for the three week course were looking at around 10 to 15 thousand."" His estimate for a 12 week program is more like $30,000.). Mostly, I just think you'd learn a whole lot more (and learn it better) surrounded by an environment as far removed as possible from the day-to-day grind rather than some airport hotel meeting room, at least if you've got the coin in the first place and only have three weeks to spend on a new career. I dunno.",,Vice,I'm surprised too.,'Nomadic Code School' Is Kind of a Good Idea,https://www.vice.com/en_us/article/9akjbp/nomadic-code-school-is-kind-of-a-great-idea
Michael Byrne,"Among the more disappointing recurring metaphors used in the teaching of computer science—based on my own informal recollection as a computer science student—has to be the stack of plates. It's really only barely a metaphor and corresponds to a data structure known as, wait for it ... the stack. The stack's defining feature is that you can only remove things (data in various forms) from the stack in the order that they were put on the stack. A plate goes on the top of the stack and the next plate that can be removed is that plate. Take away that plate and a new plate rises up and becomes the top plate. Hot fucking shit, right? Stacks are actually pretty neat—particularly the runtime stack, aka ""the stack,"" that keeps a queue of a hardware instructions and generally enables your computer to do a whole bunch of things at once, or seemingly at once—and they deserve better. A stack is more than a set of rules, it's a way of thinking. This is the case with much of computer science, which is not just a realm of coded instructions, but one of entirely new ways of considering information itself. A stack of plates at a buffet line is a stack because that's just how it works out, stacks being stacks, but a stack in CS is an ingenious solution to problems that don't even exist yet. Teaching computer science as a way of arranging reality rather than a way of arranging syntax or data structures—what's more appropriately known as programming or even coding—is among the arguments made by Thomas J. Cortina, a computer science professor at Carnegie Mellon, in a recent issue of ACM Communications . His general point, which is only sketched here, is that we should be aggressively advancing programs like CS Unplugged , which is a kid-focused curriculum of sorts developed by a group at the University of Caterbury in New Zealand, with a stated goal being the teaching of computational principles rather than programming and technical details. It's a great idea. By being physically part of the solution to a problem as it is being solved, kids learn from observations and experiences. Activity examples range from teaching data compression via rhymes, graph theory via mud, finite state automata via pirates, and so on. It's surprisingly deep given the target audience (though maybe algebra and trig would seem that way too if we weren't so used to it). ""Activities in CS Unplugged support the principle of computational thinking,"" Cortina writes, ""which promotes the idea that problem-solving skills and computational techniques used in computer science should be a part of every person's education and are applicable to a wide variety of fields, not just computer science."" There's also the fact that kids tend to do better with really any subject if it's introduced in an unconventional educational setting, e.g. not sitting quietly in a room while focusing on a teacher or instructions on a screen. The key is physicality, and there's no reason for computer science to be any different. Again: it's all just ways of thinking. One activity has kids simulate parallel sorting algorithms with help from sidewalk chalk. ""CS Unplugged exemplifies an educational theory known as experiential learning, where participants learn through activity outside of a standard academic setting,"" Cortina explains. ""By being physically part of the solution to a problem as it is being solved, kids learn from observations and experiences. Unlike some introductory programming activities that tend to promote solo activity, the CS Unplugged activities put kids physically in the middle of the problem, getting them moving, working together, sharing ideas, and designing solutions."" There are more arguments for this sort of CS education, even as a supplement to a more traditional stare-at-screen approach. For one, it lets kids learn exceedingly marketable ideas even in classrooms that don't have computers, which is still a lot of them. (Computer use during ""instructional time"" was recently found to occur in only about 40 percent of classrooms in the US.) Even if it's a baby step in narrowing the technology gap, CS Unplugged opens up a new and much-needed way of rewarding actual talent rather than class and the access afforded according to class. Cortina's whole piece is worth reading (however paywalled). Mostly, I try to imagine myself as a pre-adolescent presented with a stack of dinner plates. The result would've been something like negative-stimulation and, consequently, I might have wound up being a writer or something. Who knows.",,Vice,"Computing is a way of thinking, not of coding.",The Argument for Teaching Computer Science Without Computers,https://www.vice.com/en_us/article/ezv9wm/the-argument-for-teaching-computer-science-without-computers
 ,"Kiwi GTA V fans will feel at home in Los Santos thanks to Dre Hema-Kani, who uploaded his mods to Reddit earlier this week. Using a PC and Photoshop, the New Zealand–based father-of-two slipped local brands onto billboards and storefronts—and added a few familiar thugs. The Creators Project called Dre to find out more. The Creators Project: Why did you choose NZ mods? Dre Hema-Kani: I pretty much wanted to turn the game into somewhere that reminded me of home. Driving around, seeing familiar things. It was more for myself than anyone else, but I’ve had heaps more interest than I expected. I only put it on Reddit to see if my gaming friends were into it, and if people in New Zealand would actually download it. Are these the first treatments you’ve done? Yes, they are something I always wanted to do but I never had the time. I had two weeks off from work so thought I’d learn. After a week I uploaded them to see if I should carry on, and it turns out I should. What's your day job? I work at a slaughter board, full-time. Tell us more about the mods. I made different trucks and trailers that represent the companies from around here. Pak 'n Save, Supercheap Auto, mainly New Zealand companies, but I’ve got bigger brands that you usually see driving around a New Zealand city. Because GTAV has gangs, I made the gangs New Zealand. I’m working on changing the emergency departments to New Zealand ones. I’ve done an Air New Zealand jet, so when I do my first upload that will be on there. Do you have a background in programming? I’ve always been into computers most of my life, but then I had a child when I was pretty young, 16, which meant I needed to work. I’m still at the same place doing the same thing, but it has been good to have time to learn this. It’s quite frustrating work; I’m just glad people seem to like it. Will you release it publicly? That’s the plan. I didn’t expect it to get any attention, so I thought I’d wait to hear from the companies before I publish anything. I’ll give it a couple of weeks, do some work in the meantime, then upgrade it. Do you sink a lot of time into GTA ? Not really. I’ve probably spent more time modding it! Check out Dre Hema-Kani's original post on Reddit. Related: Grand Theft Auto Photo Essay Imagines A New Narrative Within The Game A Grand Theft Auto Subreddit Is A Communal Art-Making Machine ' GTAV' Player Channels His Inner Ai Weiwei These Stunning Photos of New Zealand's Largest Gang Will Give You Restless Nights | VICE",,Vice,Step Inside Grand Theft Auto New Zealand,Games,https://www.vice.com/en_us/article/3d5vgk/step-inside-grand-theft-auto-new-zealand
Michael Byrne,"Processing is an enormously popular, very-high-level programming language that began at MIT in 2001. As originally developed by MIT Media Lab alums Casey Reas and Benjamin Fry—both artists and technology thinkers—Processing was intended to be a learning language, a tool to get non-programmers, particularly artists, hooked on code via the promise and delivery of immediate visual feedback. ""Processing began with a simple idea,"" wrote John Maeda, an artist and computer scientist that advised the duo at MIT, in an excellent history at Technology Review. ""Fry was a gifted Carnegie Mellon grad who had a background in graphic design but could also program in Apple II assembly language. Reas was a gifted University of Cincinnati grad who had a background in graphic design and a passionate desire to understand computation; as design director at I/O 360 Digital Design in the mid-1990s, he had been one of the few classically trained graphic designers who understood the possibilities of computer programming, but he left his job (and probably a few gazillion dollars) to study computer code for real at the Media Lab."" Since its early-aughts genesis, Processing has attracted a thriving community of followers and developers, especially among new media types and artists but also well beyond. It has a similarly popular implementation in JavaScript—enabled by HTML5's draw-able, Flash-slaughtering ""canvas"" element—known just as Processing.js, as well as a soon-to-be-very-popular and still more artist-friendly variation called p5.js. In 2010, Python Mode for Processing was first released and, since 2014, processing.py has been co-developed by Google. ""Processing had two goals when we started in it 2001 and we have these goals today,"" Reas told me. ""We made it to be a flexible software sketchbook and a language for learning how to code within the context of the visual arts. We have always aspired to make the powerful ideas behind creating software accessible for visual artists and designers. We have always promoted software literacy (both reading and writing code) within the visual arts."" ""We started the Processing Foundation in 2012 to clarify these ideas,"" Reas said, ""to support the creation of Processing, and to apply these ideas to other domains."" Unlike many if not most of the languages to be featured in this series, Processing is something I would suggest that you, perhaps a non-programmer, go out and try right now because it's stupid-easy and pretty fun. (And I hate fun, usually.) This is the Processing equivalent of ""Hello, World"": a line. line(15, 25, 70, 90); Which is this (show with the IDE in the background): Processing is also an IDE, or integrated development environment. This is part of what makes it so easy to get into. Download the IDE (for free), write a line of code, and click a button to run it. Whatever you just made—a line, perhaps—will show up in a new window. Congrats, your first computer program, and all it took was about 20 seconds. Given another 20 seconds and you could be modifying or running one of literally hundreds of code examples and tutorials that come prepackaged with Processing, or one found within the brightly-colored, bottomless chasm of the openprocessing.org community . Processing is learning, but learning also means access to new stuff. The Processing Foundation ""Processing has always had a priority of access, Reas explained. ""As communities of artists and designers, we have aspired and struggled to create our own technologies and more importantly, our own cultures around emerging technologies. Often, we have different priorities and politics from corporations who make software for artists to use. We are a group of artists and designers who make software for ourselves and we share it with others."" It's true enough: As a language for and by artists—as opposed to a general-purpose tool—Processing is in a unique position to be whatever it wants. ""As a larger community, we've done a good job at reducing the barriers to learning and creating with emerging technologies. Our field is fundamentally different from a decade ago in respect to that goal,"" he said. ""However, in relation to diversity, access, and inclusion, we're not doing well enough. The current challenge is to move the environment and culture around technology forward—to improve access to larger and different groups than we've reached before. We plan to achieve this through partnerships."" The Processing basics Processing code is created in what are known as sketches, which are just files. The Processing IDE is known as a sketchbook and, as far as IDEs go, it's very lightweight, especially compared to monsters like Visual Studio, Xcode, and Eclipse (though Processing is well-suited for Eclipse, as it's Java-based). The skeleton of a Processing sketch consists of a handful of functions (or methods), which can be viewed as programs within programs. The two biggies are called ""setup()"" and ""draw()."" This isn't meant to be a tutorial by any means, but to talk about Processing even in general terms, we also need to talk about these two functions. These are the brain and heartbeat of a Processing program. Briefly, setup() does just what it sounds like. It creates a window of certain dimensions and with certain properties that will contain everything else the (rendered or live) sketch does. draw() is where the action is. While the initialization function runs once at the beginning of the sketch/program, the drawing function runs continuously, refreshing the window created in setup() at regular intervals given by the frame rate. Just imagine a paper flip-book in digital form. So, if you were to, say, make a circle bounce up and down, you would write some code that changes the location of the circle on every refresh (or every other refresh or ever third refresh, or whatever). For example, if you had a dot on a screen that started at position x , y , you could write code that increments those values every time the function runs, which is what the refresh actually is: a new instance of the drawing function being called every 1/60th of a second or so. Every time >draw() is called, instead of putting our dot at the location x , y , we put it at x+1 , y+1 , so the dot appears to moving right (along the positive x-axis) and up (along the positive y-axis). So, say we start with our point at 0,0, the origin. After 1/60th of a second, it will be at 1,1—one pixel up, one pixel right—and it will keep progressing like that: 1,1;2,2;,3,3;4,4 ... Here's an actual example from processing.org (a grey ball passing back and forith horizontally): // Learning Processing // Daniel Shiffman // http://www.learningprocessing.com // Example 5-6: Bouncing Ball int x = 0; int speed = 1; void setup() { size(200,200); smooth(); } void draw() { background(255); // Add the current speed to the x location. x = x + speed; // Remember, || means ""or."" if ((x > width) || (x < 0)) { // If the object reaches either edge, multiply speed by -1 to turn it around. speed = speed * -1; } // Display circle at x location stroke(0); fill(175); ellipse(x,100,32,32); } The guts There is a catch to Processing. It's written in Java. A Processing sketch is like a further abstracted form of a Java program. A lot of the Java guts are hidden by Processing and, given that part of the whole point of Java itself is to hide its own guts in the Java Virtual Machine, Processing can be considered to be a very-high-level language. Between it and the actual computing hardware are a whole lot of layers and additional abstraction, which is not always a good thing when it comes to speed and other things. Image: joonhyublee  Java is clunky, slow, a product of the Oracle corporation, and as a coding experience, it can kind of feel like eating a TV dinner: simplicity at the cost of everything else. Many people also love it.  Kinect Flock | Processing Concept Sketches Credit: Alex Wolfe  It's an interesting thing—an entire creative galaxy sprouting from Oracle's stuffy Java. From an FAQ at processing.org: We didn't set out to make the ultimate language for visual programming, we set out to make something that was: A sketchbook for our own work, simplifying the majority of tasks that we undertake, A teaching environment for that kind of process, and a point of transition to more complicated or difficult languages like full-blown Java or C++ (a gateway drug to more geekier and more difficult things). Processing is not intended as the ultimate environment or language. So, why use Java in the first place? There are some pretty good reasons actually. In 2001, Java was finding a lot of use in web-based animation via Applets, which are those small frames you still sometimes see on websites in which Oracle demands that you download the latest version of Java or else. This was more or less the chief resource available to web-based digital creatives, which made it a natural choice. ""In one sense, Processing arose as a response to practical problems,"" Maeda writes. ""When Java first came out, it offered minimal support for sophisticated graphics processing. Drawing a line and stuff like that was possible, of course. But it couldn't do transparency or 3-D, and you were almost guaranteed to see something different on a Windows computer and a Mac; it was incredibly cumbersome to do anything that was both sophisticated and cross-platform."" ""So [Ben] Fry,"" he continues, ""who grew up hacking low-level graphics code as a kind of hobby, built from scratch a rendering engine that could make a graphically rendered scene appear the same in a Windows or a Mac environment. It was not just any renderer–it borrowed the best elements of Postscript, OpenGL, and ideas cultivated at the MIT Media Lab in the late Muriel ­Cooper's Visible Language Workshop."" Of course, the programming language world is a whole lot different now and there are any number of ways to do cool stuff on the web or otherwise. Would Reas and Fry have done it differently had Processing been released in 2015? Maybe. ""We asked [artist and programmer] Lauren McCarthy to explore this question in one of the first [Processing] Foundation fellowships,"" Reas explains. ""How could the initial goals of Processing be achieved in 2014 outside of the technical decisions we made and have been making since 2001? The answer was to apply the ethos of Processing to JavaScript and the web. The p5.js project is the result and she is doing an amazing job as the leader of that project. I think 2015 will be a big year for p5.js."" ""We're still developing Processing with Java in 2015,"" Reas says. ""It remains a good balance in how easily ideas can be expressed in code and how quickly the code will run. C++ requires too much detail for the idea of 'sketching.'"" OpenFrameworks For those unsatisfied with Processing or its brand-name iterations, or for those who prefer programming at somewhat lower levels (meaning: less abstracted from the machine*), there is OpenFrameworks , which I like a whole lot and was my backdoor into Processing-land in the first place). OF is an open-source ""creative coding"" toolkit written in C++ providing much of the same capabilities as Processing; it's possible to translate from one to the other pretty easily and many if not most of the functions provided by Processing have suitable analogs in OF. There seems to be a solid crew out there making sure OF is keeping up (at least). For example,. the entire code-base provided by Daniel Shiffman's excellent and popular Processing-based Nature of Code —a user-friendly primer on simulating natural processes algorithmically—has been mirrored in C++ several times thanks to OpenFrameworks. (By the by, Shiffman has his own Learning Processing book and an accompanying website .) *I know, I know. If it were only so simple. Arduino Arduino is just as much of a movement as it is a hardware product, just as Processing is as much a movement as it is a programming language. Its series of microcontrollers—small, inexpensive computers that are capable of programmable input-output operations—has made embedded systems a playground for hobbyists and artists. Like Processing, it's both an entry-level to and a foundation for using technology to do creative things. The standard Arduino IDE looks a whole lot like Processing too: There's no shortage of projects and libraries marrying Arduino and Processing capabilities—and tutorials explaining how to do so—but we're getting a bit beyond the scope of this post. Just know that it's out there. As seen in        Getting started Download the Processing IDE here . It's available for Linux, OSX, and Windows. Next, there are a universe of tutorials to be found at processing.org and beyond. Processing: 0 to ""made something cool"" in mere seconds.",,Vice,What started as a programming learning tool developed at MIT now pretty much owns code-based digital art.,"Know Your Language: Meet Processing, the Lingua Franca of Creative Coding",https://www.vice.com/en_us/article/qkv597/meet-processing-the
 ,"Garbage bags aren't just for trash, you know? In new exhibiton, Sixty Eight , artist Nils Völker employs them in a site-specific kinetic installation at the Motion Science exhibition at Tokyo's 21_21 Design Sight facility. Völker often employs custom electronics and programming to breathe uncanny lives into lifeless, humdrum forms, like plastic bags or helium balloons. In Sixty Eight, the bags rise and fall at different intervals, looking like an unnatural, writhing landscape that grows and shrinks almost reassuringly. "" The garbage bags are selectively inflated and deflated in controlled rhythms, creating wavelike animations across the floor,"" writes Völker on his website . ""Although each bag is mounted in a stationary position, the sequences of inflation and deflation create the impression of lively movements. Geometrical forms appear from the matrix and disappear back into the surface."" Look long enough and it's a bit like the pareidolia of seeing faces and different forms in clouds, but this time, well, the clouds are made of plastic and they're on the floor of a museum. Click here for more from Nils Völker. Related: A Warehouse-Sized Plastic Sheet Dances in Artificial Wind 89-Year Old Magnetic Art Pioneer Debuts in U.S. A Kinetic Sea of Flowers Blooms in Tokyo",,Vice,Artist Upcycles Garbage Bags into a Kinetic Sculpture,Entertainment,https://www.vice.com/en_us/article/53w5pk/artist-upcycles-garbage-bags-into-a-kinetic-sculpture
Ben Richmond,"You'd think that the question of how to get everyone in a firing squad line to fire at the same time is best left to states desperate to keep capital punishment going without lethal injection drugs , but it might be a surprise to find that the question was also studied in the early days of computer science. Of course, in a world where something called the ""prisoner's dilemma"" is invoked for social sciences and economics, bizarre or macabre-sounding thought problems pop up with some regularity. But man, the firing squad synchronization problem has got to be one of the bleakest. To get a better grasp on the problem, the solution, and why it was ever a problem to begin with, I called up Darin Goldstein, a computer science professor in California State University Long Beach's computer science and computer engineering school, who has published papers  on the problem , and remains in thrall of the problem's elegance . He described the firing squad synchronization problem like this: Imagine a firing squad, all standing in a line, with a general on one end, who needs to tell the squad when to all shoot their guns at once. Firing at the same time was paramount for execution squads, so that no one would know who fired the fatal shot. This notion was so important that at some points in history, some bullets would be substituted with blanks, and the riflemen wouldn't even know which they were firing. Every soldier in the line is an automata. They have no memory and can only receive very simple instruction. According to the problem, the general stands at one end of the line and needs a way to tell everyone to fire at once, and doesn't want to use an audible countdown because, as Goldstein put it, ""that would be horrible."" So the only way for the line to communicate is for the riflemen to pass the message on to the soldier directly next to his left or right—as was the case in very early computing. The question, first formulated in the 1950s, was essential to automata theory , which became the foundation of computer science, Goldstein explained. ""Alan Turing came up with the general model for what a computer should look like,"" he said, taking things back to the very beginning. ""He came up with the idea of a computer with a brain, the automata, and then memory, which is a long ticker tape with cells on it that you can write things on. If you have a computer, you have a CPU (the brain) and the hard drive (the memory). And there are other parts, but that's the basic idea. Brain and memory."" ""The brain that Turing came up with had no memory. The brain was just in a state, and that's what you're thinking about at that time,"" he said. ""So every soldier in the line is an automata. They have no memory and can only receive very simple instruction. They can remember what they heard last time—and that's the automata: what you're thinking about."" Goldstein said the elegance of the problem was cool, and originally he thought it was going to be a really important problem. ""We have a lot of mobile devices, and if you think about taking these devices and making them the absolute smallest, they wouldn't have hard drives. If you miniaturize a phone or computer to the size of a dust particle, computers wouldn't be able to identify themselves at all, they could only receive and send and only keep a very small amount of information, depending on what they hear,"" he said. ""There was some talk a while ago, if you want to, you can take these dust things and spread them—insert them—and they could do things. With a lot of these things, they could collect information, but how could they collect information without holding anything? How do you make a conglomeration of things that can't think, think?"" The solution to the line of soldiers, Goldstein said, is something that a smart computer science person could work out in a few hours. Computer science pioneers John McCarthy and Marvin Minksy published this solution in the early 60s. If you want to go deep into the weeds on this, check out Stanford's treatment of it , but Goldstein explained it really well. ""What's the absolute minimum time you think [synchronization] could be done?"" Goldstein asked me. ""Say there are n soldiers. How long would it take to synchronize?"" I guessed N-times the amount of time it takes to tell each other and Goldstein said that most of the time people just say the general should just start a countdown at the number of soldiers—so, if there's 10 soldiers in the line, he tells the first soldier ""10"" and that soldier tells the next one ""nine,"" and then so forth. You only need 10 types of messages, and it seems to work, right? ""That's not legal though,"" Goldstein said, because to really solve the problem, the general can't know how many people are in the line. ""That number isn't a constant. The solution that you get has to be a constant size, no matter how long the line. So if you [use a solution that requires your soldiers operate with] a 1,000 messages, that solution stops working once you reach 1,001 soldiers."" Goldstein said the way you program has to be ""quasi-clever,"" and you have to be able to synchronize without counting or even knowing the number of soldiers. Minksy and McCarthy's solution was to send out multiple messages at different speeds, one going three times faster that the other, which allows the first message to reach the other side of the line, bounce back and reach the other right at the line's mid-point. When the messages intersect, Goldstein said, the middle guy becomes another general, and now you have two lines. ""And then as soon as you have that, you go again, you keep splitting the line in two over and over and eventually every soldier will consider himself a general,"" he said. ""And as soon as they all know the guy to left and right is a general, they fire."" So without ever knowing how many soldiers are in the line, you get them all primed and firing together. A solution that uses only six states, currently the fewest. Look at how it scales! After having solved the firing squad in a line problem, mathematicians began generalizing. ""Instead of line, they go to a grid, and once you solve the grid you go to a three-dimensional object. And then once you go to a 3D object, you move on to a general undirected graph, and once you do that you do a directed graph and on and on and on,"" Goldstein said. ""Really, the most general problem is the strongly-connected directed graph and that was solved multiple times."" People also tried to work out faster solutions and ones that used ever simpler soldiers and were able to prove that the firing squad synchronization problem can't be solved in minimal time if the soldiers are only capable of four states—soldiers who can only operate with four types of messages. The lowest functional model, for now, uses six states. You can guess where Goldstein's headed with this. ""But five [states] have been open for about 30 years,"" Goldstein said. ""So nobody knows five, and that annoys me. I really badly want to figure out five, because I feel like I could settle the problem."" And there's nothing worse than an unsettled problem, particularly one that's this unsettling.",,Vice,"The question, first formulated in the 1950s, was essential to automata theory, which became the foundation of computer science.","Firing Squad Synchronization, Computer Science's Most Macabre-Sounding Problem",https://www.vice.com/en_us/article/vvbk53/firing-squad-synchronization-computer-sciences-most-macabre-sounding-problem
Michael Byrne,"The human body is an endless stream of numbers, from brain waves to blood cells to whatever else you can imagine or secrete, and most of those numbers are changing all of the time. You might even say that its potential for quantification, for producing raw data, is limitless—making that data useful, however, is a different matter entirely. An ICU patient is monitored and assessed according to 12 different variables . These include such measurements as body temperature, heart rate, blood oxygenation, blood pH, and others. Together, they're used to formulate a quantitative answer to the question, ""How bad is it, doc?"" Many of these physiological signs are measured in real-time via electrodes and like a billion different varieties of catheter. Add to it barrages of lab tests done multiple times per day per patient and the need for 20 or so clinicians (per patient) to have access to all of this data, and the result is very a deep data problem. Multiply that data problem by hundreds of thousands of patients. This is the fundamental problem that the programming language MUMPS (sometimes called just ""M""), or the Massachusetts General Hospital Utility Multi-Programming System, aims to solve. To its proponents, MUMPS allows for a one of a kind synthesis of programming and database management, while to to its detractors , it's a bizarre anachronism with little connection to the evolution and innovation taking place elsewhere in programming. Probably to most people that do things with computers, MUMPS/M is poorly understood, at best, and more likely to be completely unknown. This is unfortunate, at least for the reason that MUMPS predated the NoSQL database movement by many decades. NoSQL is a relatively recent push away from relational databases like SQL—read: lots and lots of tables—and toward structures more amenable to Big Data. NoSQL has become a Web 2.0 backbone, supporting the databases of Facebook, Google, and Amazon. MUMPS concepts (directly) underly two of the largest contemporary NoSQL tools : GT.M and InterSystems Caché. The alternative structures offered by NoSQL might be document-oriented databases (where different types of data are stored in one unified document instead of a bunch of tabular cells) or graph databases (modeled after structures in graph theory) or columnar databases (where data is laid out in key-valued arrays) or some combination. (Confusingly, the NoSQL name corresponds to ""not only SQL"" rather than ""no SQL."") Read more: 'Primitive,' 'Asinine:' What Real Doctors Think of Electronic Medical Records  ""MUMPS is unusual in that it is two things at once: a language and a database,"" explains Rob Tweed , a longtime MUMPS developer (and, yes, booster), at the EWD Files. ""More accurately it's a database with an integrated language that is optimized for accessing and manipulating that database. It was originally designed for use in a medical/clinical setting, but in meeting the non-trivial needs of that setting, it happened to pre-empt the now hyper-trendy NoSQL database design goals by several decades. It's an exceptionally high-performance database with an equally high-performance language, capable of massive scalability."" ""MUMPS is unusual in that it is two things at once: a language and a database."" When a user is doing something with an SQL database (or other relational database), whether they're adding to it or retrieving from it, they're performing a ""query"" operation, which is built into the database software. This is an abstraction intended to hide the guts of the database from the user, who only needs to interface with it via this one query command. But the thing about abstraction in computer science, whether it's a high-level virtual machine-based programming language like Java or a bare-bones Unix shell, is that it always has a cost. In a database, this might be the speed by which some piece of data can be stored or accessed. The query operation is simpler for the end user (the database programmer, that is), but slower and clumsier. MUMPS is based on storing data in simple arrays that are accessed using a key. Imagine just a long list of things with each item having its own unique designation referred to by a variable. Variables (or keys, in this case) are just addresses of different memory locations within those arrays, which are called globals in MUMPS-speak. A MUMPS system, which might be made up of many computers, has its own collection of global arrays stored in non-volatile memory. So, unlike an array created in a language like C++, which exists only for the duration of the program or the program's existence within a computer's RAM address space, a MUMPS global sticks around on a server, accessible at any given time to a computer within the system by the addition of the ""^"" character. We say that it's persistent . The result of this is that a MUMPS programmer can tap a database directly rather than using a query. This is faster on its face, eliminating the query abstraction, but direct access also allows a bunch of alternative programming ideas. For one thing, as a programmer, I can take an item stored in one of those globals and give it ""children,"" which might be some additional properties of that item. So, we wind up with lists of different things that can be described and added to in different ways on the fly. The relationships are hierarchical. Here's the Wikipedia example, in which I take an item in a global database, a car, and give it a door and then give that door a color. SET ^Car(""Door"",""Color"")=""BLUE"" This gets to be a bit difficult, but we might say that a typical relational database is sort of a separate entity somewhere remote and it might be used by a bunch of different computers running a bunch of different programs. MUMPS packages the programming and the database itself into one package, which is potentially very fast. The language is still developed and exists in a number of different implementations, including the open-source Mumps/II , which is maintained by students at the University of Northern Iowa, as well as the aforementioned GT:M. You can also download ANSI Standard MUMPS here , which will work on a Raspberry Pi in addition to OSX and Windows under cygwin. The MUMPS claim to fame is the Veterans Health Information Systems and Technology Architecture (VistA) , which is a vast suite of some 80 different software modules supporting the largest medical system in the United States. It maintains the electronic health records for 8 million veterans used by some 180,000 medical personnel across 163 hospitals, over 800 clinics, and 135 nursing homes. VistA is used for nearly 200 distinct functions, ranging from MRSA tracking and reporting to vital signs monitoring and recording to accounts receivable. It's considered a model for current efforts to create a nationwide medical health records network, but also well beyond. A low-key corporation called InterSystems, mentioned above, has been building on a MUMPS-based (but not MUMPS-limited) system for decades—the technology, known as Cache, is now used by the European Space Agency to map the Milky Way. In 1966, when MUMPS was first developed by a pair of researchers working in an animal laboratory, the data that would even come pouring in from high-resolution sky mapping projects and social media platforms with hundreds of millions of users would have been unimaginable. But our bodies and their limitless data points were already there. Modern Medicine is a series on Motherboard about how health care and medical technology can move forward so rapidly while still being stuck in the past. Follow along here .",,Vice,In 1966 a pair of programmers saw the data future.,"Meet MUMPS, the Archaic Health-Care Programming Language That Predicted Big Data",https://www.vice.com/en_us/article/3dkmg3/meet-mumps-the-archaic-health-care-programming-language-that-predicted-big-data-2
Michael Byrne,"Csound is a creaky feeling, highly domain-specific programming language adored by an enthusiastic nucleus of musicians and audio programmers. At first glance, it's esoteric as hell, having little to do syntactically with other programming languages, and appearing instead as an ungainly hybrid of markup, assembly language, and C. Understanding why Csound is what it is takes commitment and perhaps even a leap of faith. The payoff, however, is machine-level audio rendering and, arguably, a level of control-slash-extensibility untouched by any digital audio workstation (read: Logic, Pro Tools) and-or audio programming environment (read: MAX/MSP, Pure Data, SuperCollider). ""Csound is a sound renderer,"" explains Richard Boulanger, a Berklee College of Music professor and one of Csound's leading acolytes, in the textbook Introduction to Sound Design in Csound. ""It works by first translating a set of text-based instruments, found in the orchestra file, into a computer data-structure that is machine-resident. Then, it performs these user-defined instruments by interpreting a list of note events and parameter data that the program reads from: a text-based score file, a sequencer-generated MIDI file, a real-time MIDI controller, real-time audio, or a non-MIDI devices such as the ASCII keyboard and mouse."" Image: CSounds.com One of the most basic appeals of Csound is just that it's text-based. It is a ""pure"" programming experience. Unlike, say, MAX/MSP, which is a similarly powerful albeit graphical audio programming tool based on data flow, CSound opens itself to the general world of algorithms by allowing the implementation of fundamental programming language paradigms, including basic control structures like loops and conditional (if ... then) statements. And so sound design and composition become natural extensions of bare code. Csound on its own has no graphical interface—it's composed in a generic text editor and then compiled (or rendered) via the command line. In practice, however, CSound code is more likely to have been created in one of several Csound-specific development environments, including CsoundQT, which now comes prepackaged with CSound installations. The big advantages in using such a tool are the ease in which GUI elements (widgets) can be created, step-through debugging , and a great big ""play"" button ready to render your project at the click of a button accompanied by a ""record"" button to save the results to a desired audio format. This is the ""hello, world"" of Csound, which, in addition to the usual printed output, features a basic sin-wave test tone. Note the two halves describing first the instrument to be utilized and the information to be performed. It's a bit like object-oriented programming in that the generic form of some structure or set of instructions is created and then instantiated with specific parameters elsewhere. The generic object declares a string of text, while the instantiated object declares that string of text to be ""hello, world."" Image: by the author Csound has a deep history, beginning well before even most general-purpose programming languages you know, which partially accounts for its peculiarities. It's the descendant of a long lineage of languages known as MUSIC-N , all of which are themselves descendants of the original MUSIC. MUSIC itself was created in 1957 at Bell Labs by a 30 year old engineer named Max Mathews and was the earliest program capable of generating digital audio waveforms, for music or any other sound-based application. The very first MUSIC program/composition was a proof-of-concept called ""The Silver Scale."" A few months later came ""The Pitch Variations.""    From MUSIC's 1957 birth to the latest Csound release, the MUSIC-N languages all share the same common features: signal-processing or synthesis routines called opcodes (or unit generators) are combined in different ways to create instruments. The basic idea is that these opcodes pass audio or data signals from one to another, with each successive opcode adding some new processing. It's text-based audio patching. Instruments, which are collections of opcodes working together, are then called or, properly, instantiated in the ""score"" section of the program. Like this, text gives birth to sound and that sound is digital music. Beyond Csound, many of the ideas Mathews realized via MUSIC and its children—such as array-based storage of waveform data—remain as hardware and software audio processing standards. Beyond Stereo A cool thing about the openness of Csound is in its capability for programming not just music, but music environments: spatialization. It's possible to compose music to be performed across many different channels in a really natural way—as natural as it is to craft two-channel/stereo music in a more conventional digital audio workstation. Imagine an orchestra of individual speakers, but well beyond: a room full of speakers, with each one offering some distinct but related sound or sounds. Music becomes a place. I'll let an example explain what I mean by that:  Live Coding Music Expect to see a lot more of this in the near future. Music ""hackers"" performing shows based entirely on a command line interface. It's already much easier than you might imagine to do this on a basic level—as you're reading this, I could write a few lines of code in C to play a simple sin-wave melody. But its potential, the level of low-level control offered by live coding music, could change the entire notion of computer music, or at least the still-pervasive notion of computer music as ""pushing a button."" Shoving electrons around a computer in a (more or less) direct fashion is no less literal than moving a bow across a string or hitting a drum or singing a note. I would hope, anyway. Some excellent thoughts on live coding, what it is, and, just as important, what it means:  Technically, Csound enables live coding in probably the most natural way in the whole of digital music. Csound is, after all, just code. GETTING STARTED Csound has a somewhat steep learning curve. It doesn't look like most any other programming language or behave like any other audio production environment. There are a few graphical tools created by outside developers, like blue and Cabbage , but I've tended to avoid these because, dunno, it seems to defeat the purpose of using a language like Csound in the first place. If you're more used to conventional timeline-based digital audio workstations than you are with code, a GUI-based tool might be the way to go. That said, Csound comes with loads of documentation, from the official and FLOSS manuals to Boulanger's book and several others, it's not hard to find a suitable guide. (There is even a Csound journal and a yearly Csound conference .) CsoundQT, which comes with a huge suite of examples, is a lightweight, free editor that's text-based with some visual elements. It's installed automatically with Csound. The whole package can be found here . Read more Know Your Language",,Vice,"Behind the janky-looking code, Csound offers sonic spatialization, live coding, and machine-level music production.","Know Your Language: Csound May Be Ancient, But It's the Audio Hacking Future",https://www.vice.com/en_us/article/pga53v/know-your-language-csound-may-be-ancient-but-its-the-audio-hacking-future
 ,"For the past six years, hapless Fallout 3 players have been living in a world of lies, riding the metro train in the game's Broken Steel add-on without a care in the post-apoclyptic world. But what they didn't know was literally lurking under the surface all along: they weren't on the train—they were the train. Hold onto your helmets, people. While it may look like your routine video game vehicle, upon entering the metro, the player is actually forced into wearing a ""train hat."" Thanks to programming workarounds, ""riding"" is just an illusion: inside the hefty accessory, which goes on the player like a piece of armor, the look and feel of locomotion is a special camera animation, adjusted to first-person perspective. There is no spoon, Neo. For some, the revelation comes with something of a ""Santa's actually been your parents the whole time"" vibe. The alleged originator of the discovery, gamer Trainwiz , published his find over four months ago on his personal Tumblr, but it didn't come to light until a visualization of the hack was shared just two days ago. Reactions have ranged from accusations of lazy deceitfulness on the part Bethesda, the game's developer, to disseminated misinterpretations and alternate NPC theories . To these naysayers and nonbelievers, however, Trainwiz writes , ""[The] people saying that this is somehow the fault of the engine or lazy, you guys are also pretty stupid. These kinds of solutions make up the bulk of game development. It's a clever, bug free solution that works brilliantly, even if it's only able to be used in one area. It's not like they're gonna waste time coding up a big system for running trains. Especially given Fallout 3 's... unfortunate trig functions."" In other news, Fallout 4 is in the works , promising unmatched open world detailing and plenty of material for whistleblowing, late-to-the-game Redditors. Hear 'em roar. Image via . Read up on the ""train hat"" debate on Reddit . Related: Incredible Worlds in Minecraft The AI Mario Kart Can't Beat Human Gamers—Yet Step Inside Grand Theft Auto New Zealand",,Vice,Everything You Know About 'Fallout 3' Is a Lie,Games,https://www.vice.com/en_us/article/nz4dk8/everything-you-know-about-fallout-3-is-a-lie
Michael Byrne,"Earlier this week, President Obama signed an executive order creating the National Strategic Computing Initiative , a vast effort at creating supercomputers at exaflop scales. Cool. An exaflop-scale supercomputer is capable of 1018 floating point operations (FLOPS) per second, which is a whole lot of FLOPS. But this raises the question: What's a floating point operation and why does it matter so much to supercomputing when our mere mortals of computers are rated according to the simple-seeming notion of clock speed? I haven't the faintest idea how many FLOPS this Lenovo might be capable of (though, as we'll see, it can certainly be calculated). David Bader, chair of the Georgia Institute of Technology School of Computational Science and Engineering, suggests we ""think of flops similar with horsepower to a car."" In buying the family Subaru (or whatever), you're interested in a whole list of different metrics: gas mileage, safety rating, reliability. But, if it's a racecar in question, e.g. a car that will do nothing but race around a track at top speed, the requirements are a lot more about horsepower, which is the thing wins races. So, the answer has to do with the difference in how your own computer and an extremely high-performance supercomputer are used. The latter is used for doing incredibly vast mathematical calculations while consumer computers are designed to run stuff like the browser you're currently reading this in. The key thing is that the browser's operation is all about your input and input from the internet itself. A browser spends a lot of time waiting around, in other words. And the same goes for most of the computing you do: it's input/output based. The distinction might not be very intuitive. What it means is that the computer programs you're used to running (or the sub-programs within those programs) are always stalling as they wait for other sub-programs (functions, methods, routines) and programs to finish or for some piece of input to arrive. We call this kind of computing sequential . Things happen only after other things have completed and the whole mess is characterized by complex inter-dependencies. What it looks like is logical decision making: if ... then, not, and, or, etc. Mathematical computation is in general a different sort of beast, for a couple of connected reasons. The first is that it's usually easy to do it using parallel, rather than sequential, processing. Big computations can often be broken down into smaller and smaller, easier and easier computations. Which means they can spread around to a bunch of different processors all working simultaneously. This is why GPUs are now becoming much more than graphics processing units—they exploit parallelism. Parallelism used to be useful largely for image rendering (where individual pixels can be operated on individually), but as computing becomes more and more data-focused, the idea has become much more general. So have GPUs. So, in a sense, you can imagine a GPU as a consumer-scale supercomputer. There's another characteristic of mathematical/scientific computing that sets it apart, however. This is just the nature of the numbers themselves, or their precision. Computing numbers more precise than whole numbers is a very different process. This is so much so that it occurs on its very own dedicated hardware. A CPU is built to operate on integers—whole numbers. Which is fine because it's not so much doing math as using numbers to model/implement the execution of a program that probably isn't a scientific calculator. But, that said, most any general purpose computer is going to need to calculate non-integer numbers—more properly known as floating point numbers—on a regular basis. This can't be accomplished on a CPU because floating point numbers are represented by computers in a very, very different way. An integer value described by a string of binary digits is just a simple base conversion. The numbers we use every day, which are somewhat confusingly referred to as decimal numbers even if they don't employ a decimal point, are represented in a base-10 counting system. This ""10"" is actually where the ""decimal"" of a decimal number or decimal point comes from. Every digit in a decimal number corresponds to a value of that digit multiplied by 10 raised to the position of that digit. So, the number 12 is 2 * 100 + 1 * 101 and so forth. That's the base-10 counting system. With binary, every digit is just 2 raised to the digit's position times either a 1 or a 0. So, given a decimal number, I can turn it into a binary number and vise versa just through some simple arithmetic. If I tried the same thing with a floating point number, it wouldn't work out at all. In a floating point number, digits mean very particular things. Some digits correspond to the actual digits of the number, some correspond to the base we're using to represent that number, and some correspond to where exactly in the number we should place a decimal point. In a sense, every floating point number must be decoded according to a small set of rules. Treat it in any other way and the number is just nonsense. This is why floating point numbers are computed on different hardware, which is known as a floating-point unit. An FPU is very much so built for long mathematical operations. It's possible to take the longest formula you can imagine, populated by the longest data set, and rearrange it a bit such that it can be fed through a floating-point unit in a really natural way. This is why when we talk about supercomputing, a land of deep mathematical problems and deep simulations based on mathematical problems, it's useful to talk about floating-point speeds. Floating-point numbers and parallel computations come together in how FLOPS are calculated. The basic idea is to take the number of floating-point calculations that can be performed in a single processor clock cycle (four, usually) multiply it by the processor's clock speed and then scale it by the number of available cores per socket connecting them. In the simplest case of a single core architecture operating at 2.5 GHz, we'll wind up with 10 billion (2.5 GHz * 4) FLOPS. So, based on the calculation above, it's possible to increase FLOPS without increasing processor clock speeds, which is good news for supercomputing if not for your MacBook. ""You'll notice that clock speed hasn't been increasing significantly in the last decade on consumer machines,"" Bader says, ""because we're hitting the limits of chip technology. Instead, we see chip makers like Intel, IBM, and NVIDIA, moving towards parallelism such as multicore and manycore processors."" Parallel programming hasn't quite become mainstream, and it will always be constrained by the percentage of a given computer program that must execute sequentially, but FLOPS increasingly matter, if not always at exascales.",,Vice,President Obama's call for exaflop supercomputing is as good a time as any to talk about floating-point operations per second.,"What Exactly Is a 'Flop,' Anyway?",https://www.vice.com/en_us/article/vvbajx/what-exactly-is-a-flop-anyway
Michael Byrne,"I'm not so sure Brainfuck is an actual brainfuck so much as it is exhausting. Nor was it meant to be a brainfuck, for that matter. By most accounts, the esoteric programming language's founder, Urban Müller, was interested in creating a Turing-complete language that had the smallest possible compiler, which is the in-between program that coverts high level languages like C to a given computer architecture's machine-representational assembly instructions. Müller wanted tiny, the complete minimum that a language could be and still be a ""real"" language. The Brainfuck compiler is down to about 171 bytes now, which is about half the capacity of all of the processor registers in a canonical x86 architecture combined. That is, you could imagine running it without using any memory outside of the actual processor cores. That is truly an accomplishment, of sorts. While Brainfuck is knotty as hell, there's a satisfying element. For one thing, any other programming language that I can think of takes assembly code and adds more structural and syntactical complexity while also adding new layers of abstraction. Which is the point of high-level languages. Things become simpler as code gets more specific and more abstract, but there's also a lot of extra stuff that comes with high-level languages and their myriad extensions and outgrowths. Below the surface, things become muddier, more opaque. Brainfuck, meanwhile, takes assembly instructions and reduces them to the barest whisps of code, which more or less approximates a Turing machine. It's an abstraction, but hardly one that makes programming any simpler or more intuitive or, well, reasonable. Surprisingly, coding in a highly unreasonable language is refreshing or perhaps even ... soothing, especially if you happen to already be a fan of sparse and immediate programming (e.g. C or assembly itself). Brainfuck is that, though sparse and immediate hardly means easy. Brainfuck consists of six commands. Assembly (or ASM) has at least a hundred different instructions, some that are completely primitive like ""add"" and some that are kind of just slight refinements of other instructions or instructions that could be represented in more primitive terms by combining other, more primitive instructions (like ""jump if less than or equal"" and ""jump if less than""). The basic ASM pallete, however—things you'd most likely to be using again and again—is more like a couple dozen. Which is still a lot more than Brainfuck. Nor can I think of any other language that's nearly so limited; languages, the ones used to actually do things, usually expand. Brainfuck does not expand. There are different versions of it implemented in different ways and corresponding to different compilers, but making Brainfuck larger and more functional is to miss the entire point of Brainfuck (though people certainly try). The six commands are as follows. I'll explain what they mean afterward. > : Increment the pointer. < : Decrement the pointer. + : Increment the byte at the pointer. - : Decrement the byte at the pointer. . : Output the byte at the pointer. , : Input a byte and store it in the byte at the pointer. [ : Jump forward past the matching ] if the byte at the pointer is zero.  ] : Jump backward to the matching [ unless the byte at the pointer is zero. If you don't know what a pointer is, that's surely just a bunch of nonsense. It's worth a quick lesson. A pointer in computer science/programming is basically a memory address. When I declare a variable within a given general-purpose programming language, I'm planting a flag for that variable in a specific memory location (or more likely a virtualized specific memory location). When I reference that variable in my code, I can refer to the contents of the memory location or I can refer to a pointer to that memory location. Say I declared the following in the C programming language: int i = 0 There's now a slot in memory corresponding to my variable. If I later take my i and add 1 to it, the value of i will be 1 (0+1). I can also reference the variable i by appending an ampersand symbol to it like this: &i. This will give me the physical memory address of that variable instead of the variable's contents. You can also take the memory address of a variable and add or subtract from it as you would any other value to give you a new memory address based on your variable's location in memory. This is a really good way to screw things up, actually. So, the basic idea is that a pointer is a location within a huge list of memory addresses. OK? It's possible to write a Brainfuck program with the same capabilities of any other computer program written in any other programming language. Let's get back to Brainfucking. Using Müller's original C implementation of Brainfuck—note: languages are only implemented in other languages, save for ASM itself—a Brainfuck program will be initially allocated 5,000 slots of memory. The ""real"" machine addresses of these slots are maintained by the C code as nornmal C pointers, but in Brainfuck we just imagine them to be indexed slots within a big array (a list, basically). So, at the start of a Brainfuck program, we imagine cell #0. (In programming, lists start at 0, not 1.) We have basically just one or two choices of how to proceed in writing our Brainfuck program. We can increment by one the value held within cell #0 (which is initialized to be 0) using the ""+"" command, or we can ignore cell #0 (which will then remain with a value of 0) and move on to the next memory address using the "">"" command. Say that I wanted to make cell #0 hold the ASCII character ""H,"" as if we were planning to output ""Hello, world."" I need the numerical value within cell #0 to be 72, which is the ASCII code for the letter H. The most straightforward way of accomplishing this is to just type out ""+"" 72 times, incrementing the value of cell #0 72 times. To get the next letter, we code a "">,"" which brings us to cell #1. Increment cell #1 101 times, for 101, the ASCII code for a lowercase ""e."" All we're ever doing is stepping forward and backward in memory while incrementing and decrementing the values we find by 1 and only 1 (per individual operation). Brainfuck has two input/output commands. ""."" will print whatever value is at the current memory location, while "","" will accept a single byte of input and place it in the current memory address. So, coding a period after having set cell #0 to 72, should output ""H."" In this respect, at least, Brainfuck starts to look a bit more normal. There's one more thing. Brainfuck has a control structure. In programming, a control structure might be any number of different things that somehow affect the flow of program execution through a section of code. Examples include if ... then statements (if this condition is met, execute this section of code, otherwise ignore it), for loops (execute this section of code some number of times), and while loops (execute this section of code until a given statement becomes untrue). Brainfuck's control structure consists of a pair of ""[]"" brackets. When the code encounters the first opening bracket, it will test to see if the byte at the current memory location is equal to 0. If it is, the code will jump to the command after the closing bracket. Otherwise, the code will continue as normal. This would normally be used like a for or while loop where some memory location is initialized to a value and every time the loop executes that value is decreased by 1. When it hits 0, the loop stops looping and the code continues on. Borrowed from this Brainfuck visualizer , here's the complete ""Hello, World"" code. You can see that it uses cell #0 as the counter, which is initialized to 10, so the loop (the stuff between the brackets) executes 10 times. +++++ +++++ initialize counter (cell #0) to 10 [ use loop to set 70/100/30/10 > +++++ ++ add 7 to cell #1 > +++++ +++++ add 10 to cell #2 > +++ add 3 to cell #3 > + add 1 to cell #4 <<<< - decrement counter (cell #0) ] > ++ . print 'H' > + . print 'e' +++++ ++ . print 'l' . print 'l' +++ . print 'o' > ++ . print ' ' << +++++ +++++ +++++ . print 'W' > . print 'o' +++ . print 'r' ----- - . print 'l' ----- --- . print 'd' > + . print '!' > . print ' ' Now we get to ask So what? Fair enough, but there is actually a point. Brainfuck is amazingly tiny but it's also Turing-complete. This means that it's possible to write a Brainfuck program with the same capabilities of any other computer program written in any other programming language. The syntax and control structures of Brainfuck may be limited, but the language's abilities are as unlimited as any other programming language that exists or that will exist, which is sort of another way of saying that it can doing all of the stuff a Turing machine can do, which is itself a way of saying that it can do anything, or that it can find an answer to any computational problem given enough time and memory. So, Brainfuck could be the only programming language that exists and we could have all of the technology we have now. Weird, eh. This has been proven several times, in fact. Daniel B Cristofani proved it directly by simulating a Turing-complete class know as a tag-system, which is used formally as a simulcrum of the Turing machine itself. Other proofs rely on showing reductions between Brainfuck and another esoteric language called P"" that functions as a direct Turing machine simulator and has been shown to be Turing complete by proof. (A reduction is a way of showing that a certain language or problem represented as a formal language [don't ask] can be shown to be a subset of another language or problem represented as a formal language.) Frans Faase has similarly shown that Brainfuck can be rearranged such that it itself becomes a direct representation of a Turing-machine. Getting started If you're into suffering, here's an online Brainfuck visualizer (same as above). It's worth screwing around with a bit. Beyond that, you'll find that Brainfuck is parent to a vast family of derivative languages, some adding features like procedures/functions , multithreading, strings and repetition , Arduino capabilities , math functions , and built-in debugging capabilities . At least one Brainfuck extension, known as Smallfuck, actually removes capabilities, offering operations only on individual bits and limited data storage (which makes it Turing-incomplete). Meanwhile, A cursory Google search reveals a vast pit of different Brainfuck implementations (beyond the original C version mentioned above) and compilers. You're on your own for that. Read more Know Your Language .",,Vice,Getting to know the esoteric arch-minimalist of programming languages.,Know Your Language: Brainfucked by Brainfuck,https://www.vice.com/en_us/article/8qxyj3/know-your-language-brainfucked-by-brainfuck
Alix Jean-Pharuns,"Have you ever browsed a really well designed web page or relied on a particularly useful app? Your first thought was probably, ""this is cool and I'm glad it exists."" If you bothered to look for who's responsible, you'd probably unravel a thread back to some corporation or web development group that wrote some open source code that eventually became the backbone to the thing you've enjoyed using. But if you've ever thought, say, ""I wonder who animated that cute little 'poof' in Chrome when you delete a bookmark,"" that search would be a lot more difficult. Today, the minds behind Thinkup , a social media insight service, have launched a new directory called Makerbase that might make that search easier. ""I can watch a movie and see the credits at the end and know who made it,"" said Anil Dash, who co-founded Makerbase with Gina Trapani. ""But I spend a lot more time on the web than I do watching movies, and I have no idea who made these things. There's actually no way to find out."" Makerbase aims to be a way for developers and users to get more clarity about who builds the web tools we use every day. ""There's no way to explicitly say 'I was working for this company when I did that.'"" Obviously developers have always needed ways to know who has contributed what to a project. Version control systems like Git, which creates dynamic repositories of source code and contributors, are not rare. If I want to know who wrote the functionality to an open source PDF extractor that lets me implement it in Python instead of Java, I could probably find that on Github, a web based software service that combines Git with an easy-to-understand graphical interface. For the smaller things, like this poof I mentioned earlier that I like so much, finding who made them can be a challenge. You should think of it as more an ""IMDB for developers"" as opposed to another LinkedIn or Facebook. The profiles are more personal and colorful, and at least for now, job titles aren't even displayed. It's more about the people who make things as opposed to the people who employ them to. ""There's no companies at all,"" Dash said. ""There's no way to explicitly say 'I was working for this company when I did that.' And that's on purpose...Putting your resume online or making a LinkedIn profile doesn't really show the way that a lot of people work. Especially someone that builds a cool project in nights and evenings that nothing to do with their employer or day job."" Makerbase launches to the public today. Who knows if developers will embrace it en masse and offer themselves up to the public, but its ambitions are certainly grand. Dash believes it can be a tool in the fight for greater diversity and representation in tech. ""So our last project was an open source project,"" he points out. ""Gina's probably the most prominent lesbian coder in the world, certainly up there. I think it's no accident that we ended up with the biggest project on Github where the majority of code is written by women. I think that can be true for a a lot of other projects, to just say there's somebody like me there, maybe I can be a part of that.""",,Vice,Who made your favorite part of your favorite app?,Makerbase Is an IMDb for Developers,https://www.vice.com/en_us/article/ae3mq4/makerbase-is-an-imdb-for-developers
Kari Paul,"Feminist non-profit organization the Ada Initiative announced Tuesday it will be shutting down after four years of working to advance women in open technology and culture. The organization launched a variety of efforts to that end, crafting anti-harassment policies for tech conferences, launching ""AdaCamp"" events to increase women's participation in open technology, and creating an Ally Skills workshop that taught men how to use their societal advantages to combat sexism in their communities. Valerie Aurora told me by email she and other co-founder Mary Gardiner believe the Ada Initiative, named for a female mathematician known for creating the first computer program ever in the 1800, made major headway one of the main issues it set out to tackle: sexual assault and harassment at tech conferences. ""The problem wasn't widely visible at the time we founded the Ada Initiative, and not five years later, thousands of conferences are using anti-harassment policies based on our model policy, many of which we have never directly worked with,"" she said. ""Women are much more aware that being a public presence on the Internet could result in being driven from their homes over the most mundane and trivial topics."" A statement on the organization's website said it will shutter in mid-October after using its remaining funds to complete its ""current obligations."" The post explained the shutdown is due to changes in administration and a struggle to find someone to fill the executive director role. ""We don't feel like non-profits need to exist forever,"" the organization wrote. ""The Ada Initiative did a lot of great work, and we are happy about it."" Aurora said the co-founders hope their organization has made it easier for other groups to do similar work in the future and even ""do less things than we did."" ""We think we have paved the way for further non-profit and also for-profit and private efforts to continue improving the number and status of women in open technology and culture,"" she wrote. She said they see a future in that activists can choose to work for a for-profit model as more and more corporations realize the value of feminist organizations in creating change in their organizations. She said problems that have grown since they began the organization include tokenization of women in tech and online harassment. ""Online harassment is much more coordinated now and some harassment campaigns involve thousands of people,"" she wrote. ""Women are much more aware that being a public presence on the Internet could result in being driven from their homes over the most mundane and trivial topics."" All of the policies, codes of conduct, anti-harassment framework and more that came out of the initiative will be published so the mission can continue and be modified by other groups. ""We've worked hard to make sure that all our major projects can be continued by the community or by other activist organizations in some form,"" Aurora said. The initiative's conference anti-harassment work is available for free reuse and modification on the Geek Feminism wiki . It has also shared the Ally Skills Workshop materials publicly as well as "" AdaCamp Toolkit "" for reuse and modification so that others can run similar events. Aurora said the organization will share Impostor Syndrome Training materials in August.",,Vice,The feminist organization will publish its training materials so others can use them. ,"The Ada Initiative, a Women in Tech Non-Profit, Is Shutting Down",https://www.vice.com/en_us/article/3dk99n/the-ada-initiative-a-women-in-tech-non-profit-is-shutting-down
Michael Byrne,"Computer scientists at University College London have developed a tool that automatically isolates and extracts code from one program and ""transplants"" it into another. The tool, known as MuScalpel, has the potential to, ""ultimately change the understanding of the word 'programmer' considerably,"" as UCL systems engineer Mark Harman told Wired UK recently . It does so by offering a way around perhaps the most tedious aspect of software engineering, which is re-implementing solutions to problems that have already been solved by other programmers in other programs—""recreating the wheel,"" in Harman's words. This may slightly underestimate the talents of programmers currently at work on those wheels. Good code is (already) modular, and this is a central tenet of programming. A good coder doesn't just sit down and bash out tens of thousands of lines of code in one giant torrent, they build their programs up piece by piece, always with an eye toward modularity. If I noticed that I'm rewriting something similar to what I've already written or am, heaven forbid, copy and pasting my own code, then it probably means that I need to make that segment of code into its own routine—a discrete package of code that is implemented only one time in one file, but can be invoked any number of times just by using the routine's name. This works the other way too and it's why programming libraries exist. These are the vast storehouses of pre-written code, routines, and other features that are called on constantly in the software development process, even at the most basic ground levels. If I want my C++ program to open a file stored on a hard-drive, I don't recreate this entire process, I use a class called fstream , which comes with a pre-written function called open() that does this work for me. Much of programming is knowing how and when to call on this unfathomably large base of pre-existing code and how to put it together in useful ways. This is essentially what MuScalpel wants to do for us, a process that Harman and his group liken to organ transplantation. The tool identifies first both a piece of code within a ""donor"" program and an entry point into that piece of code, or a ""vein,"" which is the path leading from the beginning of the donor software to the beginning of the code-organ to be transplanted. This is trickier than it may sound. ""A programmer must first identify the entry point of code that implements a feature of interest,"" Harman and co. explain in a recent paper . ""Given an entry point in the donor and a target implantation point in the host program, the goal of automated transplantation is to identify and extract an organ, all code associated with the feature of interest, then transform it to be compatible with the name space and context of its target site in the host."" The concept enabling MuScalpel to actually work is known as genetic programming . This is a biological evolution-inspired machine learning technique that takes a set of instructions and some evaluation criteria and lets an algorithm ""find"" the computer program best suited to a particular task. Eventually, just as evolution promotes traits beneficial to survival/reproduction, a genetic programming scheme promotes code that that is most beneficial (or is beneficial enough, rather) in terms of the given fitness function. In a sense, it's a mechanism by which programs design themselves according to our demands. It's probably the future, generally. ""While we do not claim automated transplantation is now a solved problem, our results are encouraging."" The first stage in the process is the identification and removal of the target organ, beginning with an ""over-organ,"" which is all of the code in the donor that implements said organ. Next, the tool creates a system dependence graph, which is a way of breaking a program apart into its constituent pieces and charting out the interdependencies between them. This extracted material is then used to ""grow"" a new organ in the transplant recipient that both does what it's supposed to and doesn't mess anything up. As in human organ transplantation, this is no easy task. The extracted organ comes with its own set of required inputs, or parameters, which may be representative of the donor and not the host. So, the first step in implantation is matching variables (which might be imagined as data blood vessels) with the organ's parameters. Via genetic programming, the over-organ is evolved in situ into a final organ product that's compatible with the host. Once it passes a given test suite, it's more or less good to go. Most importantly, the MuScalpel algorithm works. ""While we do not claim automated transplantation is now a solved problem, our results are encouraging,"" Harman and his group write. ""We report that in 12 of 15 experiments, involving 5 donors and 3 hosts (all popular real-world systems), we successfully autotransplanted new functionality and passed all regression tests. Autotransplantation is also already useful: in 26 hours computation time we successfully autotransplanted the H.264 video encoding functionality from the x264 system to the VLC media player; compare this to upgrading x264 within VLC, a task that we estimate, from VLC's version history, took human programmers an average of 20 days of elapsed, as opposed to dedicated, time."" So, does this mean a whole bunch of programmers are out of a job? Harman doesn't think so. It just means they won't be doing boring stuff anymore. They can be free to create and innovate. Etc. ""We want to free programmers from their shackles, not to make them redundant,"" he told Wired UK. He likens it to the human ""computers"" of the 1940s that electronic, programmed computers replaced. ""The computation was tiresome and repetitive, but it required some of the most skilled and intelligent humans, since it had to be correct. Today, that meaning of the word computer is anachronistic."" Fair enough, but there are a whole lot of programmers that exist only because of the tedious jobs, the everyday coders skilled at banging together JavaScript but perhaps less so at developing innovative algorithms. A better comparison might be assembly line automation, where engineers and technicians may keep their jobs, but the factory floor winds up getting fucked.",,Vice,"A new system offers an automated way of reusing (""transplanting"") existing code into new projects.",MuScalpel Is an Algorithmic Code Transplantation Tool,https://www.vice.com/en_us/article/bmj9g4/muscalpel-is-an-algorithmic-code-transplantation
Michael Byrne,"We like to talk about universality and portability in computer science. Alan Turing gave us his Turing Machine and, with it, the concept of Turing completeness . A complete programming language is capable of computing everything that is computable. What an amazing and bizarre idea that is, but completeness is something we happen to be able to prove (!). Though he was very well versed in quantum mechanics, Turing didn't seem to catch on to the classical physics limitations of his machine. Though you couldn't blame him: The quantum computing alternative is that weird, a realm where the very notion of information is unlike what was once assumed to be an unwavering polarity between true and false. True and false is just logic or logical reasoning, and it's ancient. Computers with transistors smaller than viruses still deal with true and false via gates and circuits that process and arrange all of those trues and falses into meaning. Computation. Quantum mechanics allows something different, which are states of trues and falses together, squished into probabilistic arrangements of cohabiting ones and zeros. This is so fundamentally different and counter-intuitive that it still seems impossible. But it is possible, or it seems to be. What does this mean for programming languages, the very things that we use to interface with our machines? This is the question approached by Benoît Valiron, a researcher at Laboratoire de Recherche en Informatique, and a small group of computer scientists in this month's Communications of the ACM via a paper titled ""Programming the Quantum Future."" It describes an entire quantum computer architecture, along with a quantum programming language dubbed Quipper, which is a bit beyond this post, but the authors were kind enough to offer a useful video introduction (above). It manages to pack quite a bit in. In any case, I'll try to go deeper on Quipper later this weekend—I'm still reading the paper!—but, in the meantime, consider yourself primed.",,Vice,"We're circled back to the beginning of computing, which means creating everything again. NBD.",Quipper Is a Language To Program the Nearing Quantum Computing Future,https://www.vice.com/en_us/article/9akx37/quipper-is-a-language-to-program-the-quantum-computing-future
Michael Byrne,"Informatics researchers from the University of Zurich have developed a not at all sinister-sounding system capable of predicting the quality of code produced by developers based on their biometric data. By using heart rate information, for example, they were able to quantify the difficulty a given programmer had in producing a piece of software. This information could then be used to identify likely sections of bad code. Pre-crime for software debugging, in other words. The Zurich researchers, Sebastian C Müller and Thomas Fritz, describe their work in a paper presented this week at the 38th International Conference on Software Engineering in Austin. Here's the not at all sinister way that Müller and Fritze open their paper: ""A commonly accepted principle in software evolution is that delaying software quality concerns, such as defects or poor understandability of the code, increases the cost of fixing them. Ward Cunningham even went as far as stating that 'every minute spent on not-quite-right code counts as interest on that debt.'"" A common first line of defense against bugs and just poor quality code is the code review, the duo notes. Basically, one developer writes some code and then passes it on to someone else, who sort of acts like a code editor. The reviewer/editor looks over the completed code for defects and places where the code might be improved. This is a costly system, however: Code reviews take time and people. Automated review systems exist and sort of work, but they run up against two barriers. ""First, they are predominantly based on metrics, such as code churn or module size, that can only be collected after a code change is completed and often require access to further information, such as the history of the code,"" the paper notes. ""Second, they do not take the individual differences between developers comprehending code into account, such as the ones that exist between novices and experts."" Enter biometrics. By looking at the programmer as they program, rather than the code after the programmer is done writing it, the system described by the Zurich researchers finds code quality issues as the code is being produced. Previous research has already show that heart rate variability (HRV) or electrodermal activity (EDA) can be accurate indicators of task difficulty or difficulty in comprehending code snippets. The more difficult the task, the higher the cognitive load and chances for errors to be made. They tested this out with two teams of developers from Canada and Switzerland. Biometric data was collected as they wrote software and this was correlated with interviews with the developers and human-based code reviews. The study's findings: Amongst other results, our study shows that biometrics outperform more traditional metrics and a naive classifier in predicting a developer's perceived difficulty of code elements while working on these elements. Our analysis also shows that code elements that are perceived more difficult by developers also end up having more quality concerns found in peer code reviews, which supports our initial assumption. In addition, the results show that biometrics helped to automatically detect 50 percent of the bugs found in code reviews and outperformed traditional metrics in predicting all quality concerns found in code reviews. The paper at least notes (barely) the obvious privacy concerns with this, but it doesn't go much further than that. It's more than just a matter of privacy, really, but of general invasiveness (which is a bit different) and the really glaring, obvious problem of monitoring programmer stress as they program itself sounding like the most stressful scenario ever—like a coding interview that never ends where you also happen to be naked. Like that. Bleak, anxious times ahead.",,Vice,Fuck.,Researchers Use Developer Biometrics to Predict Code Quality,https://www.vice.com/en_us/article/9a33d7/researchers-use-developer-biometrics-to-predict-code-quality
 ,"One of Eric Fischer's tweet maps. Images via Using the geotagging data from from Twitter’s public API, data artist and “ map geek ,” Eric Fischer created the most detailed tweet map ever . With the help of his Mapbox article that outlines both his creative process and the tools he built for the project, anyone can replicate his beautiful maps. Fischer first connected to Twitter’s “statuses/filter” API and received the Tweets in JSON, a format used to transmit data between a server and web application. Because the JSON format came with more metadata than he needed for his map, he created his own program to parse the streams for only the essential information: username, date, time location, client and text. Twitter Map, “Los Angeles,” 2014 “Even though there are six billion Tweets to map, only nine percent of them are ultimately visible as unique dots. The others are filtered out as duplicate or near-duplicate locations,” Fischer explains in his how-to. To clarify the map, he filtered out duplicate or near-duplicate locations, and eliminated the “banding” effect of Tweets sent from iPhones. When the viewer zooms in and out of areas on the map’s website , the density and massive scale of the glowing green dots becomes clear. Fisher talks about how the challenge of achieving this effect is finding a way to “include all the detail when zoomed in deeply while unobtrusively dropping dots as you zoom out so that the low zoom levels are not overwhelmingly dense.” For example, he continues, at zoom level 0, when the viewer sees the whole world, there are 1586 dots. At zoom level 14, there are 590 million. In an interview with CityLab , Fischer states that he believes a successful map is one that can “confirm something that the viewer already knows about their neighborhood or their city, and then broaden that knowledge a little by showing how some other places that the viewer doesn't know so well are similar or different.” Fischer is making milestones as he brings cartography into the digital age through stunning visuals and data-filled social maps. Twitter Map, “O’Hare,” 2014 Check out Fischer's maps below, and to see more of his projects, visit his Flickr page , or keep up with him on Twitter . This article was originally published on December 12, 2014. Related: Digital Maps Inspired By Joy Division's ""Unknown Pleasures"" Cover Sprawling 'Snow Drawings' Transform a Mountain into Art Epic Data Maps Let You Vicariously Run Through NYC",,Vice,How to Create Beautifully Detailed Maps Using Twitter Data,Entertainment,https://www.vice.com/en_us/article/kbnb9n/heres-how-you-yes-you-can-make-the-most-detailed-tweet-map-ever
Sarah Emerson,"Over the years, programmers have developed algorithms to beat the Rubik's Cube in shorter, faster, and more efficient ways than ever before. And they're even teaching AI to do it. But now, a student named Martin Španěl has created a program called Mistr Kostky that not only understands the Rubik's Cube, but shows you how to solve it using augmented reality. According to Španěl, the AI can detect two or three sides of the cube at once, and relies on the OpenCV library for image processing. Augmented reality is used to overlay instructions that indicate what your next move should be. The programmer, who says he developed the app for his bachelor's thesis project at Charles University in Prague, used the existing two-phase algorithm created by Herbert Kociemba to solve the cube. Mistr Kostky works in two phases . First, the program ""detects the permutation of the cube. The faces of the cube can be shown in arbitrary order. Errors are automatically corrected."" Finally, after the program has figured out how to solve the Rubik's Cube in 20 moves or less, it uses arrows to show you how to navigate the cube. However, when asked on Reddit what happens when someone deviates from the program's recommended moves, the author replied that users must restart the puzzle if this occurs.    Španěl's program seems to have been built upon previous attempts at guided Rubik's Cube solutions. In 2009, an iOS app called CubeCheater allowed users to take photos of cube faces and see recommendations for how to solve the puzzle in as few moves as possible. VIPAAR Labs demonstrated in 2013 that Google Glass could help wearers beat a Rubik's Cube using a rudimentary guided overlay created by someone else pointing out instructions. The app was originally created for Android, and Španěl is hoping to make it faster and usable on most cubes, calling it a ""work in progress.""",,Vice,Cheating or winning?,Augmented Reality Lets You Solve a Rubik's Cube In 20 Moves Or Less,https://www.vice.com/en_us/article/8q88e5/augmented-reality-lets-you-solve-a-rubiks-cube-in-20-moves-or-less
Larry Fitzmaurice,"Check out the promo trailer for MTV Classic above. Hey, you: Do you have cable? Does your cable provider carry VH1 Classic? Have you watched VH1 Classic at all, ever? That, my friends, is why the channel is rebranding itself as MTV Classic and coming your way August 1. According to Deadline , MTV Classic will launch in place of VH1 Classic next month, bringing back the music channel's classic shows, like Beavis and Butthead , Pimp My Ride, and Laguna Beach . It will also feature a nonstop buffet of throwback music videos, movies, and concert footage . The reboot will kick off at 6 AM that day with MTV Hour One , a rebroadcast of the channel's first hour of programming, which aired 35 years to the date. (For all you non-cable-heads, MTV Hour One will air on Facebook Live, too. Modern, meet retro!) Arguments can be made for and against the idea itself of MTV Classic —""Hey, now I can watch this show I've been illegally streaming forever!"" or ""Goddamn it, I'm so sick of perpetual nostalgia!"" It does sound, however, like the channel's programming will prove more interesting than, say, a marathon of Ridiculousness —which seems to be on all the time now. Less Ridiculousness ! More Wonder Showzen ! Follow Larry Fitzmaurice on Twitter .",,Vice,Get ready to travel back in time—with the push of a button on your remote control.,MTV Classic Is a Nonstop Nostalgia Trip from the Channel’s Glory Days,https://www.vice.com/en_us/article/9b8we3/mtv-classic-vgtrn
Matthew James-Wilson,"If you've been paying even the slightest bit of attention to this year's presidential shitshow , you've seen at least a few Donald Trump's insane Twitter attacks. But since there are almost 20,000 of them, it's been next to impossible to sift through them all. That's all about to change thanks to a new website, which lets anyone search through a comprehensive database of every shitty thing the Republican candidate has ever said on Twitter from 2009 on, the Atlantic reports . The Trump Twitter Archive is the brainchild of a for-now-unnamed programmer and former Peace Corps volunteer who is funding the whole project with the help of donations. On the site, the treasure trove of more than 16,000 tweets that the politician typed out with his tiny, tiny hands is divided into categories, like Conspiracy Theories and Retaliation, or can be searched by keyword—like "" lyin' ted cruz "" or "" loser, "" for instance. Here, let's try it out. How about..."" weak .""   Now let's try "" disgusting .""  Yikes! OK, what about "" wind turbines .""  Here's one about Obama !  Now you try! Read: A Hand Model Casting Agent Told Us Donald Trump's Hands Are 'Childlike' and 'Severely Weatherbeaten'",,Vice,"You can search the treasure trove of more than 16,000 tweets by key word—like ""Crooked Hillary""—or just browse through the categories, like Conspiracy Theories or Retaliation.",Someone Made a Searchable Archive of Donald Trump's Tweets,https://www.vice.com/en_us/article/kwkzxw/someone-made-a-searchable-archive-of-trumps-tweets-vgtrn
Michael Byrne,"Over the past few months I've been screwing around a lot with Javascript drawing libraries for a software/research project that involves displaying hierarchical trees (family trees, archetypically) that don't look like total shit. Which turns out to be a hard problem that computer scientists have been wrestling with for about as long as they've been able to draw graphics on screens (50-ish years) and have still only solved imperfectly. While working on this has involved a great deal of hair pulling, it also involves a lot of P5.js , which is way fun. P5 is a relatively recent addition to the Javascript drawing scene and counts as the JS implementation of the Processing programming language—itself a highly popular venue for creating dynamic digital art and cool visualizations. Like Processing, P5 is a really good time just out of the box and easy to get into. Unlike Processing, P5 is built for the internet and is implemented in an internet language whose kind of mushy peculiarities and syntactical leniency tend to work well for what people like to call ""creative coding."" This, friends, is animation In other words, in about 30 minutes you could be posting your creations to the internet for all to see. So, let's do that. Note that this will be easier overall than the previous Python-based Hack This tutorials , but will still involve some basic, basic coding. Prerequisites: Like Processing, P5 has its very own code editor ""sketchpad"" thing, which is new and still fairly limited. Its basic utility is to quickly and automatically render P5 sketch files on a locally-hosted web page. By default, these pages are then viewed via the P5 editor's own little built-in browser. Let's start just by having the P5 editor downloaded and installed on your system. Note also that you can access tons of examples under the file menu. 0) Draw a shape, any shape Before actually drawing anything, let's make a stop over at the P5 API . Here you'll find everything that P5 actually does. These are all functions that enable all of the cool stuff that P5 can do. This includes drawing, but also lots of stuff with sounds, images, video, and user interaction. Some things are more obvious than others, and some of those that seem obvious are or can be very complex. Sometimes it just depends on the context. So, pick a shape. I'd start with a circle (ellipse) or rectangle (rect). Click the link in the P5 API and you'll get some examples and a nice page of documentation—a quick guide for using that particular shape (or other P5 function). Here, let's assume you picked an ellipse to draw and now we can head back over to the P5 editor. If you look at the documentation page for an ellipse—which you should do—you'll see that it requires four ""parameters."" This is information that we have to pass to our ellispse about itself or about how it should be drawn. These are intuitive enough. The first two are x and y coordinates giving the location of where we want the ellipse to be drawn, and the last two correspond to the height and width of the ellipse we want to draw (if the height equals the width, we have a circle, obviously). In just a second I'll get into the layout of a P5 sketch in some more detail, but for now we're going to include our ellipse in between the brackets following the ""setup"" function: function setup() { ellipse(200,200,50,50); } function draw() { } Cool, now click the play button and see what happens. Nothing! Nothing happened. That's because we tried to draw something but don't have anything yet to draw onto. Let's change that by calling the createCanvas() function , which we need to give two parameters corresponding to the desired size of our drawing area. Almost every time we make a P5 sketch this is something we need to do and createCanvas() should only be called once per sketch or else things can get pretty messed up. Do this and press play: function setup() { createCanvas(720,720); ellipse(200,200,50,50); } function draw() { } Cool circle, dog. 0.5) A note about coordinates The coordinate system in P5 (and HTML in general) is funky. Rather than having the (0,0) origin of a two-dimensional system be in the bottom left as we usually see it out in the world, the origin is at the top left. Consequently, as we go from the top of a page to the bottom, the y coordinate counts upward. This can get confusing. 1) The canvas There are two fundamental elements of a P5 sketch, which can just be considered defaults. As we've already seen, these are draw() and setup(). What they do is simple. When a sketch runs (when we hit the play button in the editor or when it's called via some other method) everything within the setup() block of code runs one time and only one time. This is where we do all of our initialization, such as creating the canvas but really any number of other things too. Let's pause just a moment to look at something cool. You can look at the P5 code under ""JS,"" though it probably won't make too much sense yet. That's fine. Note here that you can also make and run P5 sketches online at, yep, Codepen . See the Pen &amp;amp;amp;amp;amp;amp;amp;amp;amp;amp;amp;amp;amp;amp;amp;amp;amp;amp;amp;lt;a href='https://codepen.io/winkerVSbecks/pen/zgayr/'&amp;amp;amp;amp;amp;amp;amp;amp;amp;amp;amp;amp;amp;amp;amp;amp;amp;amp;amp;gt;Lerping Triangle&amp;amp;amp;amp;amp;amp;amp;amp;amp;amp;amp;amp;amp;amp;amp;amp;amp;amp;amp;lt;/a&amp;amp;amp;amp;amp;amp;amp;amp;amp;amp;amp;amp;amp;amp;amp;amp;amp;amp;amp;gt; by Varun Vachhar (&amp;amp;amp;amp;amp;amp;amp;amp;amp;amp;amp;amp;amp;amp;amp;amp;amp;amp;amp;lt;a href='http://codepen.io/winkerVSbecks'&amp;amp;amp;amp;amp;amp;amp;amp;amp;amp;amp;amp;amp;amp;amp;amp;amp;amp;amp;gt;@winkerVSbecks&amp;amp;amp;amp;amp;amp;amp;amp;amp;amp;amp;amp;amp;amp;amp;amp;amp;amp;amp;lt;/a&amp;amp;amp;amp;amp;amp;amp;amp;amp;amp;amp;amp;amp;amp;amp;amp;amp;amp;amp;gt;) on &amp;amp;amp;amp;amp;amp;amp;amp;amp;amp;amp;amp;amp;amp;amp;amp;amp;amp;amp;lt;a href='http://codepen.io'&amp;amp;amp;amp;amp;amp;amp;amp;amp;amp;amp;amp;amp;amp;amp;amp;amp;amp;amp;gt;CodePen&amp;amp;amp;amp;amp;amp;amp;amp;amp;amp;amp;amp;amp;amp;amp;amp;amp;amp;amp;lt;/a&amp;amp;amp;amp;amp;amp;amp;amp;amp;amp;amp;amp;amp;amp;amp;amp;amp;amp;amp;gt;. ""Lerping Triangle"" by Varun Vachhar  In a P5 sketch, the draw loop is where the action happens. In the example above, we put our ellipse() within setup() because it only needs to run once. It's drawn to the page and that's it. It doesn't change and, within setup(), it can't change because it's just drawn the one time. The draw loop is different. Once our sketch is launched, draw() executes continuously. We can expect the code within to be executed about 60 times per second, but this is far from a guarantee and will depend greatly on what the system on which it's running can provide. Running code continuously in a loop means that we can program things that change over time. Each execution of a draw loop represents a frame and there are any number of ways that we might write code that changes its output from frame to frame. This, friends, is animation. 1.5) A note about functions This is a tricky concept, but we need to get it out of the way. Generally, in computer programming a function (or method or procedure, alternatively) can be understood as a self-contained block of code that can be written and then stashed in one location somewhere and then referenced elsewhere by name. It represents some unit of functionality that will likely be used more than one time. To use a pre-written function, we have to follow its rules. It may require us to give it some parameters (or input data) of a certain type, for example. And, in exchange, the function may return a specific piece of data or it might change the state of something in a program. It might just be used to output some bit of information to the screen or to gather some information from a file or keyboard or trackpad. Some functions are written and then used only within the context of a single computer program, but others may be part of huge APIs used by millions of programmers. This latter, centralized way of doing things saves work, obviously, but also guarantees some amount of consistency across programmers and programs. (It's also the soul of open-source software.) When we made our ellipse above, we used a function. That's what ellipse() is. We give it four items of data and it takes that data off somewhere and returns a nice two-dimensional shape. I don't know how exactly it does this, but it's useful to me to not have to care. The ellipse-making function is a funny thing though. To say that it ""makes"" an ellipse is not quite right. I mean, it does, but the ellipse is also an entity itself, in a sense. When we make an ellipse in P5, the ellipse() function spits out an ellipse object. This object counts as a specific instance of the function we called to make it. So, it's as if the function produces the specific ellipse on our page but is also the blueprint for all ellipses. You'll see this a lot in JavaScript: Blueprint-like ""objects"" that are also functions. In other words, functions may be performing some task in the usual sense of a function, but they might also be representing an abstracted entity or ""thing."" It's weird, but winds up making sense. 2) Animation, finally So, we have a circle and it's pretty boring. We should probably make it do something. Let's start by moving our circle from where it is right now in the setup() section of the sketch to the draw() section. Now, setup() has the createCanvas() function and draw() has the ellipse(). Go ahead and run it. Great ... the same circle. The sketch is actually redrawing the circle something like 60 times per second, but you can't see it because it's just redrawing the same stupid circle in the same place. To make the circle move we need to change stuff about it over time. For this, we need variables. A variable is simply a name we can declare as some unit of data, like a number or string of characters. We can take that variable name and reassign its values, do computations with it, and, crucially, we can use it in many places at the same time, though it may take on different values at different points in the execution of our sketch-program-thing. Let's illustrate this by making our circle, but instead of just plugging a number in for the x coordinate parameter, we give it a variable name. To do this, we have to first declare the variable, which in Javascript just means saying ""var some_variable_name."" Without the ""var,"" JavaScript won't see a variable and will instead just see some garbage and give you a bunch of errors. Other languages will want a data type , but JavaScript is cool with just var. Note that I'm declaring xCoordinate outside of both the setup() and draw() functions. This is for a couple of reasons. One is that declaring it ""globally"" means that both functions have access to it because variables declared within one function are generally not available outside of that function (outside of the scope of that function). It also means that the data I store in that variable persists through successive draw loops. If I had declared it within draw(), it would just keep redeclaring it as if it were a new variable and our data would be lost. Try this: var xCoordinate = 200; function setup() { createCanvas(720,720); } function draw() { ellipse(xCoordinate,200,50,50); } Can you even believe that I made you draw the same fucking circle a third time? Here's a reward: See the Pen &amp;amp;amp;amp;amp;amp;amp;amp;amp;amp;lt;a href='https://codepen.io/p5art/pen/MaaNLR/'&amp;amp;amp;amp;amp;amp;amp;amp;amp;amp;gt;how Bézier curves are created&amp;amp;amp;amp;amp;amp;amp;amp;amp;amp;lt;/a&amp;amp;amp;amp;amp;amp;amp;amp;amp;amp;gt; by Jerome Herr (&amp;amp;amp;amp;amp;amp;amp;amp;amp;amp;lt;a href='http://codepen.io/p5art'&amp;amp;amp;amp;amp;amp;amp;amp;amp;amp;gt;@p5art&amp;amp;amp;amp;amp;amp;amp;amp;amp;amp;lt;/a&amp;amp;amp;amp;amp;amp;amp;amp;amp;amp;gt;) on &amp;amp;amp;amp;amp;amp;amp;amp;amp;amp;lt;a href='http://codepen.io'&amp;amp;amp;amp;amp;amp;amp;amp;amp;amp;gt;CodePen&amp;amp;amp;amp;amp;amp;amp;amp;amp;amp;lt;/a&amp;amp;amp;amp;amp;amp;amp;amp;amp;amp;gt;. Jerome Herr Now, let's get on with it. We need to change the value of xCoordinate across different frames, which means changing it through time. One really cheap way to do this is just to add some small amount to xCoordinate every time the draw loop executes. So, add this line to draw(): xCoordinate = xCoordinate + 3; Click play and look at it go! Let's give it a color. Let's give everything a color. This is a bit weird because we don't give things in P5 colors directly. Instead, we call a function called fill() right before we call the actual shape-making function. You'll want to read about fill() here but for now just understand that I'm using it to assign a color to the ellipse. The color here is formatted in hexadecimal because that's what I have laying around, but fill() takes all sorts of different parameter types. Note that I'm also giving a color to the sketch background, which is happening in the setup() section because I don't need to keep declaring it 60 times every second. var xCoordinate = 200; function setup() { createCanvas(720,720); background('#59747D'); } function draw() { xCoordinate = xCoordinate + 3; fill('#36B8E3'); ellipse(xCoordinate,200,50,50); } I'm not very happy with that, though it's better than what we started with. For one thing, I'd like to not just lose the ellipse to the off-canvas void. This happens because xCoordinate just keeps counting up and eventually gets greater than the 720 pixel width of the canvas. How do you suppose I did that with the color? Let's try making our ellipse bounce. We'll do this by saying that if the ellipse's x-coordinate is greater than the width of the canvas then it should start going in the other direction. This means we have to subtract from xCoordinate instead of adding more to it. We'll use an ""if"" statement to say that if xCoordinate is greater than 720 pixels, we'll subtract three on every draw loop until xCoordinate is 0 and then we'll start adding three again. We'll do this by adding another variable, which will represent the ball's speed and which we can change from positive to negative and back again as needed. var xCoordinate = 200; var xSpeed = 3; function setup() { createCanvas(720,720); background('#59747D'); } function draw() { xCoordinate = xCoordinate + xSpeed; if (xCoordinate > 720){ xSpeed = xSpeed * -1; } else if (xCoordinate < 0){ xSpeed = xSpeed * -1; } fill('#36B8E3'); ellipse(xCoordinate,200,50,50); } That's sort of trippy, but not quite right. We really want just one ball and, as things are now, we're drawing more and more of them on every loop. So, to draw one ball, we need to erase the previous ball. This is easy: We just need to move the background() function from setup() to draw(), so that every time draw() loops, the background is redrawn over the old balls. Try it. That's still pretty damn basic. Let's upgrade again. Start by making variables for the y-coordinate and the y-coordinate's speed. We'll add the ySpeed to the y-coordinate every frame so it accelerates downward and then when it hits the ""ground,"" it bounces back but decelerates as it does so. var xCoordinate = 200; var xSpeed = 1; var yCoordinate = 10; var ySpeed = 3; console.log(ySpeed); function setup() { createCanvas(720,720); } function draw() { background('#59747D'); fill('#36B8E3'); ellipse(xCoordinate,yCoordinate,50,50); xCoordinate = xCoordinate + xSpeed; yCoordinate = yCoordinate + ySpeed; if (yCoordinate >= 720){ ySpeed = ySpeed * -.5; } else if (yCoordinate < 0){ ySpeed = ySpeed * -1; } if (xCoordinate > 720){ xSpeed = xSpeed * -1; } else if (xCoordinate < 0){ xSpeed = xSpeed * -1; } } Now, that's actually something. Gravity! This is the part of the tutorial where things could either go on for 5,000 words or I could cut you loose with a good taste of things and point you, again, to the P5 docs and examples for guidance. I'll also (highly) recommend Daniel Shiffman's YouTube series on P5 . You've made a ball do some stuff, but there's a whole universe of particle systems , random walkers , and well beyond left to explore.",,Vice,Making cool visuals in JavaScript is easier than you think.,Hack This: A Short Guide to Drawing All Over the Internet,https://www.vice.com/en_us/article/qkj3bq/hack-this-a-short-guide-to-drawing-all-over-the-internet
Michael Byrne,"I'll be honest—part of why I love C programming is because it's sort of the antithesis of how every way cooler programming language works. C code is an often rigid world of semicolons and curly braces and memory management . You can really fuck C programming up: It's the last proper programming language layer encountered before running into machine instructions and the actual machine itself. Most anything you can think of, from operating systems (pretty much all of them) to Arduino code to the Python language, reduce to C. It is from-scratch programming. After two years of ranking second below Java, C retook the top spot in the IEEE's annual programming language rankings, a popularity assessment based on various programming language's appearances in each of a dozen sources, including Google search, Github, Stackoverflow, Reddit, and others. Programming language popularity rankings are a dime a dozen, but the IEEE's is probably the best one out there—or at least the one casting the widest net. This year, IEEE Spectrum introduced a tool that allows users to reweight the influences of the survey's different sources, so you tweak its biases however you see fit. By cranking up the influence of Stackoverflow to 100, for example, I was able to kick C back to second place. That said, I don't see anything glaringly wrong with the default IEEE weightings. The popularity of C shouldn't be surprising in the least. JavaScript and Swift may capture a disproportionate share of buzz, but C is in everything. It is especially present in embedded systems: the many, many tiny computing systems that power everything from the subsystems of our cars to microwaves to the ever-expanding Internet of Things. This is mostly all C (and the next lowest-level step beyond C, which is assembly language). That said, even if you mute embedded applications in the ranking, C still comes out on top. It's almost unfair to include it at all because it's the substrate of so many other things. It's like concrete winning a contest of building materials. It would probably have an even higher score if Arduino wasn't counted as a programming language. In my opinion, this is a pretty unfair inclusion given that Arduino is not an actual language so much as it is a C/C++ platform—that is, a set of tools and libraries that simplify writing embedded C code (which can be a real fucking chore once you start flipping/masking individual bits). Every time I write something about C I get asked by someone whether or not they should be learning it. The answer is that, barring some interest in programming robots or operating systems, ""probably not."" But I think every coder, whether they're hacking together single-page web apps with JavaScript or Swifting up an iOS game, should at least understand what C is and why it's important.",,Vice,It's like concrete winning a contest of building materials.,Good Ol' C Tops the IEEE's 2016 Programming Language Ranking,https://www.vice.com/en_us/article/ezpaee/c-tops-2016-ieee-programming-language-ranking
Michael Byrne,"This question was put to me by Motherboard Editor-in-Chief Derek Mead and I can't stop thinking about it: What is the smallest code? It's an interesting question in large part because it can have several meanings all pointing back toward what code is in the very first place. Is small code the actual code that we as developers and engineers write onto a screen? Or should we measure code by what code is translated into and how it actually executes on a real machine. Better, I think, is a combination of sorts: The smallest code is the smallest amount of programming language syntax that we can write to produce the largest machine-level effect. So, let's look at all three of the abovementioned perspectives, starting with the easiest. Smallest syntax In terms of characters, what's the shortest piece of valid programming language syntax I can write? To be clear, I don't know every programming language, but I have a reasonable handle on most all of the major ones and then some. So-called interpreted languages are what make this an easy question. These are languages whose syntax (what a programmer actually writes) is fed to some intermediate piece of software that functions as a translator (or interpreter) between our higher-level code and pre-built units of machine instructions. It's like executing programs within programs. The alternative to an interpreted language is a compiled language , which is where we write out a bunch of code in a file (like a Java or C++ file) and then send that file to be converted into a whole new arrangement of machine instructions representing that input file and only that input file. The difference is a bit like building a castle out of Legos (interpreted) vs. building a castle out of a big single piece of molded plastic (compiled). Both approaches have their advantages and disadvantages. Generally, if you're going to write a big-ass piece of software that's going to be installed onto a computer, you'll write in a compiled language. Python and JavaScript are both interpreted languages. We are free to write big-ass, old-school-style programs in either one, but we can also just feed tiny bits of syntax directly into either language's interpreter, which exists as a command line that looks like your operating system's command line (which is also an interpreter, but for a different set of commands). That is, Python is a language but it's also a piece of software that's installed onto our system like any other piece of software. A single numerical digit. This is probably the smallest piece of valid syntax I can write in any programming language. I can enter a single digit into either Python or the Node.js interpreter (which is a shell that interprets JavaScript) and either interpreter will simply echo it back to me without errors or warnings. I can also enter nothing into either one and not get an error, but I'm not sure that's in the spirit of the question. In a compiled language, a lot more is needed, relatively speaking. We at least need the shell of a function providing the operating system with an entry point into the program, so we're talking a half-dozen characters, at least. The basic C++ program skeleton looks something like this: int main() { return 0; } It's not much, but still more than: 0 Smallest footprint I don't think the smallest syntax measure above is a very honest way of looking at things. To execute that ""0"" will actually take a whole lot of system resources, relatively speaking. According to my MacBook's activity monitor, the Node shell I used to interpret that single digit is occupying around 11 MB of system memory. A single character, however, can be represented by a single byte of memory. So, we're holding on to 11 million bytes to echo a byte of data. #include int main() { cout << 0; return 0; } The C++ code above modified to output the single digit ""0"" occupies about 28,000 bytes of memory at its peak (according to the code profiling tool Valgrind). That's a much smaller footprint. Still, 28,000 bytes is 28,000 bytes. I might be able to improve things by ditching ""iostream,"" which is a standard C++ library for dealing with input/output operations. Including it means that I'm including some extra code from another file, and then more code from other files that the iostream code depends upon. The iostream library itself isn't enormous, but it has to bring in a bunch of other stuff to work. This all gets planted into system memory when the code is actually executed. In the above program, iostream just gives us cout (""cee-out,"" but I'll forever say it ""kout"" in my head). This is just a piece of syntax useful for outputting data to the screen. We can do the same thing is a slightly different way, as in: #include int main() { printf(0); return 0; } We swapped libraries for a standard library used in C programming. C is generally more raw and minimal than C++. The memory usage is about the same, but we wind up making the program itself smaller. The iostream (C++) version is about 9 KB, while the leaner library stdio, which is built for the C language, lets us trim about 1 KB from the program size. We could also measure footprint by assembly language representation. Assembly language is the final stop in any program's trip from programmer to programming language to actual machine instructions. We can say that it's the final ""human-readable"" step in the process. Code is compiled to assembly language by a compiler and is then assembled from that by an assembler. The job of the assembler is basically to make a one-to-one conversion from the human-readable assembly code to binary machine instructions. In comparing the two snippets of code above, the difference strictly in terms of file size—assembly file to assembly file—is about 50 bytes, though a quick inspection reveals that the actual assembly code produced by the compiler is pretty similar. Assembly language is the great equalizer. It could care less about your favorite programming language or patterns or paradigms and is only interested in hardware efficiency given a particular system architecture. I could also just write some assembly language code from scratch to do the same number outputting thing above, though what I'd write winds up being about the same thing as what the compiler delivers, give or take a few lines. A problem exists with the example, however. Outputting a single digit to the command line window is not all that simple of an operation when we start talking about code at these scales. To accomplish it, assembly language actually has to refer back to the operating system—there's no built-in machine instruction for ""print."" Leaving print behind, we can ask what the smallest code is that does anything . The best I came up with this is: xor eax, 0x01 This is assembly language instructing the processor to flip a single bit in the register named eax. In practice, it's not as lightweight or direct as it looks, but in theory we're telling the machine more or less at the machine level to change a single bit within not even a memory address, but a processor register. In theory, it's not doing anything to system memory at all then, just the small trays of memory that the processor reserves for itself to do computations. Small code, huge footprint For no good reason, I was stumped on thinking of a good example of small code (syntax) having huge effects (machine-space). This very thing is the bane of every software engineer, usually taking the form of a memory leak. As they execute, programs (C and C++ programs, in particular) should be deallocating all of the memory they allocate as they actually run. This happens: A program launches and claims some amount of memory, and, then, as it's running it needs to grab up some more for whatever reason. It's up to the programmer to make sure that memory is deallocated when the program quits. (Many languages have what's known as ""garbage collection,"" in which some automated process cleans up after programs, usually at the expense of a larger upfront memory grab.) Memory leaks are dangerous because of how they accumulate. While every time a leaky function is called,it may only leak a few bytes, that function might be called a million times. Suddenly, misallocated memory is accumulating exponentially and this becomes a problem. From a memory or processor standpoint, all it takes to nuke a system is a loop. The most basic form of a loop looks like this, which loops forever until the thing referenced by the ""while"" construct becomes not true. while(1){} In our case, this thing (the number 1) will never become not true, so it loops forever. If we were to, say, add some code to this loop that allocates a word of memory (64 or 32 bits, usually) on every iteration, that's gonna be a problem. The other night I nuked my system, a midlevel MacBook Pro circa 2015, in a pretty spectacular fashion. I was solving a HackerRank challenge that groups and maps numbers in a particular way. There's a naive way of doing this where you basically just map every smaller number up until the number we're supposed to be mapping. The idea is that you're counting up one by one, but before you can get to the next highest number, you have to take the current number and then decrement it one by one until you get to zero. That is, to get from 9,999,999,999 to 10,000,000,000, I actually have to count down by 9,999,999,999 before I'm allowed to increment by one. It's sort of hard to explain, but by the time we're up to the tens of billions, we're flexing an insane amount of clock cycles just to do these simple subtractions and additions. So, I wound up with a frozen, unusually warm computer that had been sunk in a matter of seconds by a half dozen lines of code. So, in this case, I'm talking about CPU percentage rather than memory, but you get the idea of how a bit of code can sink a CPU like nothing. Finally, I'm not sure I can really pick a best answer. The small-code-big-impact idea is nice, but where it's likely to be at it's most extreme is in the presence of bad coding. Trying to conserve hardware resources, whether memory or processor cycles, is also a nice idea, but computers now don't usually require programmers to be all that stingy with either. As for small syntax, as in the first example, that's not ideal either. Super-terse code often comes at the cost of readability and maintainability. There are a lot of ways to make code small, but, as with most things, what we probably want is moderation.",,Vice,Is it a single numerical digit? A line of assembly language? Let's find out.,The Smallest Code,https://www.vice.com/en_us/article/4xa73j/the-smallest-possible-unit-of-code
Michael Byrne,"Not that I'm currently cruising for jobs with British intelligence or anything, but I happened upon (via Hacker News) this current coding challenge posted to the MI5 careers page. It consists of the following image file and an invitation to find the clue hidden within. Intelligence isn't always obvious and our engineers and analysts work hard to unlock it.  There's a clue in the image file below, if you can find it.  The image itself, the colors and lines we perceive with our eyes, clearly pertains to chemtrails and their effect on the health of US presidential candidate Hillary Clinton, but that the challenge wording clearly specifies image file indicates that we should probably be looking beyond the picture itself and into the data within. Images are of course data just like anything else. And for that data to be useful it needs to carry instructions on how it's to be represented at its final destination. There needs to be information in there that is about the image but is not part of the image. This is image metadata—data about other data. Image metadata varies in format and content. This can depend on the image file format itself and in many cases the camera the image was captured with. The going standard, starting in the 1990s, has been the IPTC Information Interchange Model (IMM), though that's more recently been extended via the Extensible Metadata Platform (XMP) and Exif (""exchangeable image file format""), which is where we get into GPS tagging. Photomegadata.org likens image data to a bento box: Photos taken nowadays with a digital camera in JPG format are almost all guaranteed to contain Exif data, and this is what most metadata extractors are interested in. There are a billion such tools populating the internet for getting at this metadata and the Python language has its own pretty handy Exif extraction tool called ExifRead . For the sake of learning stuff, and because we'll eventually need to find metadata beyond the Exif standard, we're going to skip ExifRead and use the Python Imaging Library (PIL), which is a much, much more general toolset for doing stuff to images via Python code. In other words, even if you don't particularly care about image metadata, you're going to learn something useful. Prerequisites: Assuming you've already downloaded and installed Python , you should do two things. One: spend 10 minutes doing this ""Hello, World"" Python for non-programmers tutorial . Two: spend another five minutes doing this tutorial on using Python modules .  0.0) Install Pillow The active version of PIL is actually known as Pillow, so this is what we need to install. You should do this with the Python package manager pip , which is covered in the second prerequisite tutorial above. Just: pip install pillow Now, create a new Python script in whatever text editor you like. I'm using Sublime Text, which is great. I called my script metaread.py. 1.0) Create an Image object First thing we're going to do is actually bring in the Pillow module we installed, which is the first line below. Next, we need to create an object representation of our MI5 image, puzzle.png. This exposes the image and all of the things we can do with it via the Pillow module to our Python script. To see some more of these capabilities, check out Hack This: Edit an Image in Python . from PIL import Image image = Image.open(""water.png"") 2.0) Extract the Exif data Not all image formats contain Exif data. Mostly just JPGs. Which is fine because that's most pictures. The MI5's image is actually a .PNG file, which we'll have to handle somewhat differently. Let's do a quick JPG though. There's really nothing to it. I create the image object as above then call the _getexif() function on it. In return, I get a dictionary data structure full of metadata. The dictionary consists of tag-value pairs, which we can extract and view using a for-loop, like this. Note that I had to import some extra stuff at the top: from PIL import Image from PIL.ExifTags import TAGS, GPSTAGS image = Image.open(""gpsample.jpg"") print(image) info = image._getexif() for tag, value in info.items(): key = TAGS.get(tag, tag) print(key + "" "" + str(value)) So, that just outputs all of the Exif data contained within a given image as a series of entries. It's hardly guaranteed to be the same for every image. I had to search online for a sample image containing GPS metadata because I got tired of scanning through everything on my computer trying to find an example (though it wouldn't be too hard to write a script that could comb through a file of images and automatically pull out those that do include it). In any case, you can find the same image here . A sampling of the output: GPSInfo {0: '\x00\x00\x02\x02', 1: u'S', 2: ((33, 1), (51, 1), (2191, 100)), 3: u'E', 4: ((151, 1), (13, 1), (1173, 100)), 5: '\x00', 6: (0, 1)} ISOSpeedRatings 100 ResolutionUnit 2 WhiteBalance 0 GainControl 0 BrightnessValue (100, 10) 2.1) Extract non-Exif data Again, PNGs don't come with Exif data. Don't panic. Just because it's not in Exif format doesn't mean that puzzle.png's metadata is all that more difficult to access. It so happens that when an image is loaded per step 1.0, the PIL module will automatically load up a dictionary with whatever metadata it can id. We can barf it all out to the screen with a simple print statement: print (image.info) Or we can loop through it as in 2.0 as such: for tag, value in info.items(): key = TAGS.get(tag, tag) print(key + "" "" + str(value)) Problem solved? So, at this point I need to confess that this .info method is not actually returning all of the metadata from puzzle.png, and I don't quite know why. In addition to regular old Photoshop and the ExifRead Python tool mentioned above, I also tried four different online metadata extraction tools and only one was able to return a complete listing: Jeffrey Friedl's Image Metadata Viewer . Said viewer is based on a command-line tool called ExifTool, which I downloaded and ran. It too worked. But I promised Python and Python we shall write. It's actually pretty easy to run a command-line program from within Python, but you'll still have to download the actual command line program, which is available here . Now, we can run this script on our image file, and the ExifTool will output the result via Python to the screen. Try it. import os os.system('exiftool -h puzzle.png') See the clue? I don't know why it was so difficult to pull metadata from this file. It may have something to do with how metadata in PNG files is laid out. Within the file, metadata is kept in data structures called chunks. Chunks are given weird coded names that define, among other things, whether they should be considered ""critical"" or not. Critical chunks include actual image data, bit depth, and color palette. Not-critical chunks offer histograms, gamma values, default background colors, and, finally, text. There are three different types of text chunks all with a standard dictionary entry format. Each text entry has a name or title, and then some associated text. They can be user-defined, but there are some text field types that come predefined, such as ""comment."" Which in our MI5 file contains this: Comment: As I read, numbers I see. T'would be a shame not to count this art among the great texts of our time Now that we're at this point, writing metadata back to the file isn't much more involved. If you want to join MI5, you should probably be able to figure that part out on your own. Start by reading up on ExifTool .",,Vice,What secrets are your JPGs hiding?,Hack This: Extract Image Metadata Using Python,https://www.vice.com/en_us/article/aekn58/hack-this-extra-image-metadata-using-python
Michael Byrne,"No matter what kind of cluttered, icon-littered hellscape your filesystem happens to be, take heart that your computer provides a pristine alternate reality in the form of its command line interface. Essentially, this is a direct, minimal way of interacting with your computer, offering a shortcut around all of the clicking and dragging in the form of a small box that takes text-based commands and is mostly mouse-agnostic. Using the command line is second-nature for me, almost as much so as using the regular old OSX/Windows GUIs. In developing software/programming, we wind up constantly using a lot of tiny utility applications that just don't have graphical frontends, nor do they need them. To interact with these utilities, we use command-line interfaces, either those provided by an operating system—Terminal in OSX or just the command prompt in Windows—or something third-party, like Cygwin. Why should you the non-hacker care about this particular functionality? For one thing, it's a slight window into how operating systems actually work. Moreover, there are a few fairly routine tasks that can really only be done via command prompt, and, beyond that, a much larger number of small, helpful utilities that you might like to employ in your regular day-today computing, especially those relating to automation. Let's start small though and run through a few operating system tasks that we'd normally accomplish via the Windows or Mac GUIs—running programs and dealing with files. But first, a general orientation. 0.0) Bash For our purposes today, the native Windows and Mac command line environments aren't enormously different. But, so I don't say anything very wrong about the default Windows shell, and also because it's pretty useful, you the Windows user should install the Bash shell. Bash is an enormously popular command-line interface (or shell) that comes built into OSX and Linux, and is now offered as a built-in feature in newer Windows 10 versions. If you have a newer Windows 10 version, follow the instructions for activating Bash here . If not, either update or you can download and install Git , which comes with a Bash emulator called Git-Bash. 0.1) What a shell is It's tempting to think of a command-shell as being somehow closer to your computer's operating system or more integrated into it at the guts-level, but it's really just a program. The shell's operation consists of users entering commands from a predefined set and then turning around and telling the operating system how to do that. Launching a program by clicking an icon will do something similar. Commands might be issued individually, as in the case of, say, ""ls"" (which will list of the files in the current directory), or they might be issued via shell scripts, which are just files containing multiple commands arranged such that they perform some more complicated function. The commands recognized by a shell make up a language, which is kind-of/sort-of a programming language, but in a more narrow sense. The heart of the shell program, like Bash, is a simple loop that executes again and again, waiting for you the user to type something. When it has that input, it takes it and tries to make sense of it. If it's able to make sense of it, the shell will then call a function like ""execvp(),"" which is a Linux system call directing the OS to claim some physical memory and execute a specific program. If it can't make sense of the command, the shell will just return an error. It's not even very hard to make a shell program, and you could design one to accommodate really an infinite number of subdomains. You could make a shell that only has built-in commands for playing audio files, for example. 1.0) Getting around So, we all should be running a Bash command-line now. On Mac or Linux, you'll just need to type ""bash"" and it will switch over, though things will look about the same. A lot of what follows will work with your default OS command prompt, but to ensure things work right and we're all talking about the same thing, Bash is preferred. When you run a shell, it's important to realize that it's running inside of a specific location within your filesystem. A directory. Your system will have a directory designated as its ""home"" directory and this is where the shell will start out. I start out under my own named profile within the ""Users"" directory, which itself lives within the computer's base-level hard-drive directory, which on a Mac just referred to as ""/."" A common utility is the aforementioned ""ls."" Tap this in, hit enter, and the shell will spit back a list of all of the files and directories within the current folder. If somehow you got lost along the way, you can check which folder you happen to be in by running the ""pwd"" command. For example: To change directories, we use the ""cd"" command followed by a valid destination directory. Valid destinations can be directories contained within the current directory or directories referenced by their absolute path , which is their location in the computer's filesystem relative to the root directory (""/,"" or ""C:/"" in Windows, usually). So, in the example above, to get from the current directory into ""justkiddingsubdirector"" I type ""cd justkiddingsubdirector"" and I'm there. To get back, however, I can't just type in ""cd justkidding"" because justkidding isn't contained within justkiddingsubdirector. Everything is relative . To get to justkidding, I can do ""cd /justkidding,"" which references the directory in relation to the root. I can also do ""cd ..,"" where the two dots are taken to mean the next directory up from the current directory. Here that happens to be justkidding. To then get from justkidding to the root directory, just do the same command again: ""cd ..."" By the way, if you're ever confused by a command in Bash, you can type in the ""man"" command followed by whatever command is problematic. This will bring up a help page (a manual, really) full of details about the mystery command and its usage. 2.0) Running a program Just type the name of the program into the shell and hit enter. That's it. It will launch as though you'd clicked its icon in the GUI. Generally, you'll need to be in the same directory as the program or script being launched, or you'll need to reference the absolute path to it. For an installed application (on Mac), a normal-ass program like Skype or Microsoft Word, the path will look something like ""/usr/local/bin."" More often, you'll be running scripts, those microprograms we talked about earlier, which will be wherever you downloaded or created them. The third option are programs that can be called from anywhere, like Python or Bash itself, because they're referenced via system variables . That's a bit beyond the scope of this, but something to be aware of. Speaking of Python—a Python installation comes with its own Python command prompt. Python is another command interpreter, one that executes the set of commands built into the Python programming language. Again, a command line interpreter is just another program. For example: 3.0) Command-line hacks So, let's actually make this thing useful. 3.1) How connected are you? A utility called netstat will give some pretty granular details about your relationship to the internet at any given time, including every socket currently active or listening for connections. As you can see by invoking the command ""netstat -a"" your machine is basically network Grand Central. Note here the command-line flag ""-a."" Flags are a way of specifying a particular usage of a command beyond its defaults. Here, the flag is indicating that we want netstat to output all connections from all connection protocols (TCP and UDP, mostly). 3.2) Quickly create, read, and write to files Cat is probably one of the most commonly used commands built into Unix-style command shells (a la Bash). It exists to ""concatenate and print (display) the content of files,"" according to its Unix man page . The simplest usage is to run ""cat"" followed by some filename, like ""cat testfile."" In this case, it will just output the contents of the file to the Bash window. If that's not useful, you can also cat the contents of one file to another, new file, like so: ""cat testfile > testfile1"". The "">"" is used to redirect the output of the cat to the second file. Couple of things to notice in the example below. First, I'm using echo to blast some text into a file. All echo does is take whatever comes after it on the command line and print it to what's known as standard output . This is usually just the screen, but it's possible to redirect it to a file, as we're doing below. Also note the usage of "">>."" What this does is change the redirect such that it doesn't overwrite what's in the recipient file. It's just appended to the end, which is pretty useful. At this point, it's important to emphasize that the basic-ness of these commands is ultimately what makes them powerful. As in the example above, they can be combined as sort of atomic elements into bigger and more functional commands. As far as stringing them together goes, the sky's the limit. I like this sort of coding because it tends to feel really ""hack-y"" in the original sense of term, which is something like: making something work quickly via improvised cleverness . The keywords are ""improvised"" and ""clever."" To hack something is to make it work—somehow. 3.3) grep Speaking of simple-seeming things that are actually enormously powerful: grep . Grep 's basic utility is searching files for some specified content. You might invoke it on the command line like so: grep ""some chunk of data"" data.txt To search, say, all of the files in the current directory, you could swap in *.txt for data.txt, which in filesystem-speak means ""every possible combination of characters that might appear before .txt."" If the string ""some chunk of data"" is contained within the file data.txt, grep will output the line it's contained within to the screen. So what? Fair question. It might seem pretty trivial to your average ctrl-f assassin, but there's a whole lot more packed into this command. It's not just that you can wield grep 's search capability against many files at once, or that you can just as easily invoke the inverse search—returning every line that doesn't contain the search string—but grep is built around an almost mythical entity known as a regular expression. Regular expressions, or regex, are used for pattern matching. I don't have to give grep a string of text at all. I can instead give it a pattern, which is described in the language of regex. Maybe I'm scraping a webpage (downloaded and saved as a text file) and want to save only text following this specific HTML tag: "" ."" Not only that, but I don't want the returned text to include any other tags, so it needs to exclude all of those. I could do all of this processing by creating a regular expression, which is wild, I think. Unfortunately, Regex can be pretty inscrutable, resulting in maddening chaos like this : ""(?<=^(?=.*\\b(\\w+) \\w+; \\G).*)?(?:\\1 |(\\w+ ))(\\w+; )"" But the ability to describe a really finely tuned search pattern in such a small space is as powerful as it is completely insane. The syntactical insanity of regex is just the cost of doing business. In any case, regex is a bit beyond the scope of this, but, again, know that it exists . 3.4) Say ""this is not at all useful"" And it's not, but still sort of fun. Mac has a command called say that will read back in a somewhat uncanny robot voice whatever you tell it to. The subsection heading above is a valid Bash command (on a Mac). NULL) Settings Just Google search ""command line hacks"" or ""terminal hacks."" There are in existence a billion cheap blog posts about 10 weird tricks for hacking with the command line and nearly all of them contain the same basic batch of system setting modifications that you can do via the command line: show hidden files! change your screenshot save location! add some spaces to something! It's out there, but kind of lame. 4.0) Writing scripts A final note. As I said before, the command line's power comes from combining commands. A script, a file containing many commands executed in sequence, is very often how this power is wielded. So, let's do that: make a script. A very simple script. It's going to be a Bash script, which will be executed by the Bash shell. Unix-like shells come with a text editor called Vim or Vi, but it's completely bonkers and could be the subject of like your entire graduate degree. So, just open a regular old text editor, like TextEdit on Mac or Notepad on Windows. Name the file and give it an "".sh"" extension. Make the first line of the file the following, which will tell the environment that executes it that the script needs to be run using Bash. #! /bin/bash Now, I'm just going to add the same exact stuff we did in 3.4, except it's all going to be in one file. In all, it will look like this. #! /bin/bash echo ""hellow world"" > somefile cat somefile cat somefile > otherfile cat otherfile echo ""gd by wrld"" >> otherfile cat otherfile Got it? Save it somewhere and use the command line to navigate to that directory. To run it, enter in the following: ./yourscript.sh The ""./"" tells the shell to look in the current directory for the script. It won't run without it. You probably got an error when you did that, saying ""permission denied."" Yeah, you need to give yourself access to the thing you just created. You can do this by issuing this command and giving yourself read and write permission: chmod 755 yourscript.sh And now: Success! Anyhow, that's just a taste of shell scripting. You can consider this a prequel to a Hack This I wrote earlier this year on scripting in general and writing Python scripts . If this is at all interesting, make that your next destination. Read more Hack This.",,Vice,One of your operating system's most powerful tools is more accessible than you think.,Hack This: Become a Command Line Assassin,https://www.vice.com/en_us/article/gv5wjy/hack-this-a-command-line-quick-start
Michael Byrne,"Hack This has been started and restarted a few times in through Motherboard history. The first batch of columns was written in 2011, when I didn't really know shit about shit. Nowadays, I'm a computer science grad student, which is pretty weird. The Hack This idea is to write tutorials on doing technical things that can be digested by pretty much anyone, about a wide range of topics and at varying depths. It's hard to get the right balance, and, left to my own devices, I err on the side of detail and length. Which is fine sometimes, but it winds up excluding whole galaxies of bite-sized one-weird-trick how-tos, which are fun and useful. They're also sort of implied by the Hack This name, right? In any case, mixing it up is a goal going forward, and I would love to hear your suggestions for topics. What do you want to know? Get at me at michael dot byrne at vice dot com, or @everydayelk. The story so far: Hack This: Programming with the Twitter Firehose  Hack This: Become a Command Line Assassin Hack This: Extract Image Metadata Using Python Hack This: How to Consult Google's Machine Learning Oracle Hack This: A Short Guide to Drawing All Over the Internet Hack This: What To Do When Shit Just Won't Work Hack This: Scripting Deeper, Better Hacks in Python Hack This: How To Go From 0 to Sniffing Packets in 10 Minutes How can Hack This best serve you?",,Vice,What do you want to learn?,Hack This: Catching Up with Hack This,https://www.vice.com/en_us/article/pgkxav/hack-this-catching-up
 ,"This article originally appeared in VICE .  As a person who grew up playing text-based roleplaying games and trolling BBS sites, I’ve always been obsessed with language generated by computers. I remember spending hours and hours scouring the garbled junk text our printer would spit out when it malfunctioned; I felt certain that somewhere in those hordes of symbols and fragments and numbers smashed together, there was a secret underneath, some kind of impossible text hidden from the rest of the world.  Untangling the strange mystery of what machines might be trying to say is an ongoing investigation helmed by literary weirdos. From oulipean concepts that fuse random alterations to human-generated language, to flarf poetry , where a writer mines the internet for errors and random speech, there continues to be a widening array of ways to build something mind boggling out of the most basic elements.  Darby Larson’s new novel, Irritant , takes the utilization of computer-generated speech to the next level. Or circuit board. Whatever. The book consists of a single 624-page paragraph, built out of sentences that seem to morph and mangle themselves as they go forward. It seems at first immediately impenetrable, but then surprisingly and continuously opens up into places normal fictions would never have the balls to approach.  “In something of red lived an irritant,” it begins. “Safe from the blue from the irr. And this truck went in it. Safe. Something of red in it back to the blue to the red. This truck and something extra. Listen. The nearby something extras in front of the truck. The man in front of the truck trampled from front to back safe from the blue. And all this while the man scooped shovels of dirt and trampled from front to back front to back. The other and the clay sighed for something of red. The irritant lay in something of red and laughed.”  If this sounds insane, that’s because it is. But it is also rare, an impossible object given flesh. Its existence is as much of a relief as it is provocative, dense, an irritant in spirit.  Darby was kind enough to answer some questions about his programming and compiling methods for building his computer-related work.   VICE: How did you get into using computer-generated text as a method for writing fiction?  Darby: I do a lot of text manipulation using code as part of my day job, so I guess I've always been aware that certain basic functions exist and are simple to employ if I ever wanted to use them. I'm not interested in using complex code to manipulate text for its own sake, or trying to make a computer be the author of a work. I'm simply interested in tools that can help a writer achieve something that is meaningful for them. My mindset is not that much different from using functions like cut and paste during revision, or using an MS Word search/replace function to change the name of a character everywhere it occurs. We all do these sorts of things without thinking of it as text manipulation. Modern programming languages provide a few more of these sorts of basic functions, such as randomizers (input sentences in order, output sentences in a random order) or reveresers (input sentences in order, output sentences in reverse order). Granted, these sorts of functions aren't useful for most writers. They only became useful for me when I started becoming interested in more experimental literature and the oulipo movement. It provided a method of restriction where textual atmosphere could remain constant, where my creative control of a work could be pushed outside the realm of content, and into the realm of that content's structure.  Can you give me some examples of constraints or rules you wanted to use, and then the gist of how you went about writing the code to generate it?  Sure. Here is the process I went through to create the work “Pigs.” The first constraint was that I only allow myself one sentence structure: [the noun/nouns] [verbed] [with/through/in/on] [the noun/nouns]. So a sentence like ""The woman danced through hoops."" is valid. The next constraint was to only allow myself a finite set of nouns and verbs to plug into this structure. The sets I came up with were:  - First noun: [Unicorns, Children, The man, The woman, Birds, Umbrellas]  - Verb: [jumped, laughed, danced, slept, fell]  - Preposition + last noun: [through hoops, with unicorns, on tables, in bed]  So now I have a situation where there are only a finite set of possible permutations of this sentence. I wrote this procedure in PERL to extract all possible permutations:  @one = (""Unicorns,"" ""Children,"" ""The man,"" ""The woman,"" ""Birds,"" ""Umbrellas"") ; @two = (""jumped,"" ""laughed,"" ""danced,"" ""slept,"" ""fell""); @three = (""through hoops,"" ""with unicorns,"" ""on tables,"" ""in bed"");  for ($a = 0; $a<=$#one; $a++) { for ($b = 0; $b<=$#two; $b++) { for ($c = 0; $c<=$#three; $c++) {  print ""$one[$a] $two[$b] $three[$c]\n"" ; } } }  Looking at the output of this, it became obvious what I was doing because each sentence changes a word in order of the next permutation. I wanted to mix all these sentences up so it would have a flow that didn't sound like a computer just spit it out. I wrote this wrapper for the randomizer function in PERL to spit out all sentences in a random order:  my @templist = (); while(@list) { push(@templist, splice(@list, rand(@list), 1)) } @list = @templist ;  for ($df = 0; $df<=$#list; $df++) { print $list[$df] ; }  So now the work feels better, but still kind of static. I felt like the piece should do something other than just be a list of permutations. I thought it would be interesting to see it all move toward an even tighter constraint, like slowly change all the words in the original set to eventually be just one word. To do this, I just copy/pasted the entire list over and over, each time re-randomizing and search/replacing one word from the set to a variation of [Pig].  Now the work feels good to me, albeit quite long. It is basically moving from sentences like ""The man danced with unicorns."" to sentences like ""The pig pigged with pigs."" For me, what's happening is there is a tightening of constraint as the piece moves forward. Like it's closing in on itself. A pig virus that is slowly eating up all the original sentences. But I wanted to see the reverse, or an opening up. To begin with this pig virus but slowly shed it to eventually reveal the original set. I wrote this wrapper for the reverser function in PERL to spit the whole thing out in reversed order:  @reversed = reverse(@list); for ($df = 0; $df<=$#reversed; $df++) { print $reversed[$df] ; }  And that's how I wrote “Pigs.”  So you kind of set up a system that will spit out a mass of text, and then, almost more as an editor or puzzle-worker than a writer, look for ways to bend that text further? Are you thinking at all of narrative or story building, in any sense, or is it purely sound and image and juxtaposition?  In earlier works like “Pigs,” I was more concerned with developing technique than creating something with plot. The more I played around with these kinds of works though, the more I asked myself, like how can I bend this thing in such a way where characters can develop or a narrative plot can rise out of it. It's tricky because I'm setting constraints so tightly. The relationship between code and language is strange. You have to apply a very tight constraint in order for code to work on it. It becomes formal language. Chomsky has done tons of research on this relationship and “Pigs” is maybe a primitive actualization of his theories.  At some point I began to move away from formal sentence structure because it's just so rigid. The more complex a sentence is, the more complex a code needs to be in order to handle the problems that creep up with conjunctions and prepositions. I hung on to using word sets that slowly change over time as a primary constraint. So now I can inject narrative via the word choices in the sets I use, so I spend a long time just coming up with a word set that I think characters may develop within whatever system I decide to shove everything through later. Works like “Pulse” and “Sack of Oranges” were written without the aid of any code. With those I was relying on my own ability to write in a sort of constrained stream of conscious, continually referring back to a predetermined word set while writing. It loosened everything up a bit so that a plot could begin to surface.  How about Irritant? Did you set out knowing it would end up as a 600+ page novel? Were there constraints you began with that changed as you went on?  Yeah. I wanted to see these kinds of rules applied on a much larger scale. Because of the parameters I'm setting, I always know roughly how long the finished thing will be. My original idea for Irritant was to use a 70-word initial set that slowly changes to a completely different 70-word final set with a one-word change occurring every 4000 words. So 4000 x 70 is 280,000 words total.Irritant ended up being quite less than 280k, I can't remember why. I think I may have decided to quicken the pace a little. If you set parameters too high, at some point the word count becomes impractical. But theoretically, I could build something like Irritant with a word count in the millions.  Irritant's constraints were similar to “Pigs”'s constraints. I wrote the first 4000 words on my own, just stream of consciousness while referring to the word set. Then I randomized that and concatenated it to the original (so now 8000 words) and did one-word substitution on the new 4000, and so on and so on until all 70 words had been substituted. The big difference between Irritant and “Pigs” is that I wasn't starting with a list of permutated formal sentences so word substitution was a bit more painstaking in Irritant. I had to watch everywhere a new substitution took place and clean it up if need be.  What do you find most pleasing about this process of using coding to generate meaning? Is there meaning? Do computers know something we don't?  Using these processes gave me the opportunity to edit from a top-down perspective, as well as bottom-up. It allowed me to consider literature as almost a different medium, one where I'm so distant from the work that my control of it becomes holistic. I become less concerned with sentence level goings-on, which I can leave to code to figure out, and more concerned with how a mass of sentences are flowing and relating to another mass of sentences. So I don't know, maybe I'm just a control freak?  As much as it provides a measure of unpredictability, I don't think of meaning as being intrinsically wound up with the code or living inside the computer somehow. The result is still a work of literature that a human author intentioned and I think the burden of extracting meaning is still on the reader. If a reader discovers meaning in the relationship between two sentences that a code decided to put next to each other, then I think it's meaningful. And extraordinary.  Previously by Blake Butler - Windows that Lead to More Windows: An Interview with Gary Lutz  @blakebutler",,Vice,This Novel Is Made of Code,This story is over 5 years old,https://www.vice.com/en_us/article/788g5y/this-novel-is-made-of-code
Michael Byrne,"​As someone with an interest in technology and the things that people write about technology, you are surely well enough aware of the torrents of absolute bullshit that pervade our merry culture. We are flanked daily and on all sides by gurus and makers and shifting paradigms and blanksourcing and etc. There is an epidemic of tech-speak in which people are absolutely terrified of saying what a thing actually is in the real-world. This will someday soon be recalled as one of the more shallow symptoms of the imminent bursting bubble, but in the meantime it's just a bunch of extra noise. Meet ​ Sans Bullshit Sans , a font designed to nuke bullshit buzzterms from prose, replacing them with comic sans stickers. And it's an actual font, not an intervening script or CSS interpretation. You can ​download and use it as a webfont right away. Roel Nieskens, a front-end developer usually operating under the ​Pixel Ambacht name, used OpenType, which is Microsoft's extremely common font format. ""Font hacking"" seems like a reasonable way to describe the task, and it all began with the open-source font Droid Sans, which offers a license allowing typographers to make modifications to and reuse it for whatever, including bullshit. So, Nieskens dove in. Hacking a font isn't an easy thing, it seems, requiring the decoding/recoding of a whole series of dense tables containing the font's every little detail and behavior. Nieskens ​describes the whole process here and it's actually pretty interesting and technical. The gist is in the usage of OpenType's ligature types. In typography, a ligature is just a series of characters or character traits (""glyphs"") that smooshed together can be considered a single unit. A classic example is the ampersand ""&,"" which is actually a paired Latin ""e"" and ""t,"" but there's no reason it can't be extended to whole words or sets of words, like ""actionable"" or ""freemium"" or ""vertical cross-pollination."" With a trio of stickers in hand, Nieskens needed to make both the ligatures themselves ( ​250 of them ) and a lookup table where they could be found with the appropriate sticker replacement. The result is ready-to-go, and should work with most browsers, though, as noted, browser support of OpenType ligatures is still spotty.",,Vice,A highly actionable project.,'Sans Bullshit Sans' Is the Font That Nukes Buzzwords from Prose,https://www.vice.com/en_us/article/vvbq94/sans-bullshit-sans-is-the-font-that-nukes-buzzwords-from-writing
by,"Programmer Daniel Shiffman guides us through the complex world of coding, simplified. Have you ever seen one of those "" Programming for Dummies "" books at Barnes & Noble (or, ahem, Amazon.com) and thought-- wow, that looks pretty thick for a book for dummies? For those of you who'd like to learn how to code but only have say, a lunch break to study up, you can now check out this new one-hour online workshop and tutorial on hello.processing.org , produced as part of Code.org 's Hour of Code 2013 project. Created to introduce people to the joys of computer programming, this initiative launches just in time for Computer Science Education Week (December 9-15th)--and features a quick series of charming videos, offering the chance to learn the basics of code in less time than a trip to Home Depot. Using Processing, a platform that combines visual art with traditional elements of programming, the software is free to use, open source, and can be downloaded here . Once you land on the site you can maneuver between the six chapters: Hello , a brief intro to the world of programming in the context of the visual arts, followed by Shapes, Color, and Interact, which show you how to create desired visual effects using code. The ""hour of code' then ends with a Q&A and a farewell segment. To inspire you, check out some of the fascinating pieces created through programming:     Strata #3 by Quayola  ""The Strata project consist in a series of films, prints and installations investigating improbable relationships between contemporary digital aesthetics and icons of classical art and architecture,"" says designer Quayola. ""Like in geological processes, layers belonging to different ages interact with one another producing new intriguing formations."" Strata #3 - Excerpt from Quayola .   The Textile Room by P-A-T-T-E-R-N-S , is an experimental media space composed of fabric swatches augmented by projected videos, further powered by Processing.   Textile Room 03 FULL from Metropolis Magazine .    Cell Cycle by Nervous System allows users to create their own jewelry at home. Written and performed by Daniel Shiffman , with Scott Garner as web developer, the project also involved Jesse Chorng and Graham Mooney (shooting and editing), Scott Murray contributing to web development, and Casey Reas (whose past work we've covered extensively ) creating the sample code and co-producing. While we can't embed the tutorials here, check out hello.processing.org for more info.",,Vice,Design,This story is over 5 years old,https://www.vice.com/en_us/article/78epqx/learn-how-to-code-in-an-hour
by,"Images via Libs Elliott Libs Elliott calls her creations “modern heirlooms.” The Toronto-based designer combines the ancient craft of quilting with Processing code , resulting in unexpected displays of wild geometry, not dissimilar to glitch-knitting , and other tech-meets-textile processes . Processing allows her to generate random, non-traditional designs in an efficient, affordable way. When a pattern strikes Elliott’s fancy, she sketches it onto fabric and then sews it by hand. She appreciates the transition from digital to analogue, the difference between the instant gratification of generative art and the tedious task of turning that pattern into a beautiful blanket. If she collaborates with Nukeme for some glitch quilting, we hope we’ll get a few shattered squares and exploding triangles squares. Ask for it when you commission your own piece of cuddly code; get in touch with Elliott here .",,Vice,Design,This story is over 5 years old,https://www.vice.com/en_us/article/9anway/wrap-yourself-in-a-code-scarf
 ,"Images via , via  Drap og Design , the Norwegian design group founded by four recent Oslo School of Architecture and Design grads, wants to give you wearable super-powers. For their first project, the group transformed one of their team into a tech-enabled chameleon and let him loose onto the streets of Oslo. This attempt to adapt animal phenomena for human use was greeted by both general delight and confusion. In their Ineracket  release video above, a Drap og Design designer collects sensory input from colorful door fronts, potted plants, and the unexpectant shoulders of passersby, and steals their colors with each careful touch. As is the case with most superheros, there's a method behind the magic of Ineracket . Although the team reminds the readers of their Hack-a-day profile that they are not programmers, but designers, their project relies upon and integrates both disciplines from the start. The programming aspect stems from their use of four Adafruit products: the Florabella code, modified and adapted for Interacket’s needs, and both the Color Sensor and Neopixel libraries. Together, these tools allow the jacket’s sensors to transmit a continuous reading to its LED strips, granting the wearer the camouflage capability of the chameleon. For the jacket itself, Drap og Design took a handful of everyday objects, including painters' coveralls and a heat blanket, and molded them into a makeshift super-suit. With their imagination and training, the team was able to shape these items into the perfect vessel for their LED sensors— while still remaining in vogue . As the final Ineracket in the video and on the project's profile is only the first installment in their enterprise, Drap og Design eventually hopes to capture all of their favorite “animal super-powers” and bring them to us via fashion and design. Below, check out photos of the Ineracket in action: Keep up to date on Drap og Design’s products through their website , as well as via Facebook and Twitter .  h/t Laughing Squid Related: This Shocking Metal Dress Can Withstand 500,000 Volts And Now You Can Play Tetris On Your Shirt Make It Wearable | The Concepts: Pauline van Dongen's Solar-Powered Fashion Designs [Video] Little Boots' Cyber Cinderella LED Dress",,Vice,DIY Chameleon Jacket Grants You LED-Enabled Super Powers,This story is over 5 years old,https://www.vice.com/en_us/article/mgpg48/diy-chameleon-jacket-grants-you-led-enabled-super-powers
Michael Byrne,"Is Python—or the syntactical aesthetic of Python—worthy of the big screen? Redditor Infintie_3ntropy caught the following Python script making a cameo in Ex Machina , recalling , ""When I first saw it I was particularly annoyed since it had nothing to do with what the character was doing. But when I got a chance to see it in detail ..."" He saw this: #BlueBook code decryption import sys def sieve(n): x = [1] * n x[1] = 0 for i in range(2,n/2): j = 2 * i while j < n: x[j]=0 j = j+i return x def prime(n,x): i = 1 j = 1 while j <= n: if x[i] == 1: j = j + 1 i = i + 1 return i - 1 x=sieve(10000) code = [1206,301,384,5] key =[1,1,2,2,] sys.stdout.write("""".join(chr(i) for i in [73,83,66,78,32,61,32])) for i in range (0,4): sys.stdout.write(str(prime(code[i],x)-key[i])) print Courtesy of another Reddit poster, the actual screen-grab: Infintie_3ntropy ran the script and found that it outputs ""ISBN = 9780199226559,"" which refers to Murray Shanahan's volume, Embodiment and the inner life: Cognition and Consciousness in the Space of Possible Minds . Relevant! I think. I haven't seen the movie. Or at least more relevant than most on-screen code. This got me wondering: What makes for a good on-screen programming language? (I know all programming languages are on-screen, but you know what I mean.) It should look technical, complex, and generally sort of inscrutable. Right? If the human talent is doing their best to ham it up blockbuster-style, it makes sense that whatever code gets cast should be able to follow suit. The whole point of Python (or one of its central tenets ) is that it should be clean and minimal. It dispatches with much of the syntactical ""gunk"" found in other languages—curly braces, semi-colons—while promising programmers the same capabilities of other languages, but with a lot fewer lines of code. Sparse, elegant, intuitive (to new programmers, at least). So, while I think Python can play the part, I'm less sure that it's the best choice for wowing everyday moviegoers forever imprinted with that one scene from Swordfish —you know the one. Hacking isn't supposed to be easy. We can use the Tumblr Source Code in TV and Films —""Images of the computer code appearing in TV and films and what they really are""—as an IRL reference. It's maintained by John Graham-Cummings, author of The Geek Atlas: 128 Places Where Science and Technology Come Alive . Let's start with the very worst, which is HTML. First, while it plays the on-screen role of super-important but undefined programming language probably more than any other language, HTML isn't really a programming language at all; instead, it's a markup language, code that defines the appearance of a web document and doesn't really do computation. It's not Turing complete, which is sort of the benchmark standard of a programming language, ensuring its ability compute ""any"" algorithm written in pseudocode (programming language-agnostic programming, basically). (The Turing completeness of HTML is up for debate, but, in principle, no it's not.) But, whatever. Movie-goers aren't very worried about Turing completeness (most of them). HTML just looks like shit. The screen-grab below (from CSI: Cyber ) isn't what all HTML looks like—by any stretch—but taken on average, it tends to start looking like angle-bracket soup. And once you start incorporating PHP and JavaScript ... ugh. It's like soup with clumps of dog hair in it. What about JavaScript by itself? Well, it is kind of a mess, at least as it's often used by developers, e.g. as a crimscene of bloated, not-really-neccessary frameworks. It does at least look the part of a proper programming language and if you throw in jQuery, you get all sorts of bonus symbols and can wind up with statements like this: $("".cta"").click(function(){}). Which kind of looks like what someone might come up with if they were asked to make a fake language for the sole purpose of putting in hacker movies, e.g. a prop programming language. The screen-grab above is from Godzilla , and Graham-Cummings offers this origin: ""This appears to be sample code for Google Maps with google changed to MONARCH."" I'm annoyed that I can't find a better screen-grab from Halt and Catch Fire 's foray into assembly language, but here's something. The code appears to have been lifted from this complete program —sleuthing courtesy of pcjs.org —and is indeed written in assembly language, which is basically the last (sort of) human-readable stage of code execution, where higher-level programming languages like Java or C++ are translated into actual machine instructions for an Intel x86 architecture (most computers). The program here has nothing really to do with hacking a BIOS chip (what they were after in Halt ) and is instead a trace utility for the DOS operating system, circa 1985, several years ahead of the show's timeline. Whoops. But the question still is: Does it look the part? Sort of, I think. It's very different from any other programming language and adds a new layer of inscrutability. Yet, it's also very pristine and gunk-free; assembly code kind of just looks like a list. It's the precise opposite of JavaScript, really. So: not an everyday prop, but it has a place. Lisp shows up a few times on Source Code, and it seems to do the job well enough (above). Part of the problem here is that a lot depends on how code is presented rather than the code itself. If I were to open the Lisp code above in Sublime Text 3, it would be pretty as hell with all sorts of neat syntax-appropriate color-coding. I'm writing this blog post in Markdown right now—which is sort of a lightweight HTML-like markup language—in ST3 and it looks pretty slick and programmer-y on its own. Well? There are, of course, many, many more programming languages than I've touched on here, and maybe there's a better answer out there. There probably is. For the sake of a conclusion, I'll just say the answer is ... C. C is computing's workhorse meta-language and it's just inscrutable enough. There'd be no hacking—or Python—without it, at least. Mostly, it's an open question. I'd be interested to hear other opinions.",,Vice,Nominees for the Academy Awards of on-screen code.,What's the Most 'Cinematic' Programming Language?,https://www.vice.com/en_us/article/z4m7x4/theres-a-python-script-easter-egg-in-ex-machina
Michael Byrne,"Based on code analyses and scans of 50,000 different applications written within the past 18 months, cloud security firm Veracode has compiled a list of the most and least secure programming languages. Software engineers won't find it especially surprising, with PHP, venue for many a popular and ready-made hack, blowing away the competition. The report looked as a subset of the most pervasive programming languages/language families used today, including PHP, Java, Microsoft Classic ASP, .NET, iOS, Android, C and C++, JavaScript, ColdFusion, Ruby, and COBOL. Some 86 percent of analyzed programs written in PHP came with at least one cross-site scripting (XSS) vulnerability; 56 revealed at least one SQL injection bug; and 73 percent had encryption issues. Of applications written in the ColdFusion language, which serves a web scripting role similar to PHP and is already fairly notorious in its vulnerabilities, 62 percent revealed an SQL injection bug. Scripting/web development languages were generally worse off than their more traditional counterparts, such as Java and C++. 21 percent of Java apps were found to have SQL injection vulnerabilities, while 29 percent of applications written within Microsoft's .NET framework, which serves to unify several different foundational languages in one execution environment (like Java), had the SQL vulnerability. Of course, different languages are used for different things and in many respects comparing PHP to Java or C++ is apples and oranges. The prior is used to glue the internet together, essentially, while the latter are used more so to develop compiled/executable software. PHP runs within a web browser, while Java (etc.) runs the web browser itself. But that's only part of it. In terms of basic design, some languages are just better security-wise. ""It is noteworthy that web vulnerabilities like SQL injection and Cross-Site Scripting are substantially more prevalent in applications written in web scripting languages such as Classic ASP, ColdFusion and PHP, compared to .NET and Java applications,"" the report explains. ""This is very likely due to differences in the feature sets of each language. There are fewer security APIs built into Classic ASP, PHP and ColdFusion than have been provided for .NET and Java."" Java, in particular, has what's known as automated garbage collection. This just means that the language itself (or its execution environment, the Java Virtual Machine) will prevent a program from doing untoward things with a system's memory. ""By removing the need (and ability) for developers to directly allocate memory, languages such as Java and the .NET language family avoid (almost) entirely vulnerabilities dealing with memory allocation, most notably buffer overflows,"" the Veracode report explains. Part of the problem also has to do with who is using these various languages and what they're level of experience is. Don't believe the hype: A web development crash course is not going to teach the same stuff as years of computer science education (really). "".NET and Java programs are typically used by computer science graduates who learned those languages in school,"" Chris Wysopal, Veracode's CTO, told Information Week . ""A lot of the scripting languages like ColdFusion and ASP came out of the Web dev world, where you're designing websites and starting to learn coding, [and] to make sites more interactive.""",,Vice,Java and the .NET family get high marks while web languages lag.,New Analysis: The Most Hackable Programming Language Is Hands-Down PHP,https://www.vice.com/en_us/article/ezp4ek/new-analysis-the-most-hackable-programming-language-is-php-by-a-mile
by,"Quaddel formations are one of those things that you know is built on a foundation of some kind of super-crazy math you’ll never understand. But like the science behind pyrite cubes , the aurora borealis , and murmurations --you don’t really have to “get it” to appreciate it because it’s just so darn beautiful. Coming from German duo Deskriptiv (whom we spoke to a few months back ), Quaddel takes the input of a simple shape and builds it out organically into really outrageous organic forms. The network of tiny fractal branches grows the way corals, and even lung tissues, develop--stretching out into a completely mesmerizing bloom.  A pair of real, plasticized human lungs showing the “bronchial tree.” Image via j natiuk The creators can allow the structures to grow freely: Freeform Quaddel shape Or they can set parameters for it to grow around. Here’s the algorithm growing this coral-like stuff within and around the surface of a virtual glass shape: Growth process with collision avoidance The resulting structures can then be 3D printed into some really mind-blowing sculptures. For example, here is the process of a rendering with the constraint of a vector field, from video to physical object: Selected still of growth process controlled by a vector field  3D printed object from growth process controlled by a vector field Quaddel may even have a future in fashion- the project page includes examples of shapes ‘trained’ to grow around a human form. We’re excited to see what these guys will think of next! Please let us know if you can think of any awesome applications for this software in the comments. All images and videos are courtesy the artists and can be found on the Quaddel project page, except where otherwise noted.",,Vice,Design,This story is over 5 years old,https://www.vice.com/en_us/article/vvyg7m/for-those-that-cant-remember-to-water-grow-and-3d-print-your-own-digital-plants
Michael Byrne,"Computer vision-assisted license plate reading seems to be a favorite spectre of a certain sort of privacy worrier, at least anecdotally speaking. Something to do with its potential for tracking and profiling an activity that seems to make Americans in particular feel their most, uh, liberated (driving cars). The technology has had a steady uptake among law enforcement agencies, who of course think it's wondertool for busting crimes, but now an open-source implementation is starting to make some waves in the developer world and beyond. This is OpenALPR, and, as Mike James notes at I-Programmer , it's not a new thing but is getting some new attention in the wake of a recent code release. LPR is here for the masses and it's incredibly easy to use. In fairness, it's incredibly easy to use because there's just not that much to it. The application is based largely on the already open-source OpenCV (open-source computer vision) and Tesseract OCR libraries. It's written in C++ and runs as a command-line application. While there's a commercial version that goes for $50, the guts of the thing are free and free to modify. It has bindings for the C#, Python, JavaScript, and Java programming languages, so it's more or less served on a silver platter to devs. What's more, the OpenALPR version can run as a daemon on a Linux system (which is like a background application) monitoring a video stream while spitting back license plate numbers as JSON files. The fundamental task of Openalpr is to take an image file and return text. That's all. It doesn't query government databases for you, and the applications suggested mostly seem to be kind of contrived-sounding small-time security schemes. The promo video is a bit off: OpenALPR works well and fast, at least judging by the demo . It's also legal for the most part. As EFF lawyer Jennifer Lynch tells Ars Technica , ""While a handful of states have passed laws explicitly restricting private citizens and companies from using ALPR technology, outside of those states, there is not much in the law that would prevent someone from using the technology unless its use rises to the level of stalking or harassment. License plates are exposed to public view, and ALPR companies like Vigilant consistently argue they have a First Amendment right to photograph plates and retain the data they collect."" The Vigilante Lynch mentions is Vigilante Solutions, the almost cartoonish dark-side to private LPR tech. Vigilante sells LRP hardware, but goes a step further by maintaining an actual license plate database of some 550 million entries, largely provided by a Texas-based auto repo firm called Digital Recognition Network and largely employed by law enforcement agencies. (Freelance repo men scanning traffic for targets is a pretty creepy idea though.) On the other hand, this stuff is also pretty neat, from a technology/algorithmic perspective. If you were interested in learning a thing about object-recognition in the real-world, the OpenALPR codebase seems like a decent place to start.",,Vice,"OpenALPR makes it all so, so easy. ",License Plate Tracking Has Gone Open-Source,https://www.vice.com/en_us/article/ezpd8z/license-plate-tracking-has-gone-open-source
 ,"Over 2,000 delicate LEDs fill the Olympus Photography Playground in artist collective Neon Golden ’s SWARM , the lights' soft, electric emissions buzzing through the 850-cubed-foot space in Vienna. Engineered with a combination of Arduino, Cinema 4D, Raspberry Pi, and Processing, the audiovisual installation reacts to movement, placing its visitors inside a colorful, carefully coded 3D environment. Immersing himself in this electrically engineered mass of ruby fireflies, Máté Czakó glides through the luminescent, coded creatures, revealing the LEDs’ subtle reactivity with his own simple wonder. For those who cannot experienc the piece for themselves, the choreographer teamed up with Neon Golden to demonstrate their kinetically sensitive light sculpture. Below, take a behind-the-scenes look at the installation, and watch Czakó get lost in the SWARM :  SWARM / Olympus Photography Playground / Installation + Performance from NEON GOLDEN on Vimeo . For more of Neon Golden’s audiovisual artworks, check out their website here . Related:  Glowing Cables Transform a Cathedral into a Massive Player Piano  2,000 Lights Illuminate Hong Kong In Jim Campbell's New Installation  Kissing Brings a Light and Sound Installation to Life",,Vice,A 'SWARM' of LEDs Immerses a Dancer in Light,Entertainment,https://www.vice.com/en_us/article/ez5pdz/a-swarm-of-leds-immerses-a-dancer-in-light
Joseph Cox,"The Government Communications Headquarters (GCHQ), the UK's signals intelligence agency, is one of the most opaque institutions on the planet. So, it might come as a surprise that it has just launched its own public facing Github account, and released a new database tool for anyone to use. ""For the first time, GCHQ has contributed to the open source software development community by this week releasing, via Github, a graph database called Gaffer,"" a GCHQ spokesperson told Motherboard in an email. Github is a platform that lets just about anyone host their code and documentation for their own projects, and allow others to contribute to them too. Users can publish their code to the world, or keep it limited to only select eyes. GCHQ, it seems, has done the former. Its account can be accessed here , and at the time of writing is hosting one project. ""As a government department and technology organisation, GCHQ software developers and technologists aim to contribute to open source software projects,"" the spokesperson continued. ""Gaffer,"" according to the project's description, ""makes it easy to store large-scale graphs in which the nodes and edges have statistics such as counts, histograms and sketches."" In short, its a framework for creating large databases, to store and represent data. The description also notes that ""Gaffer2"" is in development, which will apparently make a series of improvements. ""Gaffer2 is a project that aims to take the best parts of Gaffer, and resolve some of the above flaws, to create a more general purpose graph database system. This can be used for both large and small scale graphs, for graphs with properties that are summaries, or just static properties, and for many other use cases,"" the description reads. GCHQ has been changing its PR tactics pretty drastically recently. In November, graffiti-style recruitment ads for the agency popped up all over East London, and in October, The Times was granted access to GCHQ's hub in Cheltenham. This moves may seem an attempt to give the impression that GCHQ is an open, approachable sort of intelligence agency, but no one can ignore that it is behind some of the most invasive mass surveillance programs the world has ever seen. As part of an operation called Tempora , GCHQ has tapped into the underwater cables that carry the world's internet traffic, and constantly sweeps up emails, Facebook posts, internet histories, calls and other data, and then shares that treasure trove with the NSA. Regardless, according to GCHQ, Gaffer won't be the only project the agency posts to Github. ""Gaffer is expected to be the first of many contributions that GCHQ will make to open source software,"" the spokesperson said. ""GCHQ hopes that Gaffer will be useful to others in the community, as well as helping its own technical staff as they continue to develop the software in the future.""",,Vice,"It might come as a surprise that the agency has just launched its own public facing Github account, and released a new database tool for anyone to use.","GCHQ, the UK's Secretive Spy Agency, Now Has an Open-Source Github Account",https://www.vice.com/en_us/article/bmvxdm/gchq-the-uks-secretive-spy-agency-now-has-an-open-source-github-account
Michael Byrne,"You won't hear too many web devs saying it out loud, but there is a world beyond JavaScript. The ruling language of web applications and interactive websites, JavaScript seems as much a cornerstone of the web as C is to conventional software. Indeed, a critical piece of browser development is a JavaScript engine, which is a sort of virtual machine that takes JavaScript syntax and interprets it into instructions that can be executed by a browser. It's not terribly pretty and, what's more, it limits client-side web applications (those executed within the browser) to being written in JavaScript, which isn't very much in the spirit of programming and, well, JavaScript is also kind of crappy. But JavaScript also can't help it for the most part. It's very much the product of evolution—evolution being a process of finding just good-enough solutions—and not super-long-term thought. So, as web applications take on more and more roles formerly handled by the sorts of software you'd normally download and install and run via an operating system like Windows or OSX, it makes a lot more sense to have a low-level development environment, where any number of languages can be compiled (translated) to a uniform machine-friendly syntax, which for an operating system like Windows or OSX is asssembly language or ASM (or MASM, for Microsoft assembly language). Browsers don't really have a bytecode assembly language in this sense, however, which is where WebAssembly (WASM) comes in. This has been getting some press in the past week thanks to an announcement by JavaScript creator Brendan Eich that development will begin on a definitive WASM, a meta-language that will in the immediate future be very intimately linked to JavaScript, but will eventually have its very own life. Crucially, WASM development will be immediately open to all comers. ""A W3C Community Group, the WebAssembly CG, open to all,"" Eich writes. ""As you can see from the github logs, WebAssembly has so far been a joint effort among Google, Microsoft, Mozilla, and a few other folks. I'm sorry the work was done via a private github account at first, but that was a temporary measure to help the several big companies reach consensus and buy into the long-term cooperative game that must be played to pull this off."" There are a number of transpilation tools that do allow for the usage of high-level general-purpose languages like C++ in building web sorts of things, but taking one high-level language and having it converted into JavaScript so it can be executed as plain-text instructions by the browser is a rather clunky way of doing things. Often, you might as well just do the damn thing in JavaScript at the outset. There does exist a programming sub-language known as asm.js, which is designed to implement languages like C in a subset of the JavaScript language that's tailored towards lower-level operations like memory management. This is a big advantage (depending on who you ask and how they use code) of C and C++ in the first place: They're memory-lean and, if used correctly, they can also be very fast. This gets to be pretty important once we start programming software for embedded systems—like an Arduino controller, for example, or an ATM machine or the computer inside an MRI machine—that offer only limited processing and memory resources. ""WASM should relieve JS from having to serve two masters."" So, the very basic idea of WASM is that I could take a language I actually like, like Python or C++, and write a web application in that language and the WASM compiler will convert it into a form of browser bytecode, which would exist in binary form rather than the plain-text of JavaScript. This would occur in a way analogous to writing a program in Python or C++ in the normal way and it being compiled into an assembly dialect corresponding to actual machine instructions. As Eich notes, in the beginning WASM would be developed in close concert with asm.js, but the two would naturally diverge in the future: ""A secondary consideration: JS has a few, awkward corners even in its asm.js subset. Finally, once browsers support the WebAssembly syntax natively, JS and wasm can diverge, without introducing unsafe or inappropriate features into JS just for use by compilers sourcing a few radically different programming languages."" Yes, ""awkward corners."" It's probably the most immediate why? behind WASM itself. Even with the relatively optimized (or optimizable) asm.js subset, we are still after all converting from high-level language to not just another high-level language, but a high-level language that is JavaScript. And there are a lot of problems with JavaScript, as revealed by the bazillion articles and blog posts that searching some combination of ""JavaScript"" and ""sucks"" will yield. ( This talk from Gary Bernhardt at PyCon 2014 is a good and reasonably neutral place to start though.) JS fans need not be alarmed, however, according to Eich, who continues, ""No, JS isn't going away in any foreseeable future. Yes, wasm should relieve JS from having to serve two masters. This is a win-win plan."" That said, there isn't a really great reason for it to stick around forever. JavaScript won client-side web programming because it was there, not because it was the very best thing for web programming. It will remain in existence for precisely as long as people keep using it.",,Vice,"A joint project between Mozilla, Google, and Microsoft could change how the web is programmed.",Is WebAssembly the (Eventual) Death of JavaScript?,https://www.vice.com/en_us/article/78x3gz/is-webassembly-the-eventual-death-of-javascript
Michael Byrne,"I have fairly worthless Google Alert set up for ""programming language."" What it yields on a daily basis generally falls into two related camps: ""tech beat business news reporter discusses programming languages as though they were brands of cereal"" and ""popular programming languages."" Both are premised on the notion of a ""favorite"" programming language, as though everyday programming language usage is dictated by what programmers are really feeling at any given point in time. It's not unreasonable to think this way, to an extent. If you see a programming language popularity ranking like Stack Overflow's annual developer survey and see that JavaScript is at the top, you might reasonably conclude that developers are into JavaScript more than any other programming language—that is, developers are selecting JavaScript from a pool of options based on preference. Like cereal. But programming languages aren't cereal. They're tools. What does it mean for a hammer to be the most popular tool at a construction site? That workers like hammering nails more than they like cutting wood or pouring concrete? Well, no. It means that there are more nails to pound. A lot of people like JavaScript for sure (even me!), but its popularity has much more to do with its current utility. Web browsers (and now servers, via Node.js) feature engines for interpreting JavaScript and so JavaScript is the default programming language for web applications. If web development weren't popular—especially so among Stack Overflow users—then we would see a different ranking. Same thing goes for Swift . The aforementioned Google Alert has been handily monopolized by the Swift programming language since I set it up a year or so ago. Swift hype is actually worth its own post, but, for now, we can say the same thing as we said for JavaScript. It's another tool, albeit a tool with the backing of a shrewd corporate sponsor. The job in this case is developing iOS applications, for which Swift is now the default/requisite programming language. No shit Swift is popular/becoming more popular. It's a new language, for one thing. But it's popularity is tightly bound to the popularity of developing apps for iOS. Which is pretty popular! To be sure, some languages do well as multi-tools and are more general-purpose than others. Most languages aren't content to exist solely within specific domains, but this is especially true of languages like Python and Java. With Python, it's possible and even reasonable to run single Python commands via the Python interpreter while also building vast applications with the language. Among other things, I've used it to write full-stack web applications, web scrapers, RESTful APIs, and cheap Twitter scripts . I've also used it for signal processing. In Python, it's all pretty natural. Finally, though it may not be the factor determining the relative popularity of programming languages, there is such a thing as a favorite language. Obviously. I have one or maybe three. I had to think about it for a while. I can't really separate the idea of having a favorite programming language from the idea of having a favorite programming language for some particular programming domain (scientific programming, web programming, systems programming, cloud programming, etc). So, I guess the way to reduce it is to find the most general programming domain (the most general for) , which should yield the most general favorite programming language. Maybe I'm overthinking it. IDK. It just feels like a very pure way of expressing algorithms—connecting thoughts to whiteboard to code. In any case, my favorite programming language is the one that I like the most for implementing general computer science algorithms—programming interview-type stuff like efficiently finding prime numbers or computing shortest paths in graphs or sorting arrays. Here, C++ kind of just flows out of me. It's the language I use for Hackerrank challenges/CodeFights, though I rarely even use it in school anymore or elsewhere in the ""real-world."" C++ not infrequently invades my dreams. Does that mean I like it though? I guess I do. It's fast and can be very lean when it needs to be. There's a naturalness to C++, both to writing it and to how it actually works on a machine. It just feels like a very pure way of expressing algorithms—connecting thoughts to whiteboard to code. That's a pretty oatmeal answer to the favorite-programming-language question, but whatever. I'm always learning new languages and getting excited about new languages. Right now, it's Scala. Scala is a language that both extends the Java language and is built to extend itself in programmer-defined ways. It's a language for building a language, which is pretty wild. I'm looking forward to really getting my head around the thing. A tool for making tools. That's exciting. To circle back around to a point I've touched on a few times on Motherboard, knowing a programming language isn't anything like knowing how to program a computer. The algorithms are surprisingly uninterested in the tools wielded by programmers. This is sort of what I'm saying about C++: It's the language that, to me, most feels like the the abstract ""whiteboard"" process of programming. I guess that's a bit like saying my favorite programming language is no language at all.",,Vice,"Alternately, what does it mean for a hammer to be the most popular tool?",What It Means To Be a 'Popular' Programming Language,https://www.vice.com/en_us/article/4xab7w/what-it-means-to-be-a-popular-programming-language
 ,"A software engineer. Photo by Joonspoon This article originally appeared on VICE Sweden . Last week, two researchers at the Royal Institute of Technology in Stockholm (KTH) presented a study that suggests gender-neutral course literature helps students perform better. This means you're more likely to apply for a course and get good grades if the books you read are less about boring stereotypes and more about situations you can personally identify with. The study focused on a programming course, because programming is typically associated with nerdy dudes. ""We noticed that more men tended to apply for the course than women,"" said Maria Svedin, when I called her last Thursday. ""We tried to identify the factors that could have led to that, and gender-neutral wording was one of them."" Svedin authored the study, together with Olle Bälter. Both work as researchers in Media Technology and Interaction Design at KTH. ""Firstly, we presented programming as a technical and creative craft that also has a social aspect,"" she explained. Then they changed some of the course material to reflect the above notion. For instance, the textbook featured one problem that focused on a mother, who wanted to program a weather app because she was worried about her son's ability to dress according to the weather. Svedin and Bälter replaced that problem with the story of a person named Kim, who wanted to snooze for as long as possible in the morning with the help of a weather app (Kim didn't have a window in her/his bedroom). ""We also had to make sure that the course literature reflected what programming is today, and make sure to eliminate stereotypes such as that programmers are nerds, have dry humor, or that they are lazy,"" Bälter added. The researchers found that this led to more applications, regardless of gender. ""For this specific course, we hope that more people—men and women—get access to the correct idea of what programming is and therefore find it more interesting,"" concluded Svedin. Follow Caisa Ederyd on Twitter.",,Vice,"Unsurprisingly, Gender Neutral Textbooks Make for Better Students ",Money,https://www.vice.com/en_us/article/3b4az3/gender-neutral-course-material-makes-you-a-higher-achiever-says-science-765
 ,"Videos depicting the treatment of people of color by the police have led to a growing national conversation about race and policing in America. VICE News sat down with Yvvy, a developer who created a game that lets you play the role of a young person of color interacting with law enforcement. The game, she hopes, “gives [players] a little bit of what we feel dealing with police.”",,Vice,by,Black Lives Matter Game,https://www.vice.com/en_us/article/43qy89/black-lives-matter-game
 ,"Thanks to the ever-expanding glory of the Internet, there is now a service that instructs users how to make any image into life-size Lego art, piece by piece. Found on San Francisco programmer Sailor Mercury 's GitHub , the Legoizer breaks down an image in horizontal lines of Legos, resulting in a sheet-like structure to be built from the ground up. We're not yet sure if it uses the same technology as the fabled LEGOizer software (also known as BrickBuilder) that Lego designers have been using to create their models since the '90s, but it's a cool tool to practice climbing that vaulted master builder throne. Mosaic-ed with Legoizer, Klimt's The Kiss , for example, takes 166 lines of Legos to create a life-size reproduction that would stand nearly five feet tall. The tool features a custom list of instructions for any image, accompanied by a shopping list of how many pieces you'll need of each color. In the case of The Kiss , the bulk of it involves 4,742 bright orange pieces of varying sizes—and that's just for the background. Check out the Legoizer here . Via ARTECreative Related:  The Turntable Made of Legos Is Here  Depeche Mode Gets Covered by the World's First Lego Band  Giant Lego Machine Generates Churning House Tracks  Play with Two Tons of Lego Bricks at Olafur Eliasson's New Interactive Installation",,Vice,Make Any Image into Lego Art with ‘The Legoizer’,Entertainment,https://www.vice.com/en_us/article/pgqzg7/make-any-image-into-lego-art-with-the-legoizer
Michael Byrne,"To the internet, a webpage is just a soup of text, symbols, and whitespace. Actual content, the stuff we're interested in as webpage consumers—such as this blog post—is a part of this soup just as much as HTML tags are. The distinction is made only when our soup is consumed by a piece of software designed to interpret and possibly render it as a webpage. Most likely, that software is a browser.  Maybe this is already obvious and intuitive, but it's worth emphasizing that there's not really any mystery or magic in the HTML document itself. If you were to open up the source file for this webpage, you would find these words—the aforementioned content—sharing an essentially flat landscape with a great big old mess of code.  While the processes that produce the final HTML soup become all the more elaborate and complex, the soup itself is always there and it will always conform to HTML specifications. It may look to be a mess, but it's a standardized, useful mess.  We're not always interested in the webpage end-result of a string of HTML. It may be the case that I'm less interested in reading this blog post than I am in analyzing it. I may want to parse it for the appearance of certain keywords, for example. For this one webpage, I could as a human user just use command-f, but for many webpages this can be accomplished much easier via automation. That is, I might write a script that might scan the HTML strings representing webpages programmatically, collecting keyword statistics as it goes. This would be neccessary, even. Considering the web as raw data requires a programmatic approach.  This is web scraping, generally. It's a means of collecting data from the internet via the strings of HTML that determine the content and appearance of webpages.  What might someone do with this kind of data? Some examples:  Find email addresses proximate to certain keywords for spamming purposes/lead generation.   Funnel content from a bunch of different websites into one. Imagine, for example, a single site that aggregates (illicitly, probably) raw content from a dozen other websites.   Harvest stats from government websites.   Scan listings from multiple job sites for search strings indicating gender bias.   Perform sentiment analysis on blog sites from a variety of platforms (Tumblr, WordPress, etc.).   Monitor price fluctuations among many different web retailers for a specific product.   There's really no end to it.  The prerequisites for this Hack This are the same as for every other one that's based on the Python programming language: Assuming you've already downloaded and installed Python , you should do two things. One: spend 10 minutes doing this ""Hello, World"" Python for non-programmers tutorial . Two: spend another five minutes doing this tutorial on using Python modules .  0.0) Scrape at your own risk  First off, there's a lot of sketchiness and perceived sketchiness around web scraping. The legality of scraping is, generally, unsettled and some major cases have arisen in recent years involving startups whose entire business plans revolve around harvesting the websites of other businesses. For example, a few years ago Craigslist went after a website called 3Taps, which had been scraping and republishing housing listings from the classifieds giant. Eventually, 3Taps and Craigslist settled , with the prior paying out $1,000,000 to the former (which is to donate the sum to the Electronic Frontier Foundation over a 10 year period).  For our purposes, as web scraping tourists, we're probably fine, but it's important to keep in mind that what we're doing can be considered unauthorized use.  0.1) Use the API, if it exists  More and more sites offer public APIs that allow us to plug directly into our desired data sources without scraping at all. You'll never need to scrape a website for weather data, for example, because they all offer their content not just as a collection of webpages but as web services that we can access in predefined ways. Likewise, you will never need to scrape Twitter or Flickr or, hell, the New York Times . All offer ready-to-go developer tools that have the handy feature of well-defined usage policies.  1.0) Beautiful Soup  Disclaimer: I'm learning Beautiful Soup with you. BS is a set of Python tools (a Python module, or package) for extracting data from HTML documents, but it's hardly the only set. For one thing, it's very possible to scrape a webpage without any specialized tools at all, and that's how I learned to do it—downloading HTML and then parsing it using the pattern matching capabilities of regular expressions. Beautiful Soup just eats all of these details and hides them from view, which allows us to focus not on the guts of HTML parsing but on the data itself.  Beautiful Soup will allow us to specify at a relatively high level what it is exactly that we're after in a given HTML document (or on a given webpage) and it will go after that content with some relatively efficient (compared to my wholesale downloading and pattern matching above) parsing methods.  As usual, we'll start with a new/blank Python file. Use whatever text editor you like. I'm using Sublime Text , which costs money. Atom is a comparable freeware editor. You'll need to install Beautiful Soup, of course. Using pip , it's just: pip install bs4 Bs4 is the package name of the current Beautiful Soup release.  2.0) Find a target  My idea is to scrape the website of Clark County, Washington for data on current jail inmates. The county publishes a roster that includes a small amount of information: inmate name, booking date, location, and charges. I think it might be interesting to look at charges vs. total period of incarceration between the given booking date and today's date. We might not get to that point here, but this will provide a context for our example. I also don't feel too bad about directing a bunch of traffic-noise to the website of a jail (which is in Vancouver, Washington, near Portland, Oregon).  (The seed for this idea comes via a scraping tutorial offered at bootcamps given by the Investigative Reporters and Editors at the University of Missouri circa 2013 and 2014.)  2.1) Know your target  Even if you're not an HTML ace, it's worth taking a look at the raw code behind your target website, just to get a feel for things. So, just do a right-click on the page and hit ""view page source."" Scanning through the resulting code will reveal that inmate records are all bunched together in a big knot about halfway through the page.  We can see that each record is formatted like so (I redacted the inmate name myself): 185422 [INMATE NAME]  8/24/2016IC2D1/2/2017RETAIL THEFT EXTENUATING CIRCUMSTANCES III The tag tr denotes a row in a table, while td denotes a table cell. The different units of inmate data are jammed into these cells. The booking date and the charge don't seem to have any special identifiers. We'll see if that matters in a minute.  3.0) Parse the HTML document  This is really pretty simple. In our script (so far), we're just importing the actual Beautiful Soup module and then we're using it to open and parse a webpage, which is accomplished as below. The resulting soup keyword is now our window into the parsed webpage and the various operations that we can perform on it. ""html5lib"" here tells Beautiful Soup to use the specific parser called html5lib . There are a few different parsers HTML BS can use, and they all handle HTML a little bit differently. This one is built to parse HTML5 code in the same way that modern browsers do and it also happens to be the one that worked best for the jail-scraping problem. (You will probably need to install it using pip, e.g. ""pip install html5lib."") from bs4 import BeautifulSoup import requests url = 'https://www.clark.wa.gov/sheriff/jail-roster' response = requests.get(url) html = response.content soup = BeautifulSoup(html,""html5lib"") print soup.prettify() There are a few things happening here. For one thing, note that we're using a module called requests in addition to Beautiful Soup. This is what queries the target webpage, which then responds by barfing its HTML back to us. This HTML barf is then stashed in a variable called response . The whole response contains some stuff in addition to the actual HTML that we don't really care about, so we access a property called content to get the actual payload. We then pass that barf on to Beautiful Soup, which parses it all and makes it available via our new soup keyword.  Finally, we're just printing the HTML to the screen via Beautiful Soup's own prettify( ) function.  4.0) Trim it down  The parsing is already done, so everything that happens from here on out and is going to be done not to the HTML itself, but to a data structure maintained by Beautiful Soup (which we access through our soup variable). Let's start by grabbing the entire table of inmates. If we look at the prettified HTML we just printed, we can see that this table starts like this:  Here's how we're actually going to get at that table: from bs4 import BeautifulSoup import requests url = 'https://www.clark.wa.gov/sheriff/jail-roster' response = requests.get(url) html = response.content soup = BeautifulSoup(html,""html5lib"") table = soup.find('table',attrs={'id':'jmsInfo'}) print table.prettify() So, Beautiful Soup is looking at its parsed HTML and returning the stuff inside of the tag that we specify using the find function. Here, we tell BS to find the tag of the table element that has the attribute id with a corresponding value jmsInfo . Within this tag and its corresponding closing tag is our jail roster table. 4.1) Keep trimming Instead of just printing out the extracted HTML, we can output it programmatically as we further refine our quarry. Because we're now dealing with a Python data structure and not raw naked HTML, we can iterate through our table really easily using Python's for-in syntax . Try the following, in which we tell Python to go through our table row by row and print out the contents of each cell contained within each of those rows. from bs4 import BeautifulSoup import requests url = 'https://www.clark.wa.gov/sheriff/jail-roster' response = requests.get(url) html = response.content soup = BeautifulSoup(html,""html5lib"") table = soup.find('table',attrs={'id':'jmsInfo'}) for row in table.findAll('tr'): for cell in row.findAll('td'): print cell.text As you can see, we're just dealing with data now. But it's still not quite the data we want. The inmate name and id information needs to be left out, as does the inmate location. We're just after the charge and the booking date. NULL) As an aside, I think this particular data could be legit interesting. We expect that those who have been held in county jail the longest will be charged with the more obvious serious offenses—murder, rape, etc. The more dire the charge, the more time is required for trial preparation and the higher the bond. What if we found that there's some anomalous lighter-weight charge that's keeping defendants in jail for longer-than-expected periods? That would be interesting. 4.2) One more cut Recall the HTML formatting of the rows in our inmate table. The individual cells—again, denoted by the td tag—don't have any other identifying attributes, like a specific class or id. So, we're going to access the cells we want by navigating among the ""children"" of each individual row (corresponding to each individual inmate). HTML documents are hierarchical. Tags contained within other tags are considered children of the containing tags. Tags that are adjacent to other tags, but are not within them, are considered to be siblings of those tags. For example: With Beautiful Soup, we can access the children of a tag by using the ""contents"" property. This will contain as a list all of the tags that exist at the next level below the specified level. So, if we wanted to get the third cell of a table row (corresponding to the booking date), we would do so like this. Here, we print only the booking dates of our inmate roster table followed by the associated charges. from bs4 import BeautifulSoup import requests url = 'https://www.clark.wa.gov/sheriff/jail-roster' response = requests.get(url) html = response.content soup = BeautifulSoup(html,""html5lib"") table = soup.find('table',attrs={'id':'jmsInfo'}) for row in table.findAll('tr'): for row in table.findAll('tr'): if (row.contents[5].string and row.contents[2].string): print ""booking date: "" + row.contents[2].string + "" charge: "" + row.contents[5].string Note that I added an if-statement that checks to make sure that both the booking date and the charge exist in the current row. You probably don't want the successful execution of your code to depend on whoever is entering data at the Clark County jail. Now, instead of throwing an error, if the script finds that one of these data pieces is missing, it just skips that row. It won't do us any good anyway. 5.0) Making use of the scrapings We've successfully scraped the website of the Clark County Sheriff's Office, but we're not quite there yet. We still need the data in a format that we can analyze, and we also don't have exactly the data we want yet. We're after the total period of incarceration, from the booking date through today's date. We could export our data as is to a CSV (spreadsheet) file first and then do that bit of processing, but I'm not planning on taking this little tutorial into a whole new software domain. Once we have the spreadsheet saved, that's it. We're through. So, let's do that computation here in Python. It won't be too difficult. Here's the code: from bs4 import BeautifulSoup import requests from datetime import datetime url = 'https://www.clark.wa.gov/sheriff/jail-roster' response = requests.get(url) html = response.content soup = BeautifulSoup(html,""html5lib"") table = soup.find('table',attrs={'id':'jmsInfo'}) for row in table.findAll('tr')[1:]: if (row.contents[5].string and row.contents[2].string): now = datetime.now() b_date = datetime.strptime(row.contents[2].string,""%m/%d/%Y"") span = now - b_date print ""incarceration period: "" + str(span.days) + "" day charge: "" + row.contents[5].string A few things are happening here. Note that I've imported another module, called datetime . This is what allows us to convert the bare text from the webpage into a useful Python date format and it also allows us to do the neccessary computation to get the timespan from today's date back to the inmate's booking date. To do that, I use a function called strptime , which takes in a text string representing a date and parses it into a datetime object. I use the same module to get the current date in the same format. Then, since we now have two datetime objects, getting the difference is as easy as using the subtraction operator. The result of the subtraction is an object of the time timedelta , which has a property called days . Run the script and you'll get something like this: NULL) The name Beautiful Soup, by the by, comes from Alice in Wonderland . It's a song sung by Mock Turtle to Alice: "" Soo--oop of the e--e--evening, Beautiful, beautiful Soup! "" It's a play on ""tag soup,"" the term given to malformed, gnarly HTML code. Tag soup is the natural enemy of web scraping. 6.0) Exporting the soup Finally, we have the data we want. Our last task is to get it into a format that's useful for further analysis. This is the aforementioned CSV file, which is basically a very generic spreadsheet file consisting of comma-separated values. Once we've saved a CSV file, we can open it in Excel or Google Sheets or whatever. It's among the more universal file formats that's out there. To move our scraped data into a CSV file, we're going to amend our script to look like this: from bs4 import BeautifulSoup import requests from datetime import datetime import csv url = 'https://www.clark.wa.gov/sheriff/jail-roster' response = requests.get(url) html = response.content soup = BeautifulSoup(html,""html5lib"") table = soup.find('table',attrs={'id':'jmsInfo'}) rows = [] for row in table.findAll('tr')[1:]: cells = [] if (row.contents[5].string and row.contents[2].string): now = datetime.now() b_date = datetime.strptime(row.contents[2].string,""%m/%d/%Y"") span = now - b_date cells.append(str(span.days)) cells.append(row.contents[5].string) rows.append(cells) outfile = open(""./roster.csv"", ""wb"") writer = csv.writer(outfile) writer.writerows(rows) A lot of new stuff is happening here. First, we're importing a new Python module, called csv . This is what we'll used to create and write to a CSV file. We also stripped out all of the print stuff and replaced it with a pair of arrays: rows and cells . As we go through row by row, we're going to make small arrays containing our two data points, which we'll paste onto the end of the main rows array on every pass of the for-loop. At the end, we open a new file for writing with the open command and then use writerows to, well, write the rows to our new file. You should now have a file sitting in the same folder as your script file, called roster.csv. And that's it! We scraped some data. 6.1) The big payoff: data I imported the CSV file into Google Sheets and screwed around with it for a while. I'm far from a spreadsheet expert. Here's a sampling that gives the averages and then totals (in days) for a handful of charges. All told, there's nothing too surprising. The big winners are murder, rape, assault, burglary, and probation violations. As far as charges with the most total days, the winner is probation violation (""community custody violation"") by light-years with over 10,000 days. (One inmate has been in for 6,000 days on a probation violation charge, though that has to be a bookkeeping mistake, right?) This would probably be more interesting with a bigger data set from a bigger jail. Hopefully, you can at least see the possibilities. Big Data isn't always or even usually just sitting out there to harvest via some friendly API. The real good stuff is more likely to be squirreled away on some government website. Hunt it down.",,Vice,There's a whole universe of data waiting on the other side of the HTML looking glass.,Hack This: Scrape a Website with Beautiful Soup,https://www.vice.com/en_us/article/3dajd8/hack-this-scrape-a-website-with-beautiful-soup
Michael Byrne,"In writing the most recent Hack This ( ""Scrape the Web with Beautiful Soup"" ) I again found myself trapped between the competing causes of blog-brevity and making sure everything is totally clear for non-programmers. It's a tough spot! Recapping every little Python (the default language of Hack This ) concept is tiring for everyone, but what's the point in the first place if no one can follow what's going on? This post is then intended then as a sort of in-between edition of Hack This, covering a handful of Python features that are going to recur in pretty much every programming tutorial that we do under the Hack This name. A nice thing about Python is that it makes many things much clearer than is possible in almost any other language. 0.0) Variables A variable is a name that we can use to store data. The idea is different than the variables used in normal algebra, where variables are used most often as unknowns that we'd like to solve for or use as placeholders for solutions. In programming, we usually give the variable a value before we do the computation, which then uses the variable value to yield a solution. Just understand that a variable stands for something else: a number, some text, a true or false value, another variable. In Python, we don't have to give variables types. This is different than in many languages, where we have to specify what a variable is meant to contain. In C++, for example, it's illegal to assign some text to an integer variable. In Python, we don't even have to specify that it's a variable at all, which is very different than other programming languages. Here's a variable assignment in C++: int x = 8 In JavaScript, which doesn't require variable types: var x = 8 In Python, which doesn't require a type or a variable designation: x = 8 Here's an example of how we might use a variable. We first assign a number to a name and then use that name in a simple addition operation. Then, we assign the result to a new variable and print the contents of both variables. x = 8 print x x = x + 2 print x y = x print y The result: 8 10 10 The print command takes whatever expression follows it, evaluates that expression, and then outputs it to the screen. This evaluation step means that we can simply print values, as above, but we can also print the results of expressions, as below.  print 8 + 10 Which results in ""18"" being output to the screen (rather than ""8 + 10""). Almost anything can be assigned to a variable in Python. This means simple data like numbers and text, but also reusable sections of code, as in functions or prewritten Python modules. If you'll remember from the web-scraping Hack This , we referenced functions from the Beautiful Soup package via a single variable. This is very, very common. 1.1) None Python maintains a constant value called None . We can assign this to variables to indicate that the variable exists but that it does not contain a value. 2.0) Lists A Python list is what would be called an array in most other programming languages. It is precisely what it sounds like: an indexed list of values. Anything that can be assigned to a variable can be an entry in a list. It's the same thing, really. If we want to create an empty list, we do this: new_list = [] If we have a list that already has some stuff in it, we can overwrite an entry at a specific index like this. new_list[0] = ""hey list"" The number in brackets is the index corresponding to a location within the list that has a value. This is how we put a value in, and also how we retrieve a value. print new_list[0] This will print ""hey list."" You will often see values added to lists with a function called append() . This adds a new value to the end of the list, which has the effect of increasing the size of the list. It's also possible to add values to a list with insert() . The difference is that the latter puts a value into the list at a specified location (index, as in insert(5,""hey list"")), while the prior just puts the new value at the end. new_list.insert(5,""hey list"") If the list isn't long enough to add an item at the index 5 position, Python will put the new item at the next available index. Note that inserting values like this doesn't nuke whatever was originally in the specified index. Everything is just scooted upward. We can remove a value from a list with the remove() function. This requires us to give said function a value to look for. If we wanted to remove the value ""hey list"" from new _list , we could do this. new_list.remove(""hey list"") If there is no ""hey list"" value, Python will return an error. The pop() function is kind of the inverse of the append() function. It will remove the last item in the list. 3.0) Definitions A Python definition is what's usually called a function or method in other programming languages. It's a piece of code that is written once and can then be called again and again by name. A Python definition will very often produce or return a value. It may also require parameters, or input values. These are supplied in between the parentheses following the definition's name. Say we have a function that returns the greater of two numbers. We might call it like this: the_max = max(1,2) print the_max The number 2 will be printed (assuming that the function is actually defined somewhere that the current script can access; see below). 3.1) Modules Imagine that you wrote a bunch of code that you'll probably want to use again, or maybe that someone else would find useful in their own program or script. You could package it into a module, which is just another, separate Python file. Like, when you import a module in your script, you're just pointing the Python interpreter to some code that lives elsewhere that the current script depends on. For example, if you needed some random numbers in your program, you are surely not going to implement your own random number generator because that would be a real pain and you would also fuck it up. I would fuck it up. Instead, we import the random module, which comes with this functionality included.  import random We now have access to the random module. What does that actually mean? Well, you might start by looking at the documentation , which will list all of the module's functions and tell you how to use them. When in doubt, read the docs.  The general idea of our import is that we now have access to all of the random goodies via the keyword random . It's like a portal or doorway ... or a bartender.  If we want, say, a random decimal number between 0 and 1, we can just use the random() function that lives within the random module. It's easy:  random_number = random.random() print random_number random_nums.append(random_number) print random_nums[0] Get it? We have access to the function through the module. We access all of the functions that are built into Python's list structure in the same way.  3.2) Packages Packages are directories of modules. If we import a package into our script, we get access to all of its contained modules. We can also selectively import modules that are components of packages, so we don't have to bring in a bunch of extra code that we don't need. It would look like this:  from somePackage import someModule 4.0) For loops It's often the case that we want to repeat some operation in our code. Say, for example, that we have a list, and we want to process each list entry in the same way. We could use a for-loop. Perhaps we just want to print the contents of our list in order. We only need two lines of code:  for item in my_list: print item The print statement here will execute once for every item in my_list . Every time the loop restarts, the next entry in the list will be assigned to the variable item . Note that the colon here is required syntax and trying to write a for-loop without it will cause an error. The indentation is likewise required for code that is contained within the body of the loop, i.e. code that will execute on every loop iteration.  We can write for loops even if we don't have a list. Say we just want to add up all of the numbers between 0 and 10. We would do that like so:  total = 0 for i in range(0,10): total = i + total Python here is setting up a list of numbers between 0 and 10 for us with the range() function. This can be extremely useful. 5.0) If-then statements The conditional statement is a crucial piece of any programming language. It allows us to write code that will only be executed if some condition is met. It's pretty simple: if (2 > 1): print ""duh"" The statement is true, so the script will print ""duh."" We can elaborate on this by using an else clause, like so. if (2 > 3): print ""no way"" else: print ""derp"" Easy enough. Note again that the colons and white space are required. 6.0) Whitespace Part of the Python attraction is that it's a very pristine language. It's not all junked up with ""unnecessary"" symbols, particularly curly braces. In many if not most languages, curly braces are used to group together lines of code. This grouping has all kinds of meanings in programming—for example, it may demarcate the section of code that is to be repeated in a loop, or the section of code that is to be executed or ignored in an if-statement. Our if-then from above would look like this in C++: if (2>1){ cout << ""duh""; } Instead of curly braces, Python uses whitespace. Statements that are at the same level of indentation are taken to be grouped together. Consider this: if (x): print ""duh"" print ""derp"" No matter what, the script is going to print ""derp"" because it's not part of the conditional. However, if x is not true in the code below, nothing will be printed because the second print statement is part of the conditional. if (x): print ""duh"" print ""derp"" This is a fairly extreme crash course, but the aim is to clarify some things you'll see elsewhere in Hack This and out in the programming world. For a deeper introduction, the Python documentation has you covered in its own tutorial . Learnpython.org, meanwhile, offers a beginner tutorial that adds interactivity. And then, of course, there is Learn Python the Hard Way . Recommended. Read more Hack This.",,Vice,A very short introduction.,Hack This: An Overdue Python Primer,https://www.vice.com/en_us/article/nz7kzz/hack-this-an-overdue-python-review
Michael Byrne,"If information can be illegal, a number can be illegal. It's an obvious statement—numbers are information—but one that might lead to absurd conclusions, as a computer scientist named Phil Carmody attempted to demonstrate in 2001 with the discovery and publication of a stupidly long prime number representing a section of forbidden computer code implementing a DVD decoding algorithm known as DeCSS. Understand, there is a number for everything. There is a number for you, and there is a number for me. Certainly, they are very large and in a sense arbitrary numbers, but, because there is no limit as to how large a particular number might be, we can be assured that our personal numbers exist and that they will encode every facet of our being. There is a number, one number, that encompasses every cell, every molecule in our bodies, and that same number will contain our entire past, present, and future. All I'm really saying is that we can be described by information. And if we can be described by information, that information can then be encoded numerically—binary or otherwise. We can imagine much simpler and more deterministic-y things in the same way, such as computer programs. A computer program, after all, is naturally and necessarily described by a number in the form of binary machine code. We might simply take machine code, which is a long sequence of numbers that can be interpreted by a machine, and add it all up, with the result being some number. It doesn't have to be machine code, however. Computers only understand numbers, generally, and so we have the ASCII standard, which provides for a standardized way of representing text as numbers. This post could very easily be converted into a number just by doing a one to one ASCII conversion, and so too can higher-level computer programs, like those written in C or JavaScript or whatever. So, there are different ways of representing things as numbers. ASCII is a simple encoding in which human-readable information in the form of characters and symbols is simply swapped out for numbers. Upper-case ""A,"" for example, is 65 in ASCII. But computer code can also be compiled into machine code, which is an arrangement of data consisting of a fundamental rearrangement or translation of information such that it can be read by a machine in the machine's own language. The distinction is important when we look at a sort of number known as an illegal prime. Simply, this is a prime number that represents information that is illegal to possess. The classic case is a prime number produced by Carmody in 2001. This is a number that represents an implementation of the DeCSS decryption algorithm—which can be used to bypass DVD copyright protection—in the C programming language. More specifically, it corresponds to the binary representation of said C program once it's been compressed ( gzip'd ). Carmody's prime was one of a number of creative protests against the indictment of DeCSS' creator Jon Lech Johansen by a Norwegian court. One of these protests included production of a t-shirt featuring the DeCSS C code by Copyleft LLC , which itself would eventually be named in a MPAA lawsuit seeking to suppress/prevent publication of DeCSS code. The MPAA's attack on a t-shirt is really what got Carmody going. ""In simple terms I believe that source code, which is pure information to my way of thinking, cannot intrinsically be illegal,"" he writes . ""So the banning of the Copyleft T-shirts, for example, I find to be absolutely bizarre. I therefore just wanted to have the data replicated in another form, but specifically a form which could not sensibly be considered illegal."" ""The reason it is archived is not that it is supposedly illegal, but its size, nothing more."" This form was a very large prime number. The nuts and bolts of producing a prime number that can represent a computer program are a bit messy (it involves padding the original program code), but the basic idea was that Carmody's prime would be so large that it would crack the top 20 largest known primes, and, as such, it would have to be listed publicly for that reason alone. ""My original plan was to make sure that the 'bits,' the raw data, of the DeCSS code was archived somewhere where it should be beyond the reach of the law,"" Carmody explains. ""Somewhere where the number would be allowed to be printed because it had some property that made it publishable, independent of whether it was 'illegal' or not. I needed to find a presentation of the data such that it had an intrinsically archivable quality."" ""The reason it is archived is not that it is supposedly illegal, but its size, nothing more,"" he says. Carmody would later go on to discover a prime offering the other aforementioned sort of program code representation—a prime that is readable (executable) by a machine. To this end, he developed what is arguably the smallest possible executable code (code that can be read by a machine) that can be represented by a prime number. It consists of a single instruction returning an empty processor register, and is represented by the number 9923. For comparison, the prime representing the DeCSS code is 1,905 digits long.",,Vice,Let's start with the fact that everything is a number.,Can a Number Be Illegal?,https://www.vice.com/en_us/article/kb7km9/there-is-a-number-for-everything
Michael Byrne,"As a programmer, I have zero interest in game development, but I find this dude's deep dives into (mostly) Super Mario World fascinating. As of the past month, he has a couple of newer videos up under the Retro Game Mechanics Explained account, but a few more live at an old account , including the one that first got my attention about glitchy Yoshi . The videos are as advertised: code-level, computer-science-y explanations of what's behind the sorts of games that are primitive enough to be understood at, well, the code level. While a game nowadays might be written in a super-high level visual scripting language, here we have 65c816 assembly code . RGmechEX has a Patreon page up if you'd like to see more of this kind of thing.",,Vice,Retro Game Mechanics Explained dives deep into SNES assembly code.,A Lesson in Random Number Generation from 'Super Mario World',https://www.vice.com/en_us/article/ezpk44/lets-learn-about-random-numbers-from-super-mario-world
Maya-Roisin Slater,"Before he found a way to merge them, programmer and artist Tarik Barri considered music and visual art to be dueling disciplines. But when he learned how to use Max/MSP , a programing language often used for multimedia purposes, to create visual compositions, the marriage between sight and sound became a blank canvas for new creative endeavors. Barri started out by making a 2D visual sequencer, experimenting with visuals to accompany his music. But the desire to dismantle barriers between the digital composer and the audience, to literally show people what his compositions looked like, led to the creation of Versum , a realtime virtual 3D world that invites both an audience and a composer to look at music and listen to visuals. Photo by Lea Fabrikant “I was a big fan and user of FruityLoops [now called FL Studio] back when I was still studying psychology—I did a lot of studies half way—and I had a dream that a new version of FruityLoops had come out and that it was a 3D version,” Barri told The Creators Project during an interview in Berlin at Ableton’s Loop Summit . “All of a sudden, I could see all of these notes flashing past me in 3D, and it was awesome. It wasn’t, of course, fully fleshed out in my dream, but it was super exciting. I woke up and was very disappointed that it didn’t exist.” As the adage goes: sometimes if you want something done right, you have to do it yourself. Barri embarked on Versum’s initial programming, building its visual element in Max/MSP and introducing bits of Java coding. Now, the software mainly runs in small programs directly on Barri’s graphics cards, which allows generated images to maintain their integrity without a significant frame-per-second lag. Audio components are manipulated through Max for Live and Ableton Live . Barri has completed Versum collaborations with artists like Thom Yorke, Monolake, and Nicolas Jaar, and partnerships with other artists are in the works.  In practice, Versum feels far from watching a preconceived visual piece and more like a live exploration within a fantastic, conceptual space. Barri uses fixed grid points to ensure he doesn’t get completely lost while navigating through the virtual program, but for the most part, constant discovery is an integral part of Versum’s design. “I have presets where all kinds of visual effects and parameters are set. I can smoothly interpolate one preset or another, but that preset may or may not involve the exact location where the virtual camera is floating. Very often, I just choose to have all the stuff around me revolving and changing, while still having full control over the camera using my joystick. Sometimes it happens that I do literally get lost or it just becomes blackness, and then I can click on presets to smoothly have my camera float back into a known territory,” Barri explains. Materializing dreams is not without its technicalities, and Barri has encountered plenty of glitches along the way. But the satisfaction of giving life to your most intimate imaginings is unparalleled and precious—part of the reason why Barri has no intention of commercializing Versum anytime soon. “I don’t want to be cheap or whatever. I’m very happy to share many of these things, but the software is so specific and weirdly constructed. It’s also buggy in some ways, but I know my way around my own bugs. People have suggested that I don’t have to fix it myself—that I should just open source it—but I also really like having control of where this thing is going. It’s very tightly connected to my soul, in a sense, and I would really hate to see it go in another direction,” he explains. While this particular program will remain a personal project, Barri is working on feeding elements of Versum into his other collaborations. Barri previously created visuals for Thom Yorke and Nicolas Jaar, including a Versum collaboration with Monolake. Partnerships with other artists are also in the works. In partnership with Dutch programmers, he’s working on software called Videosync , which creates visual plugins, enabling less-proficient coders to build their own 2D and 3D realities in Ableton Live. It opens the door for those familiar with the software to create compositions in concert with Barri during rehearsals and performances. In doing so, the world which came to him in a dream remains Barri’s domain, albeit one that comes with a welcome mat and unlocked entrance, ripe for exploration.  This interview was conducted in Berlin at Ableton’s Loop Summit . To learn more about Tarik Barri’s work, visit his website . Related:  Now You Can Make Beats in Virtual Reality  Watch Clips from an Audio-Reactive Generative Visual Set  This Fish's Movements Control The Sound And Light Of A Live DJ Set",,Vice,The idea for the audiovisual sequencing system came to Barri in a dream.,This Performer Turns Musical Dreams into 3D Worlds,https://www.vice.com/en_us/article/kbn4qv/tarik-barri-musical-dreams-3d-worlds
Michael Byrne,"Welcome back to Hack This . It's been a few weeks. For that I am sorry. To be honest, I hit the end of my list. Which is good because, hey, now you can consume a bunch more Hack This entries , and bad because coming up with topics isn't always easy. To that end, I can't encourage you enough to submit tutorial suggestions to me at michael.byrne@vice.com and-or @everydayelk . I'm also not just looking for programming-centric topics, but am aiming to expand the scope of Hack This to include more everyday how-tos for navigating technology. That's actually how the thing started waaayyyy back in Motherboard's early days; I just tend to think in code. This edition of Hack This is going to be kind of in-between. Basically, we're going to be setting up programs or scripts to execute automatically at specific time intervals. Scheduling, automation. At the minimum, it will involve a wee bit of code, but, depending on what you want to accomplish, it could also involve very elaborate shell scripts glueing together even more elaborate scripts (or regular old programs). We'll focus here on a Mac OSX utility called launchd, but will make sure to nod to the Windows and Unix equivalents, which shouldn't be all that different. Prerequisites are mostly nil, but you'd do well to read the post about using the command line here . 0.0) Why automate There are computer-based tasks you do—or should do, but don't—that are the same every time, or are close enough to being the same. This might mean digital housekeeping, like deleting files or syncing things to the cloud, or it might mean checking a data feed for new updates or verifying the status of some websites that you administer. A lot of these things you might not even think of as being repetitive and automate-able. For example, every couple of weeks I check Boomkat for recommended new releases in a few specific genres and make a list of things to listen to; I do something similar with Maximum Rocknroll top-ten lists . There's no reason I couldn't just write a script that does this and then emails me a list of stuff to listen to every couple of weeks. Given the amount of time that we spend on these machines, not automating almost just seems like a form of procrastination. 1.0) launchd First off, launchd is much bigger than what we'll be using it here for. This utility is what your (Mac OSX) system uses to boot—first firmware launches the kernel, which then launches launchd. Launchd then runs through the contents of a couple of directories looking for stuff to run, first launching required background processes and then startup user processes. As things go along, it will stand ready with an army of daemons ready to launch on-demand if they're needed to support some possible user action. Given the amount of time that we spend on these machines, not automating almost just seems like a form of procrastination. 1.1) plists The jobs that launchd needs to complete (the programs or scripts it is to run) are specified in files known as a property lists or plist files. A plist is a key-value list (like a Python dictionary) where the keys are the names of different job properties, such as the file-system path to the executable file to be run; the arguments the executable needs to run properly; the id of the user asking for the program to run; and, if applicable, a future date and time for when the program is to run. 2.0) What can we launch? Pretty much anything. Any program you can open with the click of a mouse you can open using the command line ( open -a program_name ). And anything you can open on the command line, you can launch with launchd. This will be most useful to you with shell scripts, which are just listings of shell commands (command line commands) that are packaged into a single file and do (automate, perhaps) something useful. Here, let's automate something fairly simple. For me, it would be useful to have a sort of system janitor that comes around to certain file directories and trashes certain sorts of files. For example, in my day to day work for Motherboard, I download loads of image files, and, after a while, I wind up with a downloads folder that's clogged with screengrabs, Shutterstock photos, public domain photos, and so on. I need these photos for like five minutes after I actually download them, but they wind up sitting around and accumulating. Fortunately, we can solve this digital clutter problem with automation. So, we're going to make a Bash script that does this file-deleting task and then create a corresponding plist file telling launchd how and when to run it. First, we need the script. 2.1) A script to delete files in a given directory This is just an example of something that might be useful, but I'll quickly explain what it actually does. It's a single line. find $HOME/Downloads/ -maxdepth 1 -type f -name ""*.jpg"" -delete I'm using the shell command find to match every file (specified f for file following to the -type argument) in the given directory (but not any further subdirectories, per the - maxdepth argument) with the .jpg extension (an asterisk, or wildcard, means ""anything""). The final -delete argument nukes the results of the find. 2.1.1) A safer way It should be immediately apparent that this is a sketchy way to do things. If we just run this every day at a specific time, we're eventually going to fuck up and have something in our Downloads folder that we don't want to delete. I solved this by adding a check, which will make the user approve the deletion first. This seems reasonable. We need to use Apple's kind of janky built-in scripting language AppleScript for this, unfortunately. This will allow us to show a dialog requiring the user to click a button before deleting the files. AppleScript is a whole other deal, so I'll just give you the code here ( which is sourced from Stack Exchange ): set timeoutInSeconds to 60 set abortOnTimeout to true tell application (path to frontmost application as text) try set dialogResult to display dialog ""Do you want to clean downloads folder?"" default button 2 giving up after timeoutInSeconds on error number -128 return end try end tell if gave up of dialogResult and abortOnTimeout then return end if do shell script ""/Users/michaelbyrne/cleanup.sh"" Basically, if the user clicks cancel or doesn't respond within a set amount of time, the dialog will close and the job will not run. We can run the above script (which I called ""cleaner.scpt"") from the Bash shell with the command osascript cleaner.scpt . All of these script files, by the way, are just text files and can be edited in and saved from any old text editor. It's ultimately up to us to tell the computer how they should be interpreted. If I have a Bash script with a "".sh"" file extension and I'm currently operating within a Bash shell session (just type ""Bash"" into an OSX Terminal window), I can run it just by entering the filename prefixed with its filesystem path and hitting enter. If I'm in Bash or another shell (read: command line-based program), I'll need to specify that my AppleScript script is to be run with the AppleScript interpreter. That's the ""osascript"" command above. (AppleScript is a Open Scripting Architecture language and is thus interpreted by an OSA language interpreter, osascript—but really don't worry about it.) Likewise, if I had a Python script, I would need to run it with the ""python"" command. 3.0) More on plists By default, you should (on a Mac) have a directory called LaunchAgents in your user Library directory ($HOME/Library/LaunchAgents). Find it. This is where we'll put the plist files launchd needs to run scheduled jobs. You probably already have a couple of plist files in there for installed applications that automatically start when you login to your computer. In mine I already have plists for Adobe, Spotify, and Google. A plist file is actually an XML file, which is a file containing identifiers enclosed by tags specifying the types/meanings of those identifiers. If you remember, a plist file contains a list of key-value pairs. One such pair might look like this, where ""label"" is the name I've given the scheduled job (which is a different name than that of the script itself). The label is of type key while the value is of type string . label  cleaner  Here is the complete plist file for my job. It's scheduled to run every day at 4:00 PM. Don't get freaked out by the XML syntax.    Label  cleanme.job  ProgramArguments   /usr/bin/osascript  /Users/michaelbyrne/cleaner.scpt   RunAtLoad   StartCalendarInterval   Hour  16  Minute  00     You can see that the value needed to specify a scheduled job is actually an array of values, which is like a list. In this case, there's just one entry in the list, which is itself another list, this time of the dictionary type. The dictionary contains more key-value pairs, specifying the hour and minute that our job is to run every day. Note also the array containing the program arguments. Often, when running a shell script we need to provide some additional information, which is usually just listed on the command line after the name of the command. In our XML array, we just list out the items that we would normally be listing on the command line. Here, we need to specify that our script is being run as AppleScript, which is the first argument ""osascript."" I have this plist file saved in LaunchAgents as ""cleaner.job.plist."" 3.1) Loading the job With the file saved in the proper directory, we're still not quite done. We need to load the job, which we can accomplish with the Apple command line utility launchctrl. launchctl load ~/Library/LaunchAgents/YOURJOB.plist You can verify that your job is loaded by running the list command: launchctl list This will bring up every job that launchd maintains, which is a lot, but you should still be able to find yours. Using launchctl, we can also pull the trigger on our scheduled job at any time by using launchctl start my_job . Discontinuing the job is as easy as running launchctl unload my_job (which leaves the plist file intact and reloadable). 3.2) LaunchControl Unsurprisingly, there is software that can handle plist files for us. This is LaunchControl , a small program that offers a GUI for dealing with scheduled launchd jobs, effectively replacing the command line utility launchctl. And, given the small bit of background above on using launchctl, navigating LaunchControl should be pretty intuitive. Rather than deal with ungainly XML files, LaunchControl allows us to drag and drop plist properties, including scheduling, arguments, environment variables, etc. LaunchControl isn't free. It costs $10, but this is on the honor system (there are no keys or serial numbers or anything). That said, for simple scheduling tasks as in our example, I think launchctl is just fine. 4.0) Cron Launchd is supposed to have superseded a more general Unix utility called Cron, which is still available on OSX (and other Unix-based systems). Rather than plist files, it's based on crontab files, which are lists of shell scripts/shell commands (jobs) wherein each job is prefaced by a CRON expression. A CRON expression is simply a string of six single-character fields denoting when and how often a job is to be executed. Here's what our example would look like using cron: 0 16 * * * osascript /Users/michaelbyrne/cleaner.scpt So, we're again running the job at 1600 hours (4:00 PM), and leaving the date and day of the week information blank, which means that it will run daily. 5.0) Windows Task Scheduler While command-line scheduling tools for Windows exist, the canonical method is using a built-in app called Task Scheduler. I don't have a Windows computer so I can't run through the process with you, but it looks to be pretty simple (and akin to using LaunchControl for scheduling). It also seems to let you build the jobs themselves via GUI, bypassing the need to write scripts for common/non-custom tasks. 6.0) Automator Mac OSX comes with a utility called Automator, which looks a lot like the aforementioned Task Scheduler. It's GUI-based and allows users to skip the shell scripting and command lining for building and scheduling tasks, though it has capabilities well beyond basic scheduling. I've dorked around with it a bit (for task scheduling) and mostly come away frustrated and missing the open canvas of automation via shell scripting. That said, I can't tell you to avoid it. In writing this, I've realized I need to do a deeper post on shell scripting itself and what we can do with it. My file-deleting example is simple by design, but things can get much, much cooler. Imagine, for example, what we could do with automation and the Twitter API or a web scraper . Fun stuff. More Hack This:  Hack This: How to Consult Google's Machine Learning Oracle  Hack This: How to Start an IRC Channel  Hack This: An Overdue Python Primer",,Vice,A simple OSX utility is ready-made to free us from repetitive computing tasks.,Hack This: Automate Your Operating System with Launchd,https://www.vice.com/en_us/article/bmv3km/hack-this-automate-whatever-with-launchd
Michael Byrne,"Writing and learning JavaScript can feel completely bonkers. There's often little to no free space to just sit down and write lean, elegant code in a way that even approaches ""from scratch."" This is because of frameworks, which are essentially pre-written libraries of code meant to be used in fairly specific contexts according to fairly specific patterns—that is, they are often only useful when used in prescribed ways. To build a web app is to wield these libraries and architectural patterns, usually several at a time. And this means learning the libraries and architectural patterns, which, for something like AngularJS, can be a real fucking chore. Learning aside, it can also mean building a web app that is bloated with library code. This is code that will eventually need to be parsed and interpreted by a browser, or, in the case of Node, by a server. That parsing can be expensive and time-consuming. Frameworks are pretty useful, however. They provide standardization, simplification, and abstraction. Programmers write less code and, crucially, less complex code. As a result, they theoretically make less mistakes. In particular, frameworks often provide ways of dynamically updating webpages without manipulating HTML directly—and this is how webpages become fullon web apps in the first place. So, frameworks become a neccessary evil, but maybe they don't have to be quite so evil. Enter Svelte, a new JavaScript framework that offers coders the ability to use frameworks for developing web applications while delivering only ""vanilla"" JavaScript code—that is, code that looks an awful lot like it was written by hand, but was really crafted using the usual frameworks (Angular, React, Ember, whatever). The benefit comes in the amount of code that's ultimately needed to support a web app. That's the pitch, anyway. ""What if the framework didn't actually run in the browser?,"" writes Svelte developer Rich Harris in an introductory post . ""What if, instead, it converted your application into pure vanilla JavaScript?"" This is what Svelte offers: a compiler capable of analyzing source code, including that contained in outside frameworks, and carving away features that aren't actually used by the current web application. The result are small, relatively lean modules. ""Existing frameworks tend to be large enough that your application becomes slow to start up on mobile phones, especially on Android, which is where the bulk of market growth is happening,"" Harris told InfoWorld . ""Svelte solves these problems by removing those abstractions. The hard work happens at compile time rather than run time—it spits out highly optimized low-level DOM manipulation code specific to your application."" I guess the catch is that, while this may simplify things for the browser tasked with running the code, for the developer it's yet another framework. Part of where things can get really messy is when we start dealing with build tools, a subcategory of framework used for managing code dependencies and packaging code up for IRL deployment. I haven't totally figured out how Svelte fits into all of that, but am willing to give it a try.",,Vice,"A new tool carves up JS frameworks, leaving only the stuff your web app actually needs.","Meet Svelte, the Anti-Framework JavaScript Framework",https://www.vice.com/en_us/article/kb7y7v/meet-svelte-the-anti-framework-javascript-framework
Emiko Jozuka,"Andy Flessas, aka computer animator and roboticist andyRobot , wants to bring robots to life by ""animating"" them. To date, robots enabled with his unique software have taken to the stages of the likes of popstar Bon Jovi and DJ Deadmau5, to name but a few. Now Flessas wants to bring his software to the artistic masses with "" Robot Animator, "" a plugin for Autodesk's computer animation software Maya, which he released through German robotics manufacturer KUKA just last month. ""Robot Animator creates a new interface inside Autodesk Maya that's specific to robotics,"" Flessas told me. The software simplifies robotic programming for the user and ""applies speed and acceleration correction"" as users animate their robot in a simulation and see their commands replicated by a robot in real life. ""A lot of people were approaching me and wanting to buy a robot that they could control, but no one knows what they want to buy,"" Flessas told me. ""They just know they want to work with robots."" Obsessed with technology from a young age, Flessas initially trained as a computer animator before he attended Zurich-headquartered manufacturer ABB's robotics training school in Detroit in 2000, where he certified as a roboticist. A stalwart proponent of the positive uses of technology, Flessas eventually mashed together his knowledge of computer animation and robotics to create his robot animation software, which he's been fine-tuning since the 1990s. With Robot Animator, Flessas wants to allow artists to incorporate a KUKA industrial arm within their installations and art practices without having to invest time into learning complex programming skills, or dishing out lots of money for technical support. ""This software reflects what I think it is that people need,"" he said. andyRobot working on Deadmau5's robotic arm. Image: andyRobot For the moment, Flessas has started collaborating creatively with Nigel Stanford, a visual artist and musician from New Zealand, and Koichiro Doi, a Japanese fashion photographer, to bridge the divide between robot and creator. Stanford, who is using Flessas' software to transform his robotic arms into bandmates, said that working with robots wasn't exactly easy, but that Flessas' software gave him greater control. ""[Robots] are potentially dangerous and need to be treated with respect,"" said Stanford. ""Robot Animator was great to use, without it, I wouldn't have been able to program the robots in time to the music, as the standard programming tools are not time-based."" Over in Japan, Doi shared similar views, saying that Flessas' software allowed him to control robotic arms more intuitively. Doi intends to use the roving robotic arms instead of static tripods by mounting cameras, props, or lights on them. ""I want to create a scene where everything moves in sync,"" he said.""Robots have a very cryptic language and none of them speak the same one."" ""Robots have a very cryptic language and none of them speak the same one."" While Flessas aims to bridge the gap between creators and robots, he also wants his software to provide a common language for industrial robots in the future. Deadmau5's robotic arm. Image: andyRobot ""In factories, you'll see a manufacturing line with hundreds of robots and they'll all be from one brand. They don't really mix them in factories,"" said Flessas. ""Robots have a very cryptic language and none of them speak the same one. So what I'm doing is using Robot Animator to create a new language of animation that happens to be robotic."" Flessas envisions a future where robots from different companies could collaborate with one another in a manufacturing setting, drawing on the strengths of each other and making whatever task they're carrying out more efficient. ""We can mix and match,"" said Flessas. ""What that means for industry is that manufacturers can say, 'Aha, I can talk with a different brand.' It's like the animator becomes the bridge between the different robot languages, bridging together the different robot brands with the common language of animation,"" he said.",,Vice,'Robot Animator' lets artists control industrial robot arms.,This Animator-Turned-Roboticist Is Making a Universal Language for Robots,https://www.vice.com/en_us/article/pganmn/this-animator-turned-roboticist-is-making-a-universal-language-for-robots
Lauren Messman,"Before Mark Zuckerberg runs for office , he apparently wants to crank out his own version of Stranger Things or whatever, according to the Hollywood Reporter. On Wednesday, Facebook announced that it will begin building out its mobile app's new video feature by funding original scripted and unscripted programming. It's a move that's being lead by College Humor co-founder Ricky Van Veen and could place the social media site alongside streaming giants like Amazon, Netflix, and Hulu. ""We're exploring funding some seed video content—including original and licensed scripted, unscripted, and sports content—that takes advantage of mobile and the social interaction unique to Facebook,"" Van Veen said in a statement. ""Our goal is to show people what is possible on the platform and learn as we continue to work with video partners around the world."" The social network will likely partner with organizations—the way it did with Facebook Live —and individual digital stars, which could be good news for any Vine stars looking for work. It's not totally clear how the whole thing will work out, but it'll likely be reminiscent of Youtube's original channels, played in a Snapchat video format on mobile, because we could all use more reasons to stare blankly at our phones for a while. Photo via Flickr user Andrew Feinberg",,Vice,The social network is aiming to fund scripted and unscripted programming to bring to its mobile app.,Facebook Is Going to Start Making Original Shows Because Everyone Else Is,https://www.vice.com/en_us/article/vvdxp4/o-facebook-vai-comecar-a-fazer-series-originais-porque-toda-a-gente-esta-a-fazer-o-mesmo
 ,"Our increasingly digital lives, full of social media, games, films, and other web products, are mostly designed and controlled by others. But what if we could wrest some of that control back, giving users sovereignty instead of granting it to businesses? That’s the idea behind Loop , a game-like experience where users explore and alter virtual worlds with code. Created by German interaction and motion designer Stefan Wagner as part of his master studies at the University of Applied Sciences Würzburg, Germany, Loop is an attempt to override the mental programming created by media consumption. Using an Oculus Rift headset attached to a Leap Motion sensor and a mechanical treadmill, the Loop player walks through Wagner’s virtual worlds, where they are confronted with various scenes and surroundings that more or less reference real objects and scenery in the form of “system simulations.” By raising a hand and grabbing “code objects,” the user can interact with the virtual worlds, and in the process change the pieces of code upon which the simulations are built. Wagner created Loop on Unity 3D, using the Oculus Rift and Leap Motion SDKs skeletal tracking throughout the development process. To generate the landscapes seen in the background, Wagner used Terragen by Planetside Software . “I approached the design to be overall bright and positive, with many references to nature,” says Wagner, who was visually inspired by the “planet factory floor” scene in Hitchhiker’s Guide to the Galaxy . “On the other hand, I wanted the world to be understood as something artificial, which is why landscapes and objects are often simplified or even transitioning between low-polygon style and ‘real-looking.’” The colored polygon figures, which contain the pieces of code, add a touch of arcade games’ point-rewarding system into the mix. The interface that presents the code is more technical-looking, with Wagner saying that it mostly refers to hologram interfaces from recent science fiction artworks. The style is designed to create the impression of a device connected to the world surrounding the player—”a futuristic machine allowing the user to change the structure of space and time,” Wagner says. Some might ask how, on a technical level, the user alters Loop ’s code? Wagner explains that the underlying rule set, exposed through code (Javascript that Wagner learned via Processing ), defines the structure and visual presence of the game world and its inhabitants’ behavior. The gameplay is designed to connect the audience of games to the “bare substance” of digital game design. The code, or ""rule set,"" can be changed and therefore understood and deconstructed. “The game, defined by its rules, can be transformed into something else—so, if you will, the goal of the gameplay is to change the gameplay,” Wagner says. “On the other hand, the concept raises questions as to what a game is in the first place—there is no defined target, no obstacles or enemies, and at the end of the game, you start right at the beginning of it.” “I think it is important that people have a general understanding of how media is being created—how stuff ‘works,’” Wagner adds. “This way, they will hopefully develop some sort of [mature understanding] of these technologies.” Loop from Stefan Wagner on Vimeo .  Click here to see more of Stefan Wagner’s work. Related:  Explore Salvador Dalí's World in Virtual Reality  Ditch Work for a Virtual Trip to the Guggenheim  This Guy Just Spent 48 Hours in Virtual Reality",,Vice,Code Like a God in Virtual Reality Video Game ‘Loop’,Entertainment,https://www.vice.com/en_us/article/9an8pv/loop-virtual-reality
Michael Byrne,"Whether they're being registered by the rods and cones of a human eye or being sent across a computer network in binary streams, images are just data. Pixels and parameters. Image editing thus consists only of data operations, usually a whole lot of them. Even a cheap-feeling Instagram filter can consist of many pixel-by-pixel passes over an image file, while the computational demands of software like Lightroom make for some of the heaviest lifting in consumer computing. As images are just data, it's easy enough to treat them as such. That is, we can readily trim away all of the GUI trappings of image editing software and get right down to bare computation—editing images via bare code, that is. Why would you do such a thing? Well, for one thing it opens up image editing to all kinds of clever hacks and automation. For example, if you had a website like this one and wanted to make sure that it wasn't ever linking to/referencing images above a certain size or resolution (in the interests of reasonable page-load times) you might have a script somewhere that would chew through the site's HTML automatically resizing images as it encounters them. Or maybe you just wanted to make some of your own image filters that could be invoked from your system's command line or a Python shell. Or maybe you wanted to make algorithmic Photoshop art. Or maybe you just want to understand better how computers handle images. Computer vision is a huge and increasingly intense field nowadays, but it remains a very challenging one. We're still on the ground floor (OK, maybe the mezzanine) when it comes to facial recognition, and even recognizing characters from books gives computers a hard time. A computer that can look at a visual scene and make sense of it isn't that much better than sci-fi. Python makes code-based image editing pretty easy, which is no surprise because Python makes code-based anything pretty easy. (Most code-based how-tos you'll see on here will be in Python.) There are two main tools we can use within the language to accomplish this, but we're going to stick to the more basic Python Imaging Library (the other is OpenSource Computer Vision or OpenCV). Before going on, you'll obviously need to have Python installed. It can be downloaded from here . For this how-to, you can do everything from within the Python interpreter, which is just a shell that you can punch Python commands into. For writing Python scripts (bundles of commands functioning as a program, basically), you'll need a text editor like Sublime Text or Atom (but any text editor will do). If you've just downloaded Python, you should do two more things first. One: spend 10 minutes doing this ""Hello, World"" Python for non-programmers tutorial . Two: spend another five minutes doing this tutorial on using Python modules .  0) Image representation 101 I said above that images are all just data, but what does that really mean for a computer? First of all, all images are displayed as bitmaps. A bitmap isn't really a format so much as it a thing: a two-dimensional grid of pixels. A bitmap is defined by only two fundamental features: resolution (the number of pixels it contains) and color depth per-pixel. Color depth just refers to its maximum information content of a pixel in terms of bits. A one-bit pixel can only represent black or white; eight bits allows for the unique representation of 256 grayscale states between black and white; 24 bits gives us RGB color, with eight bits representing each of the red, blue, and green components for 16,777,216 possible colors. We mostly live in a 24 bit world, but color depth can increase further with the addition of extra channels, like an alpha channel. This fourth channel, which increases the pixel's depth to 32 bits, carries transparency information, which can specify how the different RGB channels are to be blended together. A RAW data file just lists bitmap image information row by row, which can wind up being way too much data to reasonably edit and-or send around a network. So, we have formats like JPGs, GIFs, and PNGs. The differences between them have to do with supported compression schemes and allowed pixel depth. A PNG, for example, might have an alpha channel, whereas a JPG most definitely won't. 1) PIL to Pillow to PIL The actively developed version of the Python Imaging Library is actually called Pillow, which you can install using Python's pip or easy_install tools . If you did the modules tutorial above, this will make sense. If not, go back and do it. It takes all of five minutes and will save you some bafflement. pip install Pillow or easy_install Pillow Next, we need to actually import the the components of Pillow/PIL that we want: from PIL import Image, ImageFilter  2) Open an image Assuming you have an image somewhere on your computer to open, the ""open"" function will give us a representation of the image that we can do stuff to: img = Image.open('jpl_pioneer_galaxy_full.jpg') You'll probably have to give it some path information as well, assuming the image isn't in your currently active directory (e.g. where in your file system you opened Python from). You can find out what that directory is by using the getcwd() function from the os module, like this: import os os.getcwd() If the image you want to work on isn't in the directory returned, then you need to be more specific in your open command. For example: img = Image.open('C:\\main_jpl_pioneer_galaxy_full.jpg') You need the double slashes when specifying file paths in Python because a single backslash is often used as an escape character . 3) Look at your image Next, let's make sure we can at least see the thing: img.show() Did it work? Probably not, actually, particularly if you're trying this on Windows. The show() function is basically just a script that tries to tell your system to do its default image opening thing and it's not guaranteed to work. It doesn't for me and I'm not even gonna bother trying to find out why. All it does is open the picture in some other photo viewer. Here's a workaround using Python's webbrowser module: import webbrowser webbrowser.open('C:\\path\\to\\image.jpg') That should definitely work with the image opening in your system's current default image viewing program. Mine is Paint. 4) Apply a filter So, what do you actually want to do to this image? Let's start with a filter. To see your options, just run the help() function on the ImageFilter submodule ( help(img.ImageFilter) ) you imported at the beginning. It's going to give you a huge list of functions corresponding to image editing stuff you're probably at least somewhat familiar with (blur, sharpen, etc.). Before going on, I need to resize the image I selected above. It's currently a big honking thing and I don't really need to deal with a big honking thing for this 640 px-wide space. Resizing my image while keeping the same aspect ratio isn't completely obvious. The pure way would be taking the desired width and then just doing the math to come up with the desired height matching that aspect ratio. Then you could use the resize() function, which you can find explained here: help(img.resize) . The resizing fudge is to the use the Image module's thumbnail method. First, you need to specify a desired size as a height/width pair: size = (640,640) And thumbnail will take the specified height and automatically calculate a width. Like so: img.thumbnail(size) Did it work? Nope. I forgot to save that shit. Let's try: img.save('C:\\main_jpl_pioneer_galaxy_full_small.jpg') webbrowser.open('C:\\main_jpl_pioneer_galaxy_full_small.jpg') Again, I use webbrowser.open to view the newly resized image. Now let's apply a filter. img = img.filter(ImageFilter.BLUR) img.save(""C:\\C_S\\blurred.jpg"", ""JPEG"") webbrowser.open('C:\\C_S\\blurred.jpg') Hot damn, it worked. That's not exactly mind-blowing stuff, but at this point we've just barely stuck a toe into the PIL library. 5) ImageOps, ImageChops There are a dozen or so PIL submodules in all. Image and ImageFilter are just two of them, while ImageOps is more recent, ""experimental"" module. It's purpose to provide a set of ready-made image processing functions. Here you can do things like cropping photos, changing photos from color to grayscale and back, deforming photos, inverting photos, solarizing photos, and quite a few more. ImageChops, meanwhile, is a module that provides tools for doing channel operations, e.g. performing arithmetical operations on images. Most of the ImageChops functions simply take in one image and return another. I could, for example, take two images and add them together, pixel by pixel. First, here's the code: The result of adding the first two is the third one: Cool. 6) Going deeper Things start to get deep pretty fast. An obvious extension of this is to start packaging sequences of operations and commands into scripts. I could have done the addition above and even the stitching of the three images (into a single image, which I really did in Paint) in Python code. I could also write my own filters and image manipulation functions that work within/with PIL. Elsewhere in the module, built-in functions exist for pixel by pixel math operations; coloring pixels; creating masks and transparency; convolution; and even applying binary arithmetic to images. You could write an entire Photoshop bot here and task it with the creation of algorithmically generated dank memes, or maybe even write something really useful. For that, you are are your own.",,Vice,"Algorithmic Photoshop, at your fingertips.",Hack This: Edit an Image with Python,https://www.vice.com/en_us/article/qkjbkw/hack-this-edit-an-image-with-python
Michael Byrne,"On its face, SQL is straight up dullsville: An ALL-CAPs chore for managing databases and only for managing databases. Specificity doesn't help much: SQL is a set of statements for manipulating and querying a certain category of database known as a relational database. Hrm. Deeper still: SQL is capable of representing bafflingly complex operations on many data tables that are simultaneously featuring many intersecting, intricate relationships among many millions of entries. It does this with simple language and intuitive forms. SQL makes much of our data-based world possible. Nonetheless, among those outside the world of relational databases, it's power is almost certain to be understated. Before describing SQL, it's worth asking if SQL is a programming language at all. It's a reasonably common question and one that gets at the heart of what SQL even is. The answer is in the name. SQL stands for ""structured query language."" So, we can at least say that it is a type of language, albeit one that is wildly different in syntax and behavior than a more traditional language like Java or C++. Which is fine because you would never use SQL to do traditional programming language tasks, though common contemporary SQL variations have been shown to be Turing complete , e.g. able to do all of the tasks of any other programming language. So, what even is SQL? SELECT FROM WHERE SQL is considered to be a fourth-generation programming language. This is in contrast to third-generation languages like Java and C++, with the difference being in abstraction and human-readability. (The generational distinction among languages is kind of esoteric, so don't stress it too hard.) Human-readability is in the eye of the reader, of course, but at a basic level, yes, SQL kinda/sorta looks like words that would come from the mouth area of a human. The main statements employed by SQL are SELECT, FROM, and WHERE, though there are several more. (They don't have to be in all-caps for the SQL engine to process them, but that's the convention.) These are just ways of telling the SQL engine what data we want to extract from a table or tables. For instance, if we wanted to get all of the names in some table called ""people"" we could just write something like this, assuming ""names"" is the name of one of the table's columns: SELECT names FROM people If we wanted to specify some condition that would give us only a smaller subset of those names, we could use the WHERE clause like this, assuming ""age"" is another column: SELECT names FROM people WHERE age > 21 What we'll get in return from these statements are really new tables. Both of the examples above will consist of only names. We could add another column to the new table, the result of our query, by doing something like this: SELECT names, age FROM people WHERE age > 21 We'll get the same names as in the second table, but we'll also get a second column showing corresponding ages. This seems simple and, honestly, a lot of SQL operations are this simple, but two main features can blow the complexity of our queries into the stratosphere: nested queries and JOINs. Queries of queries of queries As the results of our queries are really just more tables, we can do more queries on the tables we just created. We can do this more or less forever, linking tables to tables to tables to tables. Just to be clear, a single nested subquery would look like this: SELECT names FROM people WHERE name NOT IN (SELECT names FROM badPeople) So, that one would get only the names of people that are not in this other table badPeople. You can keep nesting from there until you run out of tables in the whole universe and then you could subquery your original table and start again, though anyone that has to work with your code will hate you after the first couple. The mystique of the JOIN There's a deep, almost philosophical problem at the heart of databases, which is how to handle relationships among different data entities. Some attribute of one table might be an attribute in another and those attributes might be unique identifiers of something, aka keys. (A key is some attribute that will be matched with one and only one entity.) So, in situations like this, we wind up with the problem of deleting or altering data in one table and then having to do the same to some other table, possibly resulting in a cascade of data alterations across an entire database. This is done according to some predetermined rules, like, yeah, OK, please nuke the other data too. Or don't. Or disassociate the other data with the data being altered or deleted. Sheesh. You wind up with eye-crossing webs of dependencies and no real easy way to unravel it all. Entities have unique attributes, which might also be unique attributes of other entities! Ahhh! But there's a reason these dependencies are allowed to exist, which is their power to describe relationships between data, even exceedingly far flung data. The command that SQL provides for following these paths is called the JOIN. Giving a code example of a JOIN would, I think, just make this more confusing, but the idea is that it's possible to bridge between tables, sometimes a whole lot of them, by matching unique identifiers, table by table. By following these JOIN'd bridges of unique identifiers it becomes possible to describe some way distant and obscure relationships. JOINS can get really bonkers really fast. You can imagine the result of a bunch of JOIN operations as a fucked up, kaleidoscopic Venn diagram of overlapping relationships. Relational algebra SQL has its roots in set theory and pure mathematics. The concepts involved in relational databases extend well beyond JOINs and SELECTs and computers and computer memory: entities, attributes, relationships. In 1970 , a computer scientist at IBM named Edward Codd decided to formalize this stuff into a new algebra—a novel system of symbols and symbol manipulations meant to organize data not as tangled webs of hierarchies, but as simple tables. Rows and columns. In a paper describing the new system, Codd offered a sort of manifesto: Future users of large data banks must be protected from having to know how the data is organized in the machine (the internal representation). A prompting service which supplies such information is not a satisfactory solution. Activities of users at terminals and most application programs should remain unaffected when the internal representation of data is changed and even when some aspects of the external representation are changed. Changes in data representation will often be needed as a result of changes in query, update, and report traffic and natural growth in the types of stored information. The data structures of computing—arrays and linked lists and binary trees etc.—are pretty great for doing stuff with data, but not so much for organizing it. This is more or less what's offered by a navigational database, which was the big organizational scheme of the pre-SQL era and one that still exists (for some good reasons). A user or programmer entered a database at a certain point and then sought out data as though they were traveling along a map, using paths and propositions like ""next"", ""previous"", ""first"", ""last"", ""up"", ""down"", etc. The progenitor of the navigational database paradigm was Charles Bachmann, who in 1973 offered the following in a now-famous lecture : Copernicus completely reoriented our view of astronomical phenomena when he suggested that the earth revolves about the sun. There is a growing feeling that data processing people would benefit if they were to accept a radically new point of view, one that would liberate the application programmer's thinking from the centralism of core storage and allow him the freedom to act as a navigator within a database. To do this, he must first learn the various navigational skills; then he must learn the ""rules of the road"" to avoid conflict with other programmers as they jointly navigate the database information space. ""This reorientation will cause as much anguish among programmers as the heliocentric theory did among ancient astronomers and theologians,"" Bachman promised. That same year, after mostly sitting on the idea since its 1970 unveiling, IBM decided to develop the relational database idea. Codd wasn't invited, however, and the SQL database system would be built without him, originally as the SEQUEL Structured English QUery Language. One of its early designers and advocates was the computer scientist Don Chamberlin, who offered this praise : ...since I'd been studying CODASYL (the language used to query navigational databases), I could imagine how those queries would have been represented in CODASYL by programs that were five pages long, that would navigate through this labyrinth of pointers and stuff. Codd would sort of write them down as one-liners. ... They weren't complicated at all. I said, 'Wow.' This was kind of a conversion experience for me. I understood what the relational thing was about after that. IBM finally released the first version, SQL/DS, in 1981. The much more successful DB2 version was released in 1983. Relational Software, Inc., which would go on to become the Oracle Corporation, was at the same time releasing its own variant of the relational database model (the object-relational database), with version 3 of the Oracle database hitting the market in 1983. Getting started Since the late 80s SQL has been standardized and developed jointly by American National Standards Institute (ANSI) and of the International Organization for Standardization (ISO). There are many open-source implementations, including SQLite, PostgreSQL, and MySQL. There are further sub-variants depending on features and interfaces and storage engines. It's kind of a mess. Further complicating things is the fact that SQL is very often used within another language, like PHP, which is what SQL is probably most often associated with. In this context, data can be queried and manipulated within the a regular old computer program—most likely a server. Even though I did SQL in a databases class in school, I did Khan Academy's course as well because it's really good and hands-on. You wind up spending much more time actually screwing around with databases than you do listening to lectures, which is a really good thing for SQL in particular. Read more Know Your Language .",,Vice,There's more than you think to the least cool language ever.,Know Your Language: Gazing Into the Relational Database Void with SQL,https://www.vice.com/en_us/article/aekve8/know-your-language-gazing-into-the-relational-database-void-with-sql
Michael Byrne,"What's interesting here is that this is basically how you would achieve the same effects in the year 2016 with a fashionable graphics-friendly language/tool like Processing or P5 , save for the data cards, at least. Computer animation is after all just changing or redrawing something within a looping section of code.  The animation in the video was created using Bell Labs' BEFLIX (""Bell Flicks""), which was created by computer graphics pioneer Ken Knowlton and is usually considered to be the first computer animation language. It ran on the IBM 7090, a big-ass early supercomputer that could be had for the low, low price of $2.9 million ($23.2 million in today's dollars). As explained in the early-days textbook  Computer Animation: Theory and Practice , BELFIX was based on the manipulation of 184 x 252 pixel frames. Each pixel had a color depth of three bits, e.g. contained three bits of information. Those three bits were capable of presenting eight different shades of grey. The language featured a handful of primitive operations, including reading/writing pixels, filling shapes, perspective projection , and a few others. Because of its limited math capabilities, Knowlton eventually combined his language with FORTRAN as FORTRAN IV BEFLIX. A few years later, he released EXPLOR, a funky art-minded language for creating patterns based on randomization. The above image is the product of an EXPLOR program. EXPLOR is considered by esoteric programming language enthusiasts to be an early entry into the esolang universe, but it was a tool for weirdness as much as it was weird in itself: ""Mutations"" is one of many incredible video art pieces done by Bell Labs resident artist Lillian Schwartz with help from EXPLOR:  Gaze deeply.",,Vice,Enjoy Bell Labs ' short tutorial 'Programming of Computer Animation.',Coding Computer Animation in 1964 Looked a Lot Like It Does in 2016,https://www.vice.com/en_us/article/53dg35/here-is-bell-labs-1964-computer-animation-tutorial-2
Michael Byrne,"The internet as we see it today is still a fairly recent phenomenon. In 2016, even the most pedestrian, phone book-looking website aspires to be something more than merely an ""entry"" within the grand information catalog of the web. It aspires to be an application. A program. An interactive experience. It wasn't always like this. Practically speaking, what we see today comes mostly thanks to the introduction of scripting languages to an otherwise (relatively) static web. Whereas HTML, as a markup language, tells a web browser how to make a website look a certain way via a limited set of abstract building materials, it doesn't really have the capabilities for telling it how to make a website do things. That's just what it is to be a markup language. Doing things would be the domain of scripting languages, like JavaScript and PHP, and the modern servers and browsers capable of interpreting them. Now, websites naturally become programs unto themselves. This is a recent capability/movement, but the idea of an interactive web page is decades old. One of the concept's defining moments came in 1995 with the introduction of the Java programming language, and, with it, the Java Applet. This is a small program-like unit (an app, really) that can be embedded in a web page, offering capabilities for any number of program-like things: basic interactivity (e.g. rollover effects), small games, visualizations, and other realizations made possible by the Java programming language. If you can remember the web of 10 years ago, you can almost certainly remember being nagged by some webpage to update your Java installation to run an Applet. Java's corporate parent Oracle announced this week that it's finally killing the Applet. It makes sense. There's no longer any real need for the technology, and, as such, browsers have stopped supporting them. It'd be a pretty goofy thing to continue dedicating resources to. For the better part of a decade, however, the Applet was the internet's outlet for creative interactivity. HotJava The heart and soul of the Java applet is a clever fudge. The browsers of yore weren't capable of interpreting programming languages, but browsers are also themselves just programs executing with the same contexts as other programs on a computer, including the Java Virtual Machine (JVM). The JVM is what enables the Java language, functioning as an intermediate layer between the low-level guts of a computer and its high-level programming. That is, a program in Java runs within the confines of another program, the JVM, which emulates actual computer architecture. This is somewhat analogous to how JavaScript, which is mostly unrelated to Java beyond the name, conventionally runs within a web browser. So, if a user has both a browser running and a JVM running, maybe the two could join forces. That's the basic idea of an Applet. Applet code is stored on a server with the rest of its parent web page, but when it gets to an actual browser, a separate process from the browser is created on the user's machine. Behind the scenes, the new Applet process runs on the JVM like any other Java program while the browser renders the rest of the webpage as usual. The success of Java itself would be in slow, deliberate entrenchment in the oatmeal world of enterprise software. In 1995, this was pretty exciting stuff. The newborn Java language, the creation of Sun Microsystems, was immediately tasked with taking on the web. 1997 saw the release of the HotJava browser , the first to accommodate Java Applets. It was to be Sun's big demonstration of the technology and, with it, the potential of an interactive web. ""HotJava Browser 1.0 is a powerful tool that allows developers to quickly create their own style in the user interface, or create network applications that use URLs and HTTP to transport and locate information,"" a Sun press release announced . ""For example, HotJava Browser can be used to develop customized information kiosks that businesses could use to provide information to their customers. The kiosks would display HTML-based pages and customers would use HotJava Browser to browse the information. The kiosks' graphical user interface would be completely tailored to reflect the branding of the business. HotJava Browser is completely customizable and lets any business create a browser-based user environment that is in its own image."" Applet by Lookang/Fu-Kwun Hwang. Image: Wikimedia  Hype ensued. With that hype came JavaScript, a language whipped up by Netscape engineer Brendan Eich in 10 days in 1995 to be featured in the forthcoming Navigator 2.0. The JavaScript name usually taken to be a quick cash-in on the Java buzz of the time. JavaScript would eventually crush the Java Applet, but you probably wouldn't know it at the time. JavaScript promised much of the same web functionality as Java, but Netscape (and Microsoft) were busily implementing their own JVMs, thus enabling Java Applets to run on non-Sun mainstream browsers. Flashiness would wind up being JavaScript's thing. The success of Java itself would be in slow, deliberate entrenchment in the oatmeal world of enterprise software, e.g. the long-lasting collections of programs that together serve the needs of whole organizations. The Java language promised ""write once, run anywhere"" functionality—that is, code written for the JVM would run the same regardless of the underlying machine or operating system. Programs written for the JVM, which include several other non-Java languages that run within the same virtual environment, are said to be platform-independent. A 1999 white-paper in the industry journal HP Professional extolled the virtues of the new Java way: ""Software companies spend a fortune building separate code for Windows NT, Windows 3.1, Linux, MacOS, Solaris, AIX, etc. This is a major headache for every IT shop. Java has achieved the greatest success in letting developers write Java code once for all platforms. When applications are written in Java, the resulting bytecode will run identically on any JVM."" ""Java is being widely used from developing browser applets to serious enterprise applications,"" the paper continued. ""It is being used to reengineer global reservation systems, to speed ERP implementation and to improve call centers. What started as an improved programming language is now an architecture that is reshaping our enterprises and the World Wide Web."" JavaScript returns As Java dug in in the early 00s, Applets thrived. Put simply, they could do cool things. A quick survey of academic and industry journals from 2000 to 2005 finds Applets powering early camera-phones and covert CCTV set-ups, visualizing protein sequences , processing and analyzing shipping data, enabling early online learning tools and ""3d-enabled cyber workspaces,"" and more. This is all in addition to what Applets could do within the context of everyday web interaction. It's an easy technology to hate on in 2016—inelegant, awkward, slow—but Applets spent a cool decade or more predicting the internet of applications we now have before us. So, what happened? When's the last time a Java logo nagged you to install or probably reinstall Java to run some widget? It's probably been a while. For one thing, JavaScript came back at its namesake hard. The term for what JS does is ""client-side scripting,"" which you may already be familiar with. A script within the context of a web page can be taken to mean just some instructions that manipulate the HTML of a page in a prescribed way, perhaps in response to a user's input, like a click or a mouse movement. The resulting effect to the ecosystem is that Java applets are still present in the platform, and are a major contributor to security problems. So, there is server-side scripting, which is what PHP does, and there is client-side scripting, which is what JavaScript does. The prior means that a script is running on server somewhere, and is likely meant to interact with a database in some way, while the latter means the script is running on your computer via a browser. When we usually think of ""cool stuff"" happening on a web page, we're usually thinking of stuff happening on the client side in terms of some real-time interaction. JavaScript is able to do its thing because modern browsers are built with JavaScript engines. These engines behave in a way similar to the JVM described above, but, rather than mimic a full-on computer architecture, the engine's job is to use the instructions provided in a script to change some HTML in interesting and useful ways. JavaScript didn't immediately sink the Java Applet, despite web browsers now being able to do cool stuff without having to farm it out to the JVM. This is because for many years the JVM still offered performance advantages. It makes sense: the Applet has access to the full resources of a simulated computer, while, until recently, JavaScript code was left to dork around with HTML via the relatively limited capabilities of a browser. One is essentially a program, while the other fakes being a program. ""Sand Traveler"" generative art Applet. Image: J Tarbell  V8 Everything changed in 2011. Thank Google for this one. Whereas JavaScript had conventionally been run within the confines of a browser, the release of Google's V8 JavaScript engine meant that JS code would now be compiled by Chrome and then run on the user's actual machine. In Google's words : ""V8 compiles JavaScript source code directly into machine code when it is first executed. There are no intermediate byte codes, no interpreter."" That's pretty cool. And it also one ups the Java Applet. No more JVM, just the whirr of optimized machine code running on hardware. In Java: The Legend , Benjamin Evans summarizes the current state of the Java Applet as such: Applets are a technology that are very much of their time, and they have not aged well. The technology proved to be very difficult to evolve, and so applets have not been considered to be a modern development platform for many years now. However, they doggedly persist due to some large early adopters being very resistant to change. The resulting effect to the ecosystem is that Java applets are still present in the platform, and are a major contributor to security problems.  That was written before the official demise of the Applet in 2015. The technology was already practically deprecated, even if it persists in legacy systems. Those systems will have until 2017 to migrate Applets to other platforms and frameworks, though they should probably get on that, like, yesterday. The impetus for Java dumping the Applet is chiefly that modern browsers had already dumped it or were in the process of dumping it along with other plug-in based technologies, such as Flash and Silverlight. This one is from Oracle, the current corporate parent of Java : Java's rapid rise to fame 20 years ago began with a tumbling duke applet running in the HotJava browser, long before Microsoft Internet Explorer, Mozilla Firefox or Google Chrome were released. Applets allowed richer development functionality through a browser plugin at a time when browser capabilities were very limited, and provided centralized distribution of applications without requiring users to install or update applications locally. The Netscape Navigator browser went on to popularize a standards based plug-in model with the Netscape Plugin API (NPAPI), which was in turn adopted by many other browsers, allowing plugins to extend the capabilities of browsers to provide cross-platform and cross-browser functionality.  As Java evolved to become one of the leading mainstream development platforms, so did the applet's hosts – the web browsers. The rise of web usage on mobile device browsers, typically without support for plugins, increasingly led browser makers to want to restrict and remove standards based plugin support from their products, as they tried to unify the set of features available across desktop and mobile versions.  To understand the legacy of the Applet, a good place to look is the massively popular Processing programming language , which has become the language of choice for creative coding—that is, a syntax for art. Processing is really just a Java wrapper language, and the decision to use Java instead of some other language like C++ traces back to the Applet. ""In one sense, Processing arose as a response to practical problems,"" writes John Maeda, an artist and computer scientist that advised the duo behind the Processing language at MIT, in a history of the language. ""When Java first came out, it offered minimal support for sophisticated graphics processing. Drawing a line and stuff like that was possible, of course. But it couldn't do transparency or 3D, and you were almost guaranteed to see something different on a Windows computer and a Mac; it was incredibly cumbersome to do anything that was both sophisticated and cross-platform."" ""So [Ben] Fry,"" he continues, ""who grew up hacking low-level graphics code as a kind of hobby, built from scratch a rendering engine that could make a graphically rendered scene appear the same in a Windows or a Mac environment. It was not just any renderer–it borrowed the best elements of Postscript, OpenGL, and ideas cultivated at the MIT Media Lab in the late Muriel ­Cooper's Visible Language Workshop."" In a sense then, the Applet is thriving. What it originally intended to do—make web pages into living and breathing computer programs—has reached its realization via JavaScript, while the Applet's creative aspirations live on through Processing. Some two decades after its introduction, the web has indeed become a realm of web pages doing every imaginable cool thing.",,Vice,"It's easy technology to hate, but the Applet predicted an age of web applications.",The Rise and Fall of the Java Applet: Creative Coding’s Awkward Little Square,https://www.vice.com/en_us/article/8q8n3k/a-brief-history-of-the-java-applet
Michael Byrne,"I have five goddamn email accounts: one personal, two school, and two work. There's even a sixth that I sometimes use for sketchy sign-ups. A significant portion of my life consists of quickly assessing and classifying emails, deleting emails, starring emails, and even reading and replying to emails. I'd give you a breakdown of how much time daily is spent on these tasks, but, to be honest, I don't even want to know. I've joked about hiring an assistant just to manage my email, which makes me wonder what instructions I would give them: If such and such is true about an email, then take this action, or, if this other thing is true, do something different. It would be a set of rules, ultimately, that they would be applying to my email accounts. And when I see the words ""set of rules,"" I immediately think of writing computer programs. Scanning through my emails now, it's clear that I could reasonably implement my assistant in computer code. To do this, however, I would need some way of connecting my actual email accounts to my email-reading program. This turns out to be fairly simple to do. Email is in real-life just a series of text-based protocols for moving information across networks; it's like the rest of the internet, in other words. Layers of protocols. Everything else is just an interface. Even if you have no interest in creating an email-sorting bot, handling email as raw data is just an interesting thing to do. We'll do it in Python here mainly because Python is really good at making stuff like this easy and intuitive, even for non-programmers. The instructions below are based in part on the email automation chapter in Al Sweigart's Automate the Boring Stuff with Python , which I highly recommend as both a programming introduction and a cookbook for doing all kinds of fun stuff with automation. It can be accessed for free under a Creative Commons license at automatetheboringstuff.com . Before starting, you'll need to download Python if you don't have it installed already. You'll also need a basic text editor, which can be anything, but most people are going to tell you to use Sublime Text. I'd agree with them, especially if you're interested in doing more coding. It can be downloaded here. ST is not free, but can be evaluated indefinitely with the occasional nagware message. It's worth the money, however. Python sort of has two faces. One is the Python interpreter, which allows users to tap lines or snippets of Python code into a command line interface, which will execute those lines as Python commands as they're entered (for example: print(""Hello, World!"") ). The second is Python scripting. This is basically where you take a bunch of Python commands and bundle them together into one or more files that can be invoked all at once from the interpreter. These files essentially act as programs—sometimes they're tiny, simple programs (as below), but they can also wind up being huge projects. If you've just downloaded Python, you should do two more things first. One: spend 10 minutes doing this ""Hello, World"" Python for non-programmers tutorial . Two: spend another five minutes doing this tutorial on using Python modules . Python being Python, installing and importing language extensions is criminally easy. OK! Let's get started. 0) Email protocols 101 There are three main email protocols. The old-school one is called POP (Post Office Protocol). The basic idea with POP is that a software email client (not a browser) connects to a remote server and downloads emails onto the user's actual computer so that they're available locally. This was a good idea when internet connections were more sparse and being offline was a common occurrence, but now it's largely obsolete. IMAP (Internet Message Access Protocol) is the current Internet standard for accessing email. It's much faster and more in line with how the internet is actually used now, allowing for multiple parties to connect to the same inbox and for those parties to remain connected to the server through their entire email session, start to finish. Web browsers access email via an extra layer of HTTP, but the underlying protocols are the same. Crucially, POP (POP3 is the current version) and IMAP both are used for getting emails from a server. To send emails we need a different protocol, called the Simple Mail Transfer Protocol (SMTP), because we can't just send an email to a recipient. We need to send it to some server, from which that other user can download it using IMAP or POP3. 1) Import the Python SMTP module To get started, we first need to have the right Python module in place. Smtplib comes with Python so we don't need to worry about installing it or anything. So, just tap this into your interpreter. import smtplib For an overview of the package (and to verify that it's actually imported), try using the help function: help(smtplib) That will work with any package, btw. 2) Connect to the email server First thing is to create an smtplib object, which we can view as a sort of portal through which we access our connection and the various capabilities provided by the smtplib package for doing things with that connection. We're basically creating a server. For our purposes, the function that returns our object needs two parameters or arguments. The first is a domain name, which is usually just your regular email with ""smtp"" in front of it, as below. The second is a port number, which is where on the email server we'll be connecting to. The port number will almost always be 587, which corresponds to the TLS encryption standard. ( Very rarely, a service might use 465, but you almost certainly don't use that service.) Anyhow, so it will look like this: smtpObj = smtplib.SMTP('smtp.gmail.com', 587) The object ""smtpObj"" has the object type ""SMTP."" You can verify this by entering only the object name smtpObj into the interpreter and hitting enter. You'll get back a memory address and object type, assuming you didn't mess up the above step. The variable name ""smtpObj"" could be anything of your choosing that's a legit variable name in Python. 3) Start encryption The next step in establishing a connection is to tell the SMTP object that we need to encrypt our message. So, enter this line into the interpreter (and press enter): smtpObj.starttls() What we did is send a message to Gmail telling it that that we want our connection to be encrypted using the TLS (Transport Layer Security) protocol, which is the current standard for internet communications. TLS isn't in itself an encryption scheme, but rather it specifies that an acceptable encryption scheme must be used or no deal. You'll get a confirmation message back: 2.0.0 Ready to start TLS Note what happens when you try and skip this step and just go to the login step below. Errors! Gmail won't let you do it because Gmail uses encryption via the HTTPS protocol. What is the HTTPS protocol? It's simply regular old HTTP but with the addition of the TLS protocol on top . 4) Login Not much to it: smtpObj.login('justkiddingboat@gmail.com','just123kidding') That is indeed that is your/my email password just floating in the breeze, which is potentially a very bad thing. It's not such an issue here in my Python interpreter on my computer, but if you were actually automating email using a Python script, you should be very careful about where you publish/upload that script. 5) Send a message Imagine here that I forgot this step and you had to figure out how to actually send the email on your own. You could Google it, of course, but with the Python interpreter or really any other shell-like environment , you could also just ask the interpreter itself. Just enter this in: help(smtpObj) Not too far down you'll see the method you want, or that you would (correctly) assume that you want. It will even give you an example: Note that in the above example, the login stuff is absent. This is because the connection was made only to the user's own computer (""localhost"") rather than a distant email server. For my connection, let's do this. smtpObj.sendmail(""justkiddingboat@gmail.com"",""michael.byrne@vice.com"",""go to bed!"") Did it work? Sure did! 6) Close the connection Just enter in smtpObj.quit() and you're golden. This is obviously just a taste of what you can do with email with Python. This is only sending messages, for one thing. I'd recommend continuing on in Sweigart's book for learning how to deal with an email inbox using IMAP. Spoiler alert: There's still not that much to it. You could be running your whole email show with Python scripts by morning. You can probably also see how one could be very, very annoying (or worse) when applying automation to email, so let's please avoid the dark side.",,Vice,Automating your email doesn't get much easier.,Hack This: How to Send an Email from Python,https://www.vice.com/en_us/article/pgkbn9/how-to-send-an-email-from-a-python-script
Kaleigh Rogers,"There are a lot of bots out in the Twitterverse now, but only one bot has "" the best words "" and aims to make America great again. Meet Deep Drumpf, a Twitter bot created by a researcher at the Massachusetts Institute of Technology that pulls text from the speeches and debate transcripts of Republican presidential hopeful Donald Trump. (The name refers to the ancestral surname of the Trump clan, unearthed last week by Jon Oliver.) We have competence. Our people don't need anybody. I have smart people. DeepDrumpf March 3, 2016  Bradley Hayes, a postdoc at MIT's Computer Science and Artificial Intelligence Lab, created the bot this week using an AI technique called deep learning . The bot creates the tweets randomly one letter at a time but has learned patterns by analyzing Trump's speech. Hayes was inspired by a similar model that can spit out Shakespearean-style text, he said in a press release . ""Trump's language tends to be more simplistic, so I figured that, as a modeling problem, he would be the most manageable candidate to study,"" Hayes said in the release. In fact, Trump's language is categorically simple: the Boston Globe  recently ran candidate transcripts through an algorithm that determined Trump's speech could be understood by a fourth-grader. Ted Cruz, by comparison, generally speaks at just below a ninth-grade level. ""The algorithm essentially learns an underlying structure from all the data it gets, and then comes up with different combinations of the data that reflect the structure that it was taught,"" Hayes said.  It can even interact with Trump by pulling words from a recent tweet and replying: DeepDrumpf March 3, 2016  Like many bots, a lot of the tweets wind up in a kind of uncanny zone of almost but not quite making sense. But then there are a few gems, which almost sounds pulled from the lips of the real estate mogul himself: I'm what ISIS doesn't need. DeepDrumpf March 3, 2016  Hayes said he hopes to create more bots for other candidates, with his goal being to learn through experimentation. ""Much of my actual robotics research deals with these types of modeling techniques,"" Hayes said. ""I thought this would be a good way to learn more about some of the concepts, and have a little bit of fun in the process.""",,Vice,"""I thought this would be a good way to learn ... and have a little bit of fun in the process.""",This Donald Trump Twitter Bot Is Surprisingly Convincing,https://www.vice.com/en_us/article/kb7bez/this-donald-trump-twitter-bot-is-surprisingly-convincing-gop-republican-election-tweets
Michael Byrne,"But of course there's a programming language written entirely in emoji. After all, there's a language whose syntax consists entirely of Shakespearian prose , a language that channels the rhetoric of Donald Trump , and yet another whose syntax is limited to instructions for performing card tricks —Emojicode just seems like a given within the dense forest of novelty programming languages. That said, Emojicode, which first appeared as a Github project a couple of months ago, would seem to aspire to be something more than your average esolang . For one thing, while many if not most novelty languages are ""compiled"" simply by translating them into another intermediate language, Emojicode features its very own compiler and execution engine. So, like Java, an Emojicode program is first reduced to Emojicode bytecode which then is run within its own virtualized environment. Emojicode even features its own package management system, allowing developers to create new useful tools for using the language. Packages that come currently bundled with Emojicode include one for navigating file systems and another for interacting with SQL databases. New packages add new emoji, which add new functionalities. Emojicode programs aren't written entirely using emoji. Variable names, for example, need to be written using normal text. But types (integers, character strings, etc.), classes (modular units of related code), and methods (modular units that perform a specific function) are all referred to by emoji. Other examples of emoji-based functionality include a tiny ice cream cone to ""freeze"" the value held by a variable (what would be a constant in another language), a tiny bearded face to demarcate comments (informational sections of code that aren't compiled or executed), and thumbs up and thumbs down emoji for the boolean values ""true"" and ""false."" Conditional statements are represented by orange slices. Not sure what's up with that one. Hello, World in Emojicode: Here's the installation guide , which will also point you to downloads of the Emojicode Software Development Kits (SDKs).",,Vice,Maybe we'll all be coding with emoji in a few years.,Emojicode Is More Than Your Average Novelty Programming Language,https://www.vice.com/en_us/article/vv7dwa/emojiscript-is-more-than-your-average-novelty-programming-language
Samantha Cole,"In the earliest days of home computing, the much-hyped feature we now call “dark mode” was the default. Cathode ray tube (CRT) monitors, also known as monochrome monitors, got their Matrix -esque green-on-black look from the phosphor-coated inside of the screens.  It’s been a long time since manufacturers stopped painting monitors with phosphor, but some people still swear by dark themes for everything. The Verge keeps a running list of apps and platforms that offer a dark mode for fans, and Apple announced a dark mode feature for iOS 13 at the company's annual WWDC conference this week.  ""It’s thoughtfully designed to make every element on the screen easier on your eyes,"" Apple claimed in its announcement of the feature. But there’s a wide array of human experience and physiology that keeps that from being true for everybody. For some people with astigmatisms (a common condition that most people are born with , to some degree), dark mode can be worse on their eyes, and, impairments that require it aside, the science showing that dark mode is any easier on the eyes than normal mode is still up for debate.  For a lot of users with vision impairments or disorders, dark mode offers a better experience and allows them to use technology they otherwise may not have been able to. Flipping the standard black-text-on-white color scheme has long been an accessibility option for Apple computers. But there’s little evidence suggesting that using dark mode during the day, in a brightly-lit office or on your phone, is better than the alternative for the vast majority of people. Apple's announcement page for the dark mode feature.  That dark mode is healthier for your eyes has become a bit of folk wisdom. Wired called dark themes ""an eye-friendly alternative to the traditional blindingly bright user interfaces,"" and Popular Science called them a ""comforting alternative to the blinding white"" of most websites. Experts say that's not proven, however.  ""I do not think dark mode affects eye health in any way given the data that is out there in the literature,"" Euna Koo, an ophthalmologist at the Stanford Byers Eye Institute, told CNN Business . ""The duration of use is likely much more important than the mode or the intensity of the brightness of the device when it comes to the effect of this dark mode on eye fatigue and potentially eye health.""  When we’re talking about the potentially damaging effects of white-background screens, we’re usually really talking about “blue light,” part of the light spectrum made of short, high-energy wavelengths. A 2018 study published in BMJ Open Ophthalmology notes that blue light could be a factor in eye tiredness, but cites dry eyes from not blinking for long periods as a more serious cause of eye strain, as well as too-small fonts, and conditions like uncorrected astigmatism. Read more: America's Television Graveyards  A 2018 study by researchers at the University of Toledo claimed that blue light could contribute to macular degeneration, and plenty of news  outlets ran with that claim, writing that our phones are blinding us. But the American Academy of Ophthalmology denounced that study, because the researchers didn't study real eye mechanisms, or cells taken from eyes.  Studies have shown that blue light from screens at night can mess with circadian rhythms and make it harder to get quality sleep , but whether blue light from our phones is causing eye strain is still up for debate.  There is a physiological reason why black text on a white screen is better for many people. I have severe astigmatism, meaning my corneas are irregularly-shaped, causing my vision to be a little blurry all of the time, no matter how strong my contact lenses are. Most people have astigmatism to some degree, but it's usually unnoticeable, according to the American Optometric Association .  But when an astigmatism is bad enough to impair vision, light text on dark backgrounds aggravates the condition, making text harder to read—and therefore making people squint more to try to correct it. As Gizmodo wrote in 2014, citing research by the Sensory Perception and Interaction Research Group, at University of British Columbia , white backgrounds act as a ""crutch"" for astigmatic eyes:  People with astigmatism (approximately 50% of the population) find it harder to read white text on black than black text on white. Part of this has to do with light levels: with a bright display (white background) the iris closes a bit more, decreasing the effect of the ""deformed"" lens; with a dark display (black background) the iris opens to receive more light and the deformation of the lens creates a much fuzzier focus at the eye.  My own very-astigmatic eyes are exhausted by dark mode, but for many others, dark themes are an accessibility benefit. White backgrounds emphasize floaters , those tiny spots of fibers that appear in some people's vision. People with disorders like photophobia or keratoconus , conditions that cause high sensitivity to light, might read more easily with dark themes.  Dark themes are also commonly seen as less distracting, especially for platforms like Spotify and Steam: The designers want the outside world to fall away, so you spend more time in the app. That may also be why programmers use dark backgrounds for long coding sessions, to focus more easily on each line.  On one point, I must concede: Dark mode is good for battery life on the latest iPhones. According to iFixit , on OLED screens—which light pixels individually, meaning turned-off pixels don't use any power—the swaths of black areas in dark themes conserve battery. LED screens light all pixels from the edges of the screen and use the same amount of power whether the screen is black or white. The newest iPhones have OLED screens, but any models before the iPhone X uses a LCD screen.  In the end, more display options are better, and people should use whichever lighting theme they want. It’s great that dark mode is coming to iOS for people who it helps, but there’s simply not evidence to make the blanket claim that dark mode is “easier on your eyes.”  If you enjoy pretending you're a hacker, or just feel more comfortable using darker themes, use whatever screen style you want. Ultimately, it's a matter of personal preference and accessibility. But in most cases, what would really help your eye strain is to not stare at a screen all day.",,Vice,"Apple’s new dark mode for iOS 13 can save battery life, but it won’t save everyone's eyes from screen strain.",Dark Mode Isn’t ‘Easier on the Eyes’ for Everybody,https://www.vice.com/en_us/article/ywyqxw/apple-dark-mode-eye-strain-battery-life
Samantha Cole,"A research team consisting of machine learning experts from Stanford and Princeton Universities, as well as the Max Planck Institute for Informatics and Adobe, created a method for literally putting words in people's mouths, using just text inputs and a source video.  By analyzing the speech patterns in a video of one person speaking into a camera, they're able to swap whole phrases for completely different ones. The result is pretty realistic.  Their project, "" Text-based Editing of Talking-head Video, "" was published online this week, as a whitepaper to be presented at the SIGGRAPH conference in July.  In the video demonstration, a woman saying ""one hundred and ninety one point four"" is made to say ""one hundred and eighty two point two"" instead. A man reciting the Apocalypse Now quote ""I love the smell of napalm in the morning,"" is made to seem like he's saying ""I love the smell of french toast in the morning.""   To do this, the researchers identify and isolate the phonemes —tiny units of sound within speech—in the spoken phrase. They also create a 3D face model that tracks how the face and lips move with the words. Then, they find the visemes—sounds that look the same in how the lips move, such as ""v"" and ""f""—and combine parts from the face model to match the new phrases. Someone saying ""viper"" and ""-ox"" in different parts of a video can be sliced up into a face model saying the word ""fox.""  To make the edited voice match the rest of the person's speech, the researchers use VoCo, Adobe's voice-editing software which the company unveiled in 2016.   If you watch closely and know what to look for (slightly stiff mouth movements, small inconsistencies in the motion of the mouth) you can tell where the new words are inserted. But without knowing where the edits occur, your eyes easily gloss over it.  In terms of fact versus reality, this might be the most startling part of this processing technique. It's a small change, isolating the mouth and jawline and inserting only a few words within a longer sentence. In context, your eyes see what you want to see, filling in the rough spots to look seamless.  To test how convincing their edited videos were, the researchers conducted a web-based study of 138 participants which them to view real and edited videos, and rate their response to the phrase “this video clip looks real to me"" on a five-point scale. More than half of the participants—around 60 percent—rated the edited video clips as real. The unedited, ""ground truth"" videos were only rated to be ""real"" around 80 percent of the time.  Since the beginning of algorithmically-altered face swap videos, or deepfakes , as a mainstream phenomenon, scholars have been sounding the alarms about what realistic, manipulated videos will do to society's concepts of truth and reality. This small user study shows that those concerns are valid, and already coming to life—when everything is fake, nothing is real.  People are already easily duped when it comes to videos that confirm what they want to believe, such as we saw with the edited video of Nancy Pelosi . But when they're put on alert, confirmation bias swings the other direction, to total disbelief of what's actually real.  In their whitepaper, the researchers on this text-swapping method commit several paragraphs to the ethical considerations of such technology. ""The availability of such technology—at a quality that some might find indistinguishable from source material—also raises important and valid concerns about the potential for misuse,"" they write. ""We acknowledge that bad actors might use such technologies to falsify personal statements and slander prominent individuals. We are concerned about such deception and misuse.""  They write that they hope bringing these methods to light will raise awareness of video editing techniques, so that people are more skeptical of what they see.",,Vice,"Using text inputs, a team of machine learning experts edited videos of people to realistically form words they never said. ",Researchers Can Make People Say Anything in Videos by Rewriting Their Words,https://www.vice.com/en_us/article/ywyqab/ai-text-editing-talking-head-video-siggraph-2019
Samantha Cole,"Two artists and an advertising company created a deepfake of Facebook founder Mark Zuckerberg saying things he never said, and uploaded it to Instagram.  The video, created by artists Bill Posters and Daniel Howe in partnership with advertising company Canny, shows Mark Zuckerberg sitting at a desk, seemingly giving a sinister speech about Facebook's power. The video is framed with broadcast chyrons that say ""We're increasing transparency on ads,"" to make it look like it's part of a news segment.   (On Tuesday evening, CBS requested that Facebook take down the video, as it displays ""an unauthorized use of the CBSN trademark,” a spokesperson said.) ""Imagine this for a second: One man, with total control of billions of people's stolen data, all their secrets, their lives, their futures,"" Zuckerberg's likeness says, in the video. ""I owe it all to Spectre. Spectre showed me that whoever controls the data, controls the future.""  The original, real video is from a September 2017 address Zuckerberg gave about Russian election interference on Facebook. The caption of the Instagram post says it's created using CannyAI's video dialogue replacement (VDR) technology.  This deepfake of Zuckerberg is one of several made by Canny in collaboration with Posters, including ones of Kim Kardashian and Donald Trump , as part of Spectre , an exhibition that took place as part of the Sheffield Doc Fest in the UK.   “We will treat this content the same way we treat all misinformation on Instagram,"" a spokesperson for Instagram told Motherboard. ""If third-party fact-checkers mark it as false, we will filter it from Instagram’s recommendation surfaces like Explore and hashtag pages.”  Read more:  Inside Facebook’s Struggle to Moderate Two Billion People  Following the viral spread of a manipulated Facebook video of House speaker Nancy Pelosi , Facebook has been forced to take a stance on whether fake or altered images are allowed to stay up on the site. Instead of deleting the video, the company chose to de-prioritize it , so that it appeared less frequently in users' feeds, and placed the video alongside third party fact-checker information.  At the time, Neil Potts, Facebook’s director of public policy, said that if someone posted a manipulated video of Zuckerberg like the one of Pelosi, it would stay up . Now that there's a deepfake of Zuckerberg implying he's in total control of billions of people's stolen data and ready to control the future, on Facebook-owned Instagram, that stance will be put to the test.  Read more:  There's No 'Correct' Way to Moderate the Nancy Pelosi Video  Canny's founders, Omer Ben-Ami and Jonathan Heimann, told special effects blog FXGuide that their work comes after algorithms developed by University of Washington researchers, which turned audio clips of people speaking into realistic videos of people made to look like they're speaking those words. The UW researchers demonstrated this, at the time, using Barack Obama's face. They said they're also inspired by Stanford's Face2Face program , which enabled real-time facial reenactment.  Ben-Ami told Motherboard that to create the fake videos, Canny used a proprietary AI algorithm, trained on 20 to 45 second scenes of the target face for between 12-24 hours. That doesn't seem like much, but we've already seen deepfakes made from as little as one image of a face.  For the Zuckerberg deepfake, Canny engineers arbitrarily clipped a 21-second segment out of the original seven minute video, trained the algorithm on this clip as well as videos of the voice actor speaking, and then reconstructed the frames in Zuckerberg's video to match the facial movements of the voice actor.  The result is fairly realistic—if you leave the video muted. The voice superimposed on the video is clearly not Zuckerberg, but someone attempting an impression. But this deepfake blinks, moves seamlessly, and gestures like Zuckerberg would.  Ben-Ami said that Canny saw this as both an opportunity to educate the public on the uses of AI today, but also to imagine what's next. ""The true potential we see for this tech lies in the ability of creating a photo realistic model of a human being,"" he said. ""For us it is the next step in our digital evolution where eventually each one of us could have a digital copy, a Universal Everlasting human. This will change the way we share and tell stories, remember our loved ones and create content.""  Update Wednesday, June 12, 9:20 a.m.: This piece was updated with comment from CBS.",,Vice,A fake video of Mark Zuckerberg giving a sinister speech about the power of Facebook has been posted to Instagram. The company previously said it would not remove this type of video.,This Deepfake of Mark Zuckerberg Tests Facebook’s Fake Video Policies,https://www.vice.com/en_us/article/ywyxex/deepfake-of-mark-zuckerberg-facebook-fake-video-policy
 ,"Any seasoned graffiti writer will tell you, it's only partially about the love. In reality, beneath every commissioned wall piece on the underside of a Soho streetwear storefront, there's still one part vandal's tendency to wreck shit, one part narcissist's desire to write your name as big as humanly possible. Many will openly admit to the addiction, the rush that comes from seeing your name in lights—even if it means sometimes those lights are flashing red and blue. Maybe more than will admit to the dirty little secret in every veteran's black book: it takes practice. Tons and tons of practice. Combining the urge to get up with the need to up your handstyles is Fat Tag , an app that lets you draw graffiti on your smartphone using photos from your library and an array of different settings, including cap styles, ink flow, and more. We had two vandals and a dev guru explain how, while an app might sound like a fun way to toy around, to not let its simple premise fool you. Fat Tag is a sketching tool by and for active writers, allowing you to not only draw up your plans before you execute them, but to keep yourself occupied in situations where you can't casually fool around with paint cans. ""Honestly, it's a tool for criminals,"" New York writer KATSU, drone graffiti mastermind and one of the app's co-creators, with Theo Watson, tells The Creators Project. ""If you want an app for making pretty drawings, there are hundreds of other apps to download. If you are an actual graffiti vandal you will understand what this app is and how it is designed exactly for you."" It all comes down to the interface. ""[Fat Tag] was designed to keep up with the speed of tagging and the rapid deleting you need to tag and tag and tag and tag,"" KATSU explains. A key component is the sheer pace Fat Tag allows. One tap and you're writing, two taps and you're already onto your next tag. Writers like Fat Tag because it feels real, right down to the uncannily accurate gesture control Watson programmed. ""It's another form of paper to practice styles and concepts,"" Rehab tells The Creators Project. ""I see it as a typography tool or design app like [Adobe] Illustrator. It does help with your personal hand style and flow, and lets you produce concepts faster and more smoothly."" While Rehab doesn't necessarily agree with KATSU that Fat Tag is like ""graffiti methadone""—KATSU also calls it ""a digital nicorette patch,"" and a ""girlfriend mute button""—he believes it's important to ""keep your hands moving with the tools needed to."" ""A lot of the initial feature requests came from KATSU, but after I gave him early versions of the app to test he would show it to other writers and give me feedback from them as well,"" says Theodore Watson, Fat Tag's developer. ""One of things that surprised me was how quickly writers would do their tags over and over again. I tried to change the 'clear tag' to a button hidden in the menu, but that made the process of writing a new tag much slower, so I switched it back to the double tap to clear."" ""One of the really nice features of the app is it lets you take photos with your camera and make it the background that you can write over,"" Watson continues. ""However a lot of things you might want to tag in your photo can be at funny angles and perspectives. So we made simple tools that let you rotate and scale the tag in virtual 3D space, so that it can lineup with the perspective in the photo. ""Fat Tag also allows you to upload your tag as the open format GML (Graffiti Markup Language) to blackbook . This allows you to see both the image from Fat Tag, but also the way the tag was drawn. So far we've had over 40,000 tags uploaded. A couple of years ago I made a visualization of all the GML tags uploaded sorted by most prolific ."" Even longtime writers like KATSU and Rehab have used the app to learn new things. ""Sometimes you're on a train or bus and don't want to practice on paper, so the convenience factor helps,"" Rehab explains. ""Also I feel some of the various brushes most people wouldn't use in real life, 'cause most writers like to stick to what they are used to and wouldn't experiment with new or unfamiliar styles on the live streets at the risk of looking toy."" ""I learned that I'm really good with a chisel tip and can surprisingly do decent Philly hand style,"" he adds. It's an app with as much clout as potential. Speaking with KATSU, Watson, and Rehab, it sounds like a lot more could be added to turn Fat Tag into the digital writing suite of your wildest inhalant-induced dreams. Ideas including an animated tag recorder, gestural tagging, geo mapping, a live feed, a glass scratching feature, a head-to-head tagging game, and even the ability to link the app to drones are all bouncing around, promising to make Fat Tag so much more than the perfect digital black book. Maybe they'll even add ""a fuckin stylus,"" KATSU chides. ""So wack, why did I say that?""   Click here to download Fat Tag from the App Store.  Related:  Kabul's Female Graffiti Master | TCP Meets Shamsia Hassani  Honolulu's 'Graffitification' Problem Can't Stop the POW! WOW! Art Festival  Anish Kapoor’s ‘Vagina Sculpture’ Graffitied Again—But This Time He’s Not Cleaning It",,Vice,'Fat Tag' Is Like Graffiti Methadone for Real Writers,Entertainment,https://www.vice.com/en_us/article/wnpk9m/fat-tag-graffiti-sketchbook-app-katsu-rehab
Michael Byrne,"The code itself, the symbols and strings that eventually dictate the behavior of a machine, is one thing, but properly preparing it and executing it requires a dedicated venue. Your operating system's standard text editor—TextEdit on Mac and Notepad on Windows—will really only get you so far before becoming a limiting factor. These stock tools are generally meant for writing text intended for human consumption and offer few of the even most basic coding features, including looking the part. Unfortunately, the world of code editors is a complete zoo. Some editors are elaborate Photoshop-like behemoths offering platforms suitable for building vast software projects, while others are esoteric and minimal, rewarding only those with patience and willingness to cope with a steep learning curve. Some editors are suited for specific programming languages and platforms, like Swift and iOS, while others aim to be general but extensible. A few are completely meaningless beyond one specific language, while a small set of code editors become programming languages unto themselves. So, given all of the noise, where should one even start? I think I can help. What follows is a field guide intended for those most likely to be asking the above question: the code curious, beginners, advancing novices. 1.0) Supercharged text editors: Sublime Text and Atom For the type of coding described in the Hack This series , this is the ideal category of text editor. It's dominated by Sublime Text and Atom . The latter is free, while the prior theoretically costs money but can be evaluated indefinitely as nagware (you should pay for it!). When you first open one of these editors, at first glance it may just seem like an iteration of TextEdit/Notepad with better colors. It's when you start digging around in the menus that the depth and capabilities become clear. For one thing, both come equipped out of the box with syntax highlighting for pretty much any programming language you're likely to be interested in. This might seem kind of trivial when you're starting out, but once you know a language a bit and what a script or program in that language should look like, highlighting becomes immensely valuable in making code readable. Eventually, monochrome code will just look wrong , like English written without paragraph breaks or punctuation. Editors in this category are extensible via package systems. The package ecosystem for Sublime Text is vast, with installable packages offering new editor capabilities ranging from those enabling regular word processing (like word count and spell check), HTML templating engines (like Handlebars), Git integration, code quality tools, and well beyond. Whatever sort of coding you're into, it's possible to essentially build up your own ideal code editor via these third-party packages. Personally, I don't take enough advantage. 2.0) IDEs In my early-days community college programming courses, I starting out writing code within the beast that is Visual Studio, Microsoft's integrated development environment (IDE) geared toward developing full-on software for the .NET framework. Within Windows, this is the natural home for writing old-school PC software in C++, C, and C#. In school, this meant making things like prime number checkers, record collection organizers, and Colossal Cave Adventure clones. IDEs are enormously powerful, and, unlike the aforementioned text editors, this power is flung at users out of the box. It can be intimidating. Most offer extensions for new programming languages and frameworks that essentially amount to new IDEs within IDEs. The Scala IDE, for example, is really a specialized version of the Java-centric Eclipse IDE (Scala is an extended version of Java). Similarly, Android Studio is really an extension of the more general Java IDE IntelliJ IDEA . What drives me from a text editor like Sublime Text to a full-fledged IDE is usually organization and-or debugging. Sublime and Atom both offer the ability to manage multiple files and directories, but this just feels more natural in an IDE, particularly when things get really messy in contemporary web development, which demands extreme modularity and the inclusion of often many, many outside libraries. When dealing with long, complicated programs, debugging becomes a far more involved process than just staring at a dozen lines of code for a few minutes until the error is magically revealed. Instead, it becomes neccessary to watch the code in action as it executes. This is what step-through debugging achieves. A debugger allows programmers to manually step through their code line by line as it executes, providing a means to observe side effects and state changes as they happen. As such, bugs can be traced down to the lines where they occur, which is pretty neccessary when dealing with hundreds or tens of thousands of lines of code spread across many individual files. An IDE can allow programmers to even observe code execution at extreme low levels, including physical processor registers and memory addresses. Part of this low-level view is what's known as code profiling, where precise performance statistics can be collected about a given section of code. Bugs are, after all, more than just program-sinking errors, but include system-draining inefficiencies. You could say these are algorithm-level bugs. As far as IDEs go, you'll find many arguments for the JetBrains family of software, including PyCharm for Python (above), WebStorm for JavaScript and website development, CLion for C and C++, and others. These are all highly worthy (but not free). 2.1) Xcode Xcode is sort of its own class of IDE. This is where Apple pushes iOS development. It's built for writing code in Swift and its predecessor language Objective-C, the languages of iOS, and comes with specialized tools for simulating and profiling Apple devices. Part of its appeal is an emphasis on visual coding. As far as IDEs go, it's unusual in this friendliness to non-programmers. You can get pretty far in building an iOS app just through dragging and dropping. There's no Windows version. 3.0) Hardcore text editors: Vim and Emacs Vim is hard enough to describe, let alone use. On its face, it's a hyperminimal text editor, a realm where even the mouse has no meaning and drop-down menus have yet to be invented. Menu actions instead are accomplished via text commands. Saving? That would be :w . Quitting without saving? :q! . Copy and pasting? Glad you asked. Behold, the official Vim documentation: *04.6* Copying text To copy text from one place to another, you could delete it, use ""u"" to undo the deletion and then ""p"" to put it somewhere else. There is an easier way: yanking. The ""y"" operator copies text into a register. Then a ""p"" command can be used to put it. Yanking is just a Vim name for copying. The ""c"" letter was already used for the change operator, and ""y"" was still available. Calling this operator ""yank"" made it easier to remember to use the ""y"" key. If that makes sense, then congrats, you understand Vim. Based on years of reading Vim-related answers on Stack Overflow, this understanding is a point of pride among Vim users, which is putting it mildly. I kinda-sorta understand Vim. Vim (and Emacs, its traditional competitor) offer more than esotericism for its own sake. There's immense power behind the maddening anti-interface (which is really a function of Vim and Emacs predating graphical operating systems and even PCs, generally). Interfacing with Vim is based entirely on keystrokes—even the cursor keys are shunned in favor of the h , j , k , and l keys—which winds up being really, really fast when you have the hang of it. Consider that in a typical GUI environment most everything is accomplished by interrupting keyboard typing and interfacing with some menu element. Even if that element displayed front and center in a toolbar, the fingers are forced to leave and then return to the keyboard. In Vim, those same commands are achieved as a part of the same keyboard typing flow. That's the key word: flow. Vim is extremely customizable and constitutes a programming language in itself. Much as we might enter a Python command into a Python interpreter or a Bash command into the Bash interpreter, we interact with Vim by entering Vim commands onto the Vim command line. As with Python and Bash, we can collect those commands into scripts, which are referred to in Vim configuration files. There's a whole history here that I'll save for another post, but Vi (Vim's earlier incarnation) and Emacs are both now 40 years old and constitute the earliest text editors for any purpose, coding or otherwise. Both still come built into Unix-based operating systems, including OSX. Just type vim or emacs in the terminal. So where should you write code? If you're asking the question, the answer is probably Atom or Sublime Text. Both are simple out of the box but will grow with you. Eventually, you'll be forced to pick something else up—like Rstudio for data analysis in the R language, or Xcode for app development—but when it comes to writing useful scripts for tasks like, say, webscraping or automating operating system tasks, you want a space that can offer features only when you need them and a space where you can focus on the code, not the environment. Read more Hack This.",,Vice,"From bare-bones text editors to IDE monsters, the tools for building new tools.",Hack This: Where To Write Code,https://www.vice.com/en_us/article/3dayq3/hack-this-where-to-write-code
Samantha Cole,"A programmer created an algorithmically-generated face, and then made the network slowly forget what its own face looked like.  The result, a piece of video art titled "" What I saw before the darkness ,"" is an eerie time-lapse view of the inside of a demented AI’s mind as its artificial neurons are switched off, one by one, HAL 9000 style.  The face in the network's mind doesn't exist in real life. Instead, it was composed by generative adversarial networks, or GANs, a type of machine learning program that “learns” from existing photos to produce new things. In this case, the GAN trained on millions of portraits to produce a realistic human face. The network's interconnected neurons dictate her features: eyes, skin color, shapes, strands of hair, similarly to how a human brain uses a network of neurons to construct a mental image of a face.  Then the project's creator, an artist who simply went by ""the girl who talks to AI"" in an email, said that the AI gradually shuts off individual neurons, repeating the process until the network effectively “forgets” what a face looks like.  The effect is pretty creepy. At first, it seems like the generated face is aging. Lines appears under her eyes and around the edges of her face, and her hair thins and fades. After a few seconds, something altogether different begins. Her skins turns a greenish hue, and her features begin to wash away as neurons continue to go dark. Within sixty seconds, she's completely decomposed—nothing but a white and brown smudge.  ""The inspiration behind the project is rooted in contemplation of human perception,"" the creator said. ""Everything we see is the brain’s interpretation of the surrounding world. A person has no access to the outside reality except for this constructed image."" Read more:  Turn Your Pet Into Another Species With This AI Tool  She compared this to how Claude Monet's paintings shifted to blurred, muddled greens and yellows as he aged: Our eyes and brains and the networks that connect them undergo changes and deterioration that we might barely notice as it’s happening.  Neural networks are still somewhat mysterious things, and their decisions can be tough for experts to tease apart after the fact. One way that computer scientists have attempted to lift the veil on AI is by working backwards, systematically deleting neurons to see which are most important to an AI’s picture of the world, as Alphabet-owned machine learning firm DeepMind did last year .  ""The brain constructs a different reality, but there is no one to call a bluff,"" she said. ""Neuroscientists tell us deep neural networks are similar to the visual system in certain aspects, so this project for me is a unique opportunity to see the world changing in someone’s mind, albeit artificially.""",,Vice,This time lapse of a neural network with the neurons slowly switching off is a haunting experiment in machine learning. ,Watching AI Slowly Forget a Human Face Is Incredibly Creepy,https://www.vice.com/en_us/article/evym4m/ai-told-me-human-face-neural-networks
Andrew Egan,"A version of this post originally appeared on Tedium , a twice-weekly newsletter that hunts for the end of the long tail. A New Yorker and a tourist walk into the subway station at 42nd Street, also known as Times Square. One is delighted to be there; the other, extremely annoyed. One knows how to get out of there as quickly as possible. The other doesn’t. The New Yorker and the tourist are distinct but in this moment, they are the same.  Both are about to swipe their MetroCards, subject to the whims of the Metropolitan Transit Authority and the unheard-of reliability of a marginally successful operating system from the early 1990s.  And that operating system remains in use as the subway system feels the daily strain from heavy use.  According to one metric, 5.7 million people rode the New York City subway on an average weekday in 2016. This was the highest average total for the system since 1948.  If you ask an anecdotal average New Yorker, their response is usually, “That’s it?” Their disbelief is understandable since the city has some 8 million permanent residents and often swells to as much as 20 million during peak hours and holidays. (Guess a lot of people really enjoy flagging down cabs.)  But either way, 5.7 million is a lot, and one of the elements managing it is an operating system that fell into obscurity a quarter-century ago: IBM’s OS/2.  Betting on the future is hard, but the MTA kind of did it  In a Tedium article from this past March, Ernie Smith wrote about IBM’s big bet on a microkernel for operating systems —effectively a minimal layer of software through which an operating system’s basic tasks are handled—a bet that included a variant of its well-known-if-less-remembered OS/2. His article demonstrates how big the bet was. However, IBM’s confidence in its operating system prowess led others to take similar chances.  No bigger bet was made than the one by the MTA, the Metropolitan Transit Authority, who needed some process to eliminate tokens while moving into an era where everything was expected to be digitized. The result was the iconic MetroCard. A thin slice of yellow plastic with a prominent black strip, the MetroCard has been a staple of New Yorkers wallets since its introduction in 1993.  The story of how the current method for accessing New York’s subway is interesting for its insights into public infrastructure and the way it serves the public. Before we can touch that topic, it’s helpful to understand how the current system came to be. Because when you build something as important as the infrastructure to the NYC subway, it needs to work.  You pretty much get just one shot—and any mistakes will likely cost billions to repair and frustrate the lives of millions. Among many choices, one of the most robust turned out to be one of IBM’s most high-profile failures.  Image: Wikimedia Commons  How IBM’s much-hyped but underwhelming OS found a home serving millions  Smith’s previous piece on the story behind IBM’s push into microkernels and its overall failure gives a whole host of backstory that I’m only going to briefly touch on here. For my purposes, the most relevant line from that article is nice and succinct: “OS/2, of course, did have its adherents.”  New York City's subway system was one of those adherents. The role of OS/2 in the NYC subway system is more of a conduit. It helps connect the various parts that people use with the parts they don’t.  “There are no user-facing applications for OS/2 anywhere in the system. OS/2 is mainly used as the interface between a sophisticated mainframe database and the simple computers used in subway and bus equipment for everyday use. As such, the OS/2 computers are just about everywhere in the system,” OS/2 and MTA consultant Neil Waldhauer said in an email interview. Waldhauer maintains an important OS/2-based tool that helps the MetroCard system function.  The reasons why the MTA ultimately decided to utilize OS/2 as it digitized some aspects of the subway system mirror the hype surrounding the operating system’s launch in the early 1990s. But a lot of that conversation and development started years before. Behind the scenes, Microsoft and IBM were working on the next generation of operating systems. While the general belief in popular culture dictates that Gates and Microsoft fleeced IBM over MS-DOS (the truth is more complicated ), at the time IBM clearly felt it had a strong partner and joined forces with Microsoft again.  Rather than bemoan lost profits, IBM seemed to recognize a gap in its knowledge and it began the push to develop next-generation operating systems at a fundamental level, at first in partnership with Microsoft. This, almost predictably, worked out for IBM about as well as the MS-DOS deal did. However, in a very narrow window in the late 1980s, executives at the MTA were looking to remove tokens from the subway system and replace them with a prepaid card. The benefits were obvious, allowing for easier fare increases while offering tiered pricing. Riders would now have the option to choose between an individual or round trip option and an unlimited option that covers a set amount of time.  To implement this revolutionary upgrade, the MTA went with a known entity, IBM. At the time, it made a lot of sense.  “For a few years, you could bet your career on OS/2,” Waldhauer said.  To understand why, you need to look at the timing. “The design is from a time before either Linux or Windows was around. OS/2 would have seemed like a secure choice for the future,” Waldhauer said.  So for a lack of options, the MTA went with OS/2. And it’s worked out for decades, as one of the key software components of a complex system.  And it might even survive beyond that, according to Waldhauer: “I will go out on a limb and say that as long as MetroCard is accepted in the system, OS/2 will still be running.”  This is an exceptionally interesting point because the MTA is currently testing a system that can replace MetroCards with various forms of contactless payments. This transition should make things more efficient, while helping the MTA collect additional revenue.  It may sound nice, but the gaps are easy to see, especially if you look at a weird quirk in the current MetroCard system.  Image: Flickr/ MTA  The mysterious magnetic strip and how it affects the lives of others  The move from tokens to MetroCards took years, and was by no description smooth. Tokens were officially phased out in 2003. By then, MetroCards were accepted at every station in the city—but no one was happy.  Access to the subway is usually easy but complaints about swiping your MetroCard are everywhere . And a lot of this seems to be a stupid disconnect between various parts of the system. While OS/2 is used to connect various parts of the subway system to a larger mainframe, the input components weren’t held to a higher standard.  The turnstiles in any given subway station in NYC are notoriously fickle—but they could work with IBM’s system.  ATMs used to rely on OS/2, too. Image: Malvineous/Wikimedia Commons  Despite the failure of OS/2 in the consumer market, it was hilariously robust, leading to a long life in industrial and enterprise systems —with one other famous example being ATMs.,  “Thinking about all the operating systems in use [in the MTA], I’d have to say that OS/2 is probably the most robust part of the system, except for the mainframe,” Waldhauer said.  It’s still in use in the NYC subway system in 2019. IBM had long given up on it, even allowing another company, eComStation , to maintain the software starting in 2001 after it became clear that there was still a maintenance need. Eventually, another firm, Arca Noae ended up carrying the OS/2 torch: it sells an officially supported and regularly updated version of OS/2 , ArcaOS, though most of its users are in similar situations to the MTA.  At this point, we’re talking about an OS designed in the late 80s, and released in the early 90s as part of a difficult relationship between two tech giants. The MTA had to ignore most of this because it had already made its decision, and changing course would cost a lot of money.  The integration between the backend and the things New Yorkers/tourists actually confront can be stupidly uncoordinated.  “I feel like the designers really considered MetroCard to be a mainframe database application with some random electronics to tie it together,"" Waldhauer said.  And now we get to talk about the magnetic strip. The black bar at the bottom of every MetroCard, regardless of branding, simply has to work. How it actually works is, for an obvious reason, a secret.  “People have hacked the MetroCard,” Waldhauer said. “If you have a way to see the magnetic encoding, the bits are so large you could see them under a magnifying glass. The encoding of the magnetic stripe is so secret that I have never seen it… It’s amazing the lengths people will go to for a free ride.”  Why does any of this matter? As a point, it really doesn’t. The MTA has made it clear it wants to move to contactless payments, just like the Oyster Card in London. But that process also has issues. It even hired Andy Byford, who spent years in leadership roles with the London and Toronto train systems, to help modernize the NYC Subway —with the end goal of ultimately eliminating MetroCards. The just-launched OMNY system , which will roll out over the next few years. Image: MTA  Access to the subway of the future will be contactless, using a patchwork series of devices and cards that will largely rely on digital payments. With device partners such as FitBit , catching a train in midtown will be more like queuing for a roller coaster at Disney World . If New Yorkers are lucky, the process will still easily accept cash but even that is no guarantee.  As I highlighted at the beginning of this, the MetroCard, for all its weaknesses and with dusty but functional technology at its core, was the great equalizer. But with a system that seemingly favors users with high-end phones, smart watches, and bank accounts, are we moving towards a world that feels a lot less equal? (And consider that we managed to ask these questions without even really diving into the Great Swipe Debate.)  It's an open question that we may only know the true answer to as the old tech gives way to new.",,Vice,"Vintage technology has powered the innards of the NYC subway system for decades—and sometimes, it surfaces in interesting ways. This one’s for you, OS/2 fans.",The Forgotten Operating System That Keeps the NYC Subway System Alive,https://www.vice.com/en_us/article/zmp8gy/the-forgotten-operating-system-that-keeps-the-nyc-subway-system-alive
Samantha Cole,"The viral success of Snapchat's gender-swap filter doubled downloads of the app after its introduction in May. Some trans users have criticized the filter, but it also enabled one college student to catch an alleged sexual predator who turned out to be a cop.   According to NBC Bay Area , 20-year-old San Francisco area college student Ethan (whose last name was withheld for his privacy) used the filter to pose as an underage girl online. He was inspired to go catfishing for predators after one of his friends told him they were sexually abused as a child, Ethan told NBC.  Using Snapchat's gender-swap filter, he took a photo of himself swapped as a girl, and set up a Tinder profile with the name ""Esther."" A man messaged him there, saying, ""Are you down to have some fun tonight?""  ""I decided to take advantage of it,"" Ethan told NBC.  That man turned out to be Robert Davies, an officer for the San Mateo Police Department.  Ethan moved their Tinder conversation to Kik messenger, and then Snapchat's direct messaging, according to CBS , where the tone became more explicit. ""Esther"" told Davies that she was 16 years old, and asked if that bothered him. He said it didn't, according to police who saw screenshots of the conversation.  The pair texted for over 12 hours. Because Snapchat notifies the other person when someone takes a screenshot of their conversation, getting evidence to send to authorities was tricky. Ethan said he had to turn his phone on airplane mode before taking several screenshots at once, to avoid the platform alerting Davies and possibly being blocked.  He sent the screenshots to the local police department's tip line, and Davies was arrested on one count of communicating with a minor for the purposes of committing a felony.  ""While the criminal investigation is still running its course, this charge, if true, is disturbing and the conduct alleged violates the very values and principles of this department,"" a spokesperson for the San Mateo police department told Motherboard in an emailed statement.  “This alleged conduct, if true, is in no way a reflection of all that we stand for as a Department, and is an affront to the tenets of our department and our profession as a whole,"" San Mateo Police Chief Susan Manheimer said in the statement.  Davies is currently on paid leave, according to the San Mateo police department.",,Vice,"The college student used a picture of himself made to look like a girl, and posed as a 16-year-old on Tinder.",A Man Used Snapchat's Gender Swap Filter to Catch a Cop Allegedly Soliciting a Minor​,https://www.vice.com/en_us/article/zmpgd9/snapchat-gender-swap-filter-to-catch-a-cop-allegedly-soliciting-minor
Samantha Cole,"When three of the five members of the Badass Army's technical team— a group of activists working against revenge porn —were granted scholarships to attend the world's largest hacking conference, the group didn't want to leave the other two behind. So they got creative.  ""Selling nudes to attend #defcon2019 . Ten bucks a tit. DM for cashapp,"" Kelsey Bressler, the CTO of revenge porn activism organization Badass Army, tweeted in early June:   The public discussion that followed online sparked a conversation about who ""belongs"" at a hacking conference like DEF CON—and even got the founder of the conference involved.  ""We set up the GoFundMe based on that joke, and it spiraled from there,"" Katelyn Bowden , founder and CEO of Badass, told Motherboard. ""We are determined to get the whole team to DEF CON because we want to raise awareness for our cause, make connections to those who would be allies against non-consensual pornography, and get a feel for the people who would hack into people's accounts to steal nudes without consent.""  Tickets to attend DEF CON cost $300, plus travel and lodging in Las Vegas. Badass's GoFundMe campaign has raised nearly $12,000 so far, with proceeds beyond their travel budget going back to the organization and helping revenge porn victims.  Since tweeting about selling nudes to make it to DEF CON, some people have told Kelsey they're disappointed, disgusted, and offended that she chose to sell nudes to make it to the conference.  ""Because I am a woman with a body, everyone had an opinion,"" Kelsey said. ""Either they thought it was a clever marketing scheme, or they got pissed off at the idea of a woman doing whatever she wants.""  ""We’ve seen reactions that range from EXTREMELY angry to incredibly positive,"" Bowden said. ""I wasn’t shocked to see either, honestly. Nudity and sex, especially when it involves women, is taboo—it evokes some feelings in people.""  The founder of DEF CON, who goes by Dark Tangent online, tweeted in support of the ""tits to DEF CON"" campaign. ""It’s not a zero sum game or only so many slices are available,"" he said. ""The more people that attend the bigger the pie for everyone.""   Jackie Singh (formerly Stokes), founder and CEO of Spyglass Security, stumbled across Kelsey's tweets about her campaign and decided to jump into the conversation.  ""Her being maligned for her actions seemed wrong, especially in connection with a 'hacker' event—and that many people might not understand the underlying issues, so it made sense to speak out,"" Singh told Motherboard. ""Transgressive behavior is the norm for hackers, and breasts deserve the freedom to exist inside a technical space. When women use their bodies in ways that make others uncomfortable, many are quick to establish a public pillory.""  Kelsey's intentionally left vague whether she's actually sending photos of herself nude to people; many, she said, are messaging her just to see if it's real.  ""The whole nature of DEF CON is to be subversive, to buck tradition, and to circumvent any traditional methods, and I think our tongue-in-cheek campaign did just that,"" Bowden said. ""Did we sell nude pics? Did we troll and send pictures of silly things instead? No one knows. That's the point—the whole thing was a social engineering experiment to raise both funds and educate people on the concepts of agency and consent.""  If you can social engineer the internet into giving you more than $10,000 using a viral crowdfunding campaign and some boobs, you deserve to attend the hacking conference, Bowden said.  ""To all the people who are angry about a woman using her body—a weapon previously used against her—we hope they bring that same energy the next time they see someone’s agency not being respected online,"" she said. ""And we will be seeing them at DEF CON!""  Subscribe to our new cybersecurity podcast, CYBER .",,Vice,"The ""tits to DEF CON"" campaign, led by Badass Army, has already raised more than $11,000.",Why This Revenge Porn Activist Is Selling Nudes to Get to DEF CON,https://www.vice.com/en_us/article/d3nkqm/revenge-porn-activist-badass-army-is-selling-nudes-to-get-to-def-con
Madeleine Gregory,"The Oregon Department of Corrections has banned prisoners from reading a number of books related to technology and programming, citing concerns about security.  According to public records obtained by the Salem Reporter , the Oregon Department of Corrections has banned dozens of books related to programming and technology as they come through the mail room, ensuring that they don’t get to the hands of prisoners.  At least in official department code, there is no blanket ban on technology-related books. Instead, each book is individually evaluated to assess potential threats . Many programming-related books are cited as “material that threatens,” often including the subject matter (“computer programming”) as justification.  Rejected books that are geared towards hacking, such as Justin Seitz’s Black Hat Python, may represent a clearer threat to the Department of Corrections, which fears that prisoners could use those tools to compromise their systems. But how did books such as Windows 10 for Dummies , Microsoft Excel 2016 for Dummies, and Google Adsense for Dummies (marked as posing ""clear and present danger""), fail the prison’s security test?  “I’m not entirely surprised that my book is on that list,” Seitz told Motherboard. “I think what’s more surprising is some of the other, much more baseline ones. Learning a programming language in and of itself is not dangerous.”  Proficiency in Excel and Windows 10 isn’t viewed as dangerous in the outside world. Instead, it's a prerequisite for most entry-level jobs. Andy Rathbone, author of the Windows for Dummies series, said that he doubts anything in his books could be used to compromise the prison’s systems. Some of his blacklisted books date back to the 90s, he said, their contents so outdated that it would be hard to imagine a prisoner using them maliciously.  Rathbone sees a big problem, though, in not teaching prisoners basic computer skills that they will need when they re-enter society.  “As soon as they get out of prison and have to deal with today’s world when just about everything is computerized, they won’t know what to do,” Rathbone said. “If they can’t get legitimate jobs, what are they going to do?”  Officials at the Oregon Department of Corrections (DOC) argue, however, that knowledge of even these basic programs can pose a threat to prisons.  “Not only do we have to think about classic prison escape and riot efforts like digging holes, jumping fences and starting fires, modernity requires that we also protect our prisons and the public against data system breaches and malware,” DOC spokesperson Jennifer Black said in an emailed statement. “It is a balancing act we are actively trying to achieve.”  In one instance, a prisoner allegedly used a malicious thumb drive (prisoners are allowed to have thumb drives for educational or work-related purposes) to copy staff files from an Excel spreadsheet when an employee inserted it into a computer, Black said.  According to Rutgers law professor Todd Clear, security concerns are overblown because learning to hack can require more than reading a book (for example, unrestricted internet access and some savvy comrades), and prison staff can monitor prisoners’ activities.  “They are different places, no doubt, but the security claim is often specious,” he said.  Acknowledging the importance of technological knowledge for reentry, the department has been working to provide more technology to prisoners, including Windows 10 and limited access to the internet.  The Oregon DOC has approved nearly 98% of books and magazines sent in the last three years, Black said. In the last seven months, the DOC has approved a handful of tech-related titles such as C Programming Absolute Beginner's Guide Part 1 and An Introduction to Using Windows 10.   As for why some books get banned and others don't, Black said that staff make a decision ""based on IT experience, DOC technical architecture and DOC’s mandate to run safe and secure institutions for all."" Each book is evaluated based on the information inside, the exact subject matter, and level of detail it provides.  Oregon is not alone in restricting prisoner access to tech-focused books. Ohio and Michigan, too, have banned books that teach programming . In Kansas, technology-focused books also feature in the thousands of titles banned from prisons .  On the outside, reading books is usually a pastime, and sometimes a chore. To those in prison, books are a rare link to the outside world, a source of education and entertainment. Many prisoners, especially those with long sentences, are completely cut off from the technological world. Some have never touched a smartphone or worked on a laptop. Reading about blockchains may be the closest they get to understanding how technology is evolving in modern society.  For those incarcerated people leaving prison, programming books act as job training. Coding classes are becoming increasingly popular in prisons, giving incarcerated people a glimmer of hope at a tech career outside prison walls. Books that give prisoners a head-start on programming can help them once they’re back in society. Lack of access to skills or education, by contrast, is a big driver of putting people back in prison .  “Prison officials tend to be overzealous in their banning, and often err on the side of banning,” said Emerson Sykes, staff attorney at the ACLU. “It speaks to an underlying approach that focuses on the punitive nature of incarceration rather than a rehabilitation approach.”  Listen to CYBER , Motherboard’s new weekly podcast about hacking and cybersecurity.",,Vice,"Oregon prisons have banned dozens of books about technology and programming, like 'Microsoft Excel 2016 for Dummies,' citing security reasons. The state isn't alone. ",Prisons Are Banning Books That Teach Prisoners How to Code,https://www.vice.com/en_us/article/xwnkj3/prisons-are-banning-books-that-teach-prisoners-how-to-code
Samantha Cole,"The creator of DeepNude , an app that used a machine learning algorithm to ""undress"" images of clothed women announced Thursday that he's killing the software, after viral backlash for the way it objectifies women.  On Wednesday, Motherboard reported that an anonymous programmer who goes by the alias ""Alberto"" created DeepNude, an app that takes an image of a clothed woman, and with one click and a few seconds, turns that image into a nude by algorithmically superimposing realistic-looking breasts and vulva onto her body.  The algorithm uses generative adversarial networks (GANs), and is trained on thousands of images of naked women. DeepNude only works on images of women, Alberto said, because it's easy to find thousands of images of nude women online in porn.  Following Motherboard's story, the server for the application, which was available for Linux and Windows, crashed:   By Thursday afternoon, the DeepNude twitter account announced that the app was dead: No other versions will be released and no one else would be granted to use the app.  ""We created this project for users' entertainment months ago,"" he wrote in a statement attached to a tweet. ""We thought we were selling a few sales every month in a controlled manner... We never thought it would become viral and we would not be able to control traffic.""  When I spoke to Alberto in an email Wednesday, he said that he had grappled with questions of morality and ethical use of this app. ""Is this right? Can it hurt someone?"" he said he asked himself. ""I think that what you can do with DeepNude, you can do it very well with Photoshop (after a few hours of tutorial),"" he said. If the technology is out there, he reasoned, someone would eventually create this.  Since then, according to the statement, he's decided that he didn't want to be the one responsible for this technology.  ""We don't want to make money this way,"" the statement said. ""Surely some copies of DeepNude will be shared on the web, but we don't want to be the ones to sell it.” He claimed that he's just a ""technology enthusiast,” motivated by curiosity and a desire to learn. This is the same refrain the maker of deepfakes gave Motherboard in December 2017 : that he was just a programmer with an interest in machine learning. But as the subsequent rise of fake revenge porn created using deepfakes illustrated, tinkering using women's bodies is a damaging, sometimes life-destroying venture for the victims of ""enthusiasts.""  ""The world is not yet ready for DeepNude,"" the statement concluded. But as these victimizing algorithms and apps show, there is no simple solution for technology like DeepNudes, and the societal attitudes that erase women's bodily autonomy and consent.",,Vice,"An app that algorithmically 'undressed' images of women was taken down by its creator, citing server overload and potential harms.","Creator of DeepNude, App That Undresses Photos of Women, Takes It Offline",https://www.vice.com/en_us/article/qv7agw/deepnude-app-that-undresses-photos-of-women-takes-it-offline
Samantha Cole,"On Monday morning, chat platform Discord banned a server that was selling a version of DeepNude , a program that creates non-consensual nude images of women using machine learning algorithms.  The server, which Motherboard accessed, included a link to download a version of the DeepNude software and instructions for how to pay the creators—who were asking for $20 donations to use the program—through Amazon gift cards or Bitcoin.  It also included several algorithmically-generated nude images of women created with the program and discussions about how this version is new and improved over the original.   Motherboard reported on the existence of DeepNude on June 26, after its creator started promoting the program on his website for $50. The following day, the programmer, who goes by Alberto, took the site and servers offline , and stated that any further use of his creation would be against the terms of use for his software.  But since the program was available to download and use offline, it was easy to replicate once someone had it on their hard drive. From there, users could upload it elsewhere or share it in private forums—which is how it got to Discord.  An invite link to the server was posted to a Twitter account devoted to bringing DeepNude back to life according to its bio, and advertised access to the application within the server.  Twitter did not immediately respond to a request for comment. The account tweeted that although the server is banned, its organizers plan to launch a new website.  When activity is flagged, Discord investigates and decides whether to take action. In this case, Discord found the server and its users in violation of the platform's community guidelines , which forbids sharing or linking to content ""with intent to shame or degrade another individual,"" and promoting sexually explicit content without the subject's consent.  “The sharing of non-consensual pornography is explicitly prohibited in our terms of service and community guidelines,"" a spokesperson for Discord told Motherboard. ""We will investigate and take immediate action against any reported terms of service violation by a server or user. Non-consensual pornography warrants an instant shut down on the servers and ban of the users whenever we identify it, which is the action we took in this case.”  This is a similar stance Discord took after people started using the platform to share datasets, results, and troubleshooting tips for deepfakes . Other social media platforms, including Twitter , Gfycat , and Reddit , took similar stands against algorithmically-generated non-consensual porn after Motherboard found evidence of it on their sites.  Emanuel Maiberg contributed reporting to this article.",,Vice,"A small community of people is dedicated to reviving DeepNude, a program that creates non-consensual nudes of women.","Discord Just Banned a Server Selling DeepNude, an App That Undresses Photos of Women",https://www.vice.com/en_us/article/d3n9xa/discord-banned-a-server-selling-deepnude-an-app-that-undresses-photos-of-women
Michael Byrne,"Presumably there are people that think the PHP language is awesome. An afternoon spent writing PHP code is like a fine meal and a backrub in one transcendent coding experience while JavaScript and client-side scripting can just go to hell. They must exist, right? After all, the language and its interpreter, the Zend Engine, are actively developed by volunteers. PHP 7, the latest complete PHP overhaul, was released just two weeks ago (Dec. 5) and at the very least the language's creators must be excited about that, if no one else. Maybe they even threw whole PHP parties. It's not that PHP doesn't deserve a party. It's in some very large part responsible for the web we experience and take for granted today, which is the web of interactive web pages and web apps. It wasn't always like this and not even all that long ago the internet tended to look a lot more like a collection of billboards and widgets than the collection of experiences we're more familiar with. Web pages nowadays do stuff in ways more and more resembling software and to get to this point it took a suitable web scripting language. For HTML to interact with users there needs to be something there to change the HTML and that's what PHP does. PHP is a server-side web scripting language, which means that it lives in the same place as all of the other website junk while functioning dynamically to assemble all of that junk in useful ways. Let's see what that means. Hello, World If I wanted to make a webpage with one single function of telling users ""hello, world"" it would be pretty easy to do in HTML. All it would really take is sticking the words ""hello, world"" in a document inside a pair of tags. Every time you loaded up my URL, a server would send you the same page with the same words and it would look the same every time, at least when viewed with the same browser. Maybe that's cool, but maybe you'd like to be able to tell returning users something different, like ""hello, again."" In this case, I could write a little bit of PHP code telling the web page to only show you ""hello, world"" if the page had not registered your visit in a text document it maintains also on the server. If your IP address is already logged on that document, the page would say ""hello, again."" This is something that HTML can't do. HTML is a markup language and really only tells a browser how things are supposed to look rather than what they're supposed to do . Image: php.net A fundamental feature of what we usually think of as programming are conditional statements, e.g. if some condition is met then do something. HTML has none of these, nor does it have variables or most anything else needed to turn logic into functionality. It's most accurate to not even refer to HTML as a programming language at all. Again: a markup language. When we need the dynamism of a conventional programming language we very often turn to PHP. Here we can have conditional statements (and beyond) because the whole purpose of the language is to change things. In PHP we have variables and data types and many of the trappings of object-oriented programming . As such, we can expect that PHP is capable of mirroring the functionality of any other program written in any other programming language, however inefficiently and-or clumsily. So, for our example, I would write a PHP script that uses a conditional statement to test for whether or not your IP address has been logged. If it has been, the script will do one thing. Otherwise, it will do another thing. This script will look not unlike the C programming language, from which the syntax and structure of PHP draws heavily. How PHP works PHP is a scripting language, not a programming language. While the distinction can get pretty muddy, it's crucial for the nuts and bolts of web development. Unlike something like C++, which eventually gets assembled into object code and machine instructions in a step known as compilation , all of the PHP pieces are already there pre-built, in a sense. These pieces all live in something called the Zend Engine, which is an open-source project sponsored in part by the private for-profit Zend Enterprises. The Zend Engine is the actual computer program that lives on a server and interprets PHP scripts. As in other interpreted languages (Perl, Python, JavaScript, and others), this sort of blobby entity has all of the needed machine code and memory allocations ready at hand for whatever a script may demand and it's just a matter of arranging all the pre-built stuff in just the right way (again: in a sense). PHP doesn't communicate or interface with users or the web browser directly: it only generates HTML. It's sort of like a puppetmaster living in the background of interactive web pages. If, say, you enter some data onto a nice looking web form, it's probably PHP that actually grabs your input and does something with it. The accidental language The history of PHP is short. It originated in the mid-1990s as ""Personal Home Page Tools (PHP Tools) version 1.0."" A lot of what modern PHP is was present then, but its development until 2014, when the language's first formal specification was released, was sort of a mess. An oft-repeated quote from Rasmus Lerdorf, the language's founder, is as follows: ""I don't know how to stop it, there was never any intent to write a programming language […] I have absolutely no idea how to write a programming language, I just kept adding the next logical step on the way."" Is this an elephant preparing to poop? Image: php.net Developers hate PHP Even the blog post I sourced the above Lerdorf quote from, which is ostensibly a PHP defense from a Zend-certified engineer named Felipe Riberio, concludes with saying that PHP has pushed itself into eventual obsolescence. ""In a very personal opinion,"" Ribeiro writes, ""I believe PHP is a game changer in the history of the Web as it changed the way we develop software in this era and the whole paradigm of this industry. But with the poor decisions and indifference to the 'state of the art' displayed by the core developers, PHP is now behind the others in many aspects and honestly, at least for me, it's no longer my personal language of choice if I would start a new project or my own company/startup."" It's not hard to find literature about ""why PHP sucks,"" but one of the key issues is inconsistency, both internally and with respect to other languages. It's unpredictable and inelegant. It's like slang, but without a formal, ""proper"" language underneath. Or a toolbox filled with esoteric and half-broken tools that will still be able to fix most things, but only in the most difficult and-or unpredictable manner possible. PHP is also incredibly insecure . Further reading: Coding Horror : ""Is PHP so broken as to be unworkable? No. Clearly not. The great crime of PHP is its utter banality. Its continued propularity is living proof that quality is irrelevant; cheap and popular and everywhere always wins. PHP is the Nickelback of programming languages. And, yes, out of frustration with the status quo I may have recently referred to Rasmus Lerdorf, the father of PHP, as history's greatest monster. I've told myself a million times to stop exaggerating."" Fuzzy Notepad, ""PHP is a fractal of bad design"" : ""PHP is an embarrassment, a blight upon my craft. It's so broken, but so lauded by every empowered amateur who's yet to learn anything else, as to be maddening. It has paltry few redeeming qualities and I would prefer to forget it exists at all."" Just Google ""PHP sucks."" PHP is unavoidable There are alternatives to PHP—languages that do the same stuff, usually better. They include Python, Ruby on Rails, and Perl. Worth noting is that PHP actually began as a set of Perl programs (""Perl hacks"") and it shares much of the syntax. PHP, however, is designed specifically for the web, while Perl is general-purpose. There is also client-side scripting, another way of generating dynamic web pages, but one that depends on the user's browser rather than a program on a server. JavaScript is an example of this ( the example really). But PHP is still everywhere and holds an 80 percent market-share for server-side scripting languages. Overall, it's still the sixth most popular programming language . I and every other programmer can hate it until we catch fire, but PHP will persist because it's completely saturated the internet. And it's accomplished this simply by being there when the internet needed something like it to evolve: a language to make HTML do stuff. Unfortunately, PHP may now be in over its head. Read more Know Your Language .",,Vice,Meet the broken monster at the heart of the dynamic web.,Know Your Language: PHP Lurches On,https://www.vice.com/en_us/article/pgkbey/know-your-language-the-wither-of-php
Michael Byrne,"On Jan. 1, 1970, Unix time was born . It didn't actually exist on that day; the Unix operating system only kind of/sort of existed then anyhow. But when the first edition of the Unix manual was released in 1971, it was thus declared that the beginning of Unix time—the Unix epoch, correctly—hath began on New Year's Day, 1970. Maybe you've heard of the Unix epoch. Simply, it's the reference date that Unix-based computers use to tell time. It is just a count of the number of seconds that have elapsed since the beginning of the epoch. If you're running a Unix or Unix-like machine, you can get this count in its raw form by entering ""date +%s"" at the command line/terminal. (""Date"" by itself will just give you the boring old date-date.) As of this writing, we're at 1,451,688,846 seconds. And now 1,451,689,116 seconds. We're getting close. It's a simple system. Every day a new 86,400 seconds turn over. The Unix time system is agnostic about things like the day of the week, the month, the hour, or really any of the trappings of human timekeeping. The Unix clock is agnostic about Earth's orbit around the Sun, and Earth's rotation about its axis. It's just a counter. The downside is that Unix time doesn't account for leap seconds, which are the consequence of Earth's rotation not being quite on the money, e.g. not taking exactly 86,400 seconds to make one full spin. It might take an extra few milliseconds here and there because it's a great big-ass planet and doesn't care much about our timekeeping methods. Leap seconds are slipped into Unix time only very rarely. The clock might tick only once for two seconds, or it might not tick at all for a single second. Nothing is ever added or subtracted from it, which is a system that keeps a lot of stuff from breaking. Speaking of breaking, Unix time runs into a problem at 03:14:07 UTC on 19 January 2038. The 2038 epochalypse will occur when the Unix time counter overflows the limits of the 32-bit integer data type (at 2,147,483,647 seconds). Time will just rollover to 0 and start again. Upgrading the epoch system from the 32-bit formats to a 64-bit format won't be easy, especially given how many embedded (""Internet of Things"") devices are expected to be around by then. Sneaking an unupgraded date format through in the guts of a smart-dishwasher is a whole lot easier to imagine than an iPad or laptop. But we also don't have much of a choice.",,Vice,"46 years ago, Unix time was born.",Happy 1.5 Billion Unix Seconds,https://www.vice.com/en_us/article/xygzn3/happy-15-billion-unix-seconds
Samantha Cole,"Update June 27, 3:03 p.m. EST: The creator of DeepNude announced that he's taken down the app. Read more, here. A programmer created an application that uses neural networks to remove clothing from the images of women, making them look realistically nude.  The software, called DeepNude, uses a photo of a clothed person and creates a new, naked image of that same person. It swaps clothes for naked breasts and a vulva, and only works on images of women. When Motherboard tried using an image of a man, it replaced his pants with a vulva. While DeepNude works with varying levels of success on images of fully clothed women, it appears to work best on images where the person is already showing a lot of skin. We tested the app on dozens of photos and got the most convincing results on high resolution images from Sports Illustrated Swimsuit issues.  Since Motherboard discovered deepfakes in late 2017, the media and politicians focused on the dangers they pose as a disinformation tool. But the most devastating use of deepfakes has always been in how they're used against women: whether to experiment with the technology using images without women's consent, or maliciously spreading nonconsensual porn on the internet. DeepNude is an evolution of that technology that is easier to use and faster to create than deepfakes. DeepNude also dispenses with the idea that this technology can be used for anything other than claiming ownership over women’s bodies. ""This is absolutely terrifying,"" Katelyn Bowden, founder and CEO of revenge porn activism organization Badass, told Motherboard. ""Now anyone could find themselves a victim of revenge porn, without ever having taken a nude photo. This tech should not be available to the public.""  This is an “invasion of sexual privacy,” Danielle Citron, professor of law at the University of Maryland Carey School of Law, who recently testified to Congress about the deepfake threat, told Motherboard. “Yes, it isn’t your actual vagina, but... others think that they are seeing you naked,” she said. “As a deepfake victim said to me—it felt like thousands saw her naked, she felt her body wasn’t her own anymore.” DeepNude launched as a website that shows a sample of how the software works and downloadable Windows and Linux application on June 23 .  Motherboard downloaded the application and tested it on a Windows machine. It installed and launched like any other Windows application and didn't require technical expertise to use. In the free version of the app, the output images are partially covered with a large watermark. In a paid version, which costs $50, the watermark is removed, but a stamp that says ""FAKE"" is placed in the upper-left corner. (Cropping out the ""fake"" stamp or removing it with Photoshop would be very easy.)  Motherboard tested it on more than a dozen images of women and men, in varying states of dress—fully clothed to string bikinis—and a variety of skin tones. The results vary dramatically, but when fed a well lit, high resolution image of a woman in a bikini facing the camera directly, the fake nude images are passably realistic. The algorithm accurately fills in details where clothing used to be, angles of the breasts beneath the clothing, nipples, and shadows.  But it's not flawless. Most images, and low-resolution images especially, produced some visual artifacts. DeepNude failed entirely with some photographs that used weird angles, lighting, or clothing that seem to throw off the neural network it uses. When we fed it an image of the cartoon character Jessica Rabbit, it distorted and destroyed the image altogether, throwing stray nipples into a blob of a figure.   In an email, the anonymous creator of DeepNude, who requested to go by the name Alberto, told Motherboard that the software is based on pix2pix , an open-source algorithm developed by University of California, Berkeley researchers in 2017. Pix2pix uses generative adversarial networks (GANs), which work by training an algorithm on a huge dataset of images—in the case of DeepNude, more than 10,000 nude photos of women, the programmer said—and then trying to improve against itself. This algorithm is similar to what's used in deepfake videos, and what self-driving cars use to ""imagine"" road scenarios.   The algorithm only works with women, Alberto said, because images of nude women are easier to find online—but he's hoping to create a male version, too.  ""The networks are multiple, because each one has a different task: locate the clothes. Mask the clothes. Speculate anatomical positions. Render it,"" he said. ""All this makes processing slow (30 seconds in a normal computer), but this can be improved and accelerated in the future.""  Deepfake videos, by comparison, take hours or days to render a believable face-swapped video. For even a skilled editor, manually using Photoshop to realistically change a clothed portrait to nude would take several minutes.  Why DeepNude was created  Alberto said he was inspired to create DeepNude by ads for gadgets like X-Ray glasses that he saw while browsing magazines from the 1960s and 70s, which he had access to during his childhood. The logo for DeepNude, a man wearing spiral glasses, is an homage to those ads.  ""Like everyone, I was fascinated by the idea that they could really exist and this memory remained,"" he said. ""About two years ago I discovered the potential of AI and started studying the basics. When I found out that GAN networks were able to transform a daytime photo into a nighttime one , I realized that it would be possible to transform a dressed photo into a nude one. Eureka. I realized that x-ray glasses are possible! Driven by fun and enthusiasm for that discovery, I did my first tests, obtaining interesting results.""  Alberto said he continued to experiment out of ""fun"" and curiosity.  ""I'm not a voyeur, I'm a technology enthusiast,” he said. “Continuing to improve the algorithm. Recently, also due to previous failures (other startups) and economic problems, I asked myself if I could have an economic return from this algorithm. That's why I created DeepNude.""  Unprompted, he said he's always asked himself whether the program should have ever been made: ""Is this right? Can it hurt someone?"" he asked.  ""I think that what you can do with DeepNude, you can do it very well with Photoshop (after a few hours of tutorial),"" he said, noting that DeepNude doesn't transmit images itself, only creates them and allows the user to do what they will with the results.  ""I also said to myself: the technology is ready (within everyone's reach),"" he said. ""So if someone has bad intentions, having DeepNude doesn't change much... If I don't do it, someone else will do it in a year.""  Better, and much worse, than deepfakes  In the year and a half since Motherboard discovered deepfakes on Reddit , the machine learning technology it employs has moved at breakneck speed. Algorithmic face-swaps have gone from requiring hundreds of images and days of processing time in late 2017, to requiring only a handful of images , or even just text inputs , and a few hours of time, in recent months.  Read more:  It's Getting Way Too Easy to Create Fake Videos of People's Faces  Motherboard showed the DeepNude application to Hany Farid, a computer-science professor at UC Berkeley who has become a widely-cited expert on the digital forensics of deepfakes. Farid was shocked at this development, and the ease at which it can be done.  ""We are going to have to get better at detecting deepfakes, and academics and researchers are going to have to think more critically about how to better safeguard their technological advances so that they do not get weaponized and used in unintended and harmful ways,"" Farid said. ""In addition, social media platforms are going to have to think more carefully about how to define and enforce rules surrounding this content. And, our legislators are going to have to think about how to thoughtfully regulate in this space.""  Deepfakes have become a widespread, international phenomenon, but platform moderation and legislation so far has failed to keep up with this fast-moving technology. In the meantime, women are victimized by deepfakes and left behind for a more political, US-centric political narrative. Though deepfakes have been weaponized most often against unconsenting women, most headlines and political fear of them have focused on their fake news potential.  Even bills like the DEEPFAKES Accountability Act , introduced earlier this month, aren't enough to stop this technology from hurting real people.  ""It’s a real bind—deepfakes defy most state revenge porn laws because it’s not the victim’s own nudity depicted, but also our federal laws protect the companies and social media platforms where it proliferates,"" attorney Carrie Goldberg , whose law firm specializes in revenge porn, told Motherboard. ""It’s incumbent on the public to avoid consumption of what we call at my office humili-porn. Whether it’s revenge porn or deepfakes, don’t click or link or share or like! That’s how these sites make money. People need to stop letting their Id drive internet use and use the internet ethically and conscientiously.""  DeepNude is easier to use, and more easily accessible than deepfakes have ever been. Whereas deepfakes require a lot of technical expertise, huge datasets, and access to expensive graphics cards, DeepNude is a consumer-facing app that is easier to install than most video games that can produce a believable nude in 30 seconds with the click of a single button. Emanuel Maiberg contributed reporting to this article. Editor's note, June 27 1:05 p.m. EST: This story originally included five side-by-side images of various celebrities and DeepNude-manipulated images of those celebrities. While the images were redacted to not show explicit nudity, after hearing from our readers, academic experts, and colleagues, we realized that those images could do harm to the real people in them. We think it's important to show the real consequences that new technologies unleashed on the world without warning have on people, but we also have to make sure that our reporting minimizes harm. For that reason, we have removed the images from the story, and regret the error.",,Vice,The $50 DeepNude app dispenses with the idea that deepfakes were about anything besides claiming ownership over women’s bodies.,This Horrifying App Undresses a Photo of Any Woman With a Single Click,https://www.vice.com/en_us/article/kzm59x/deepnude-app-creates-fake-nudes-of-any-woman
Michael Byrne,"On Monday, Intel completed its largest-ever acquisition, paying $16.7 billion for chip-maker and recent Intel partner Altera. The move, the culmination of seven months of negotiations, represents an all-in bet on what most of us already assume to be the computing future: data, the Internet of Things, intelligent systems. At the heart of this bet is a change-everything chip technology known as the field-programmable gate array (FPGA). ""We will apply Moore's Law to grow today's FPGA business, and we'll invent new products that make amazing experiences of the future possible—experiences like autonomous driving and machine learning,"" Intel CEO Brian Krzanich said in a statement. So, what is an FPGA anyhow? Sure, it's a ""programmable chip,"" but so is every other chip, right? I write code for a piece of software that is compiled into machine instructions specific for a given piece of hardware and usually I just call that ""programming."" I have programmed a chip to accomplish a new task. But that's only true in a sense. There is another level of programming: programming actual hardware. At this level, I, as a typical programmer, don't have much to do. Mostly, I'm not even thinking about the vast arrangements of wires and gates that make a chip do chip stuff, nor would I know where to even begin changing all of that chip stuff and then writing new programs for my new chips. Seems like a good way to have your head explode. But that's just what an FPGA allows: the retooling of physical hardware via software. The fundamental technology has been around since the mid-1980s ( in a primitive form ) and it's employed nowadays mostly in high-performance math-heavy roles: radar systems, missile guidance, MRI machines. Here, an FPGA can be very, very fast. Generally, this speed boost comes via parallelism. Parallel processing is a very different world from the one we're used to, which is based on sequential processing. Our everyday computers step through instructions one at a time and are always waiting for the results of some prior calculation to do the following calculation. For example, if a statement is true, the computer will do one thing, and if it's false, the computer will do another thing. Sequential computing can involve a lot of waiting around for certain events. Image: llnl.gov Parallel computing, not so much. Here, many pieces of data can be computed at one time so long as they're not interdependent. A classic example of this is in image editing, where some software might make a single change applying to all of the thousands of pixels in an image at once. Because this operation is applied to each pixel independently, it makes sense to do them simultaneously. This is the whole point of a GPU: massively parallel computation. An FPGA is, as the name would imply, a big old list of gates‐AND gates, NOT gates, XOR gates... the whole family‐a line of digital turnstiles stretching around the entire globe. These gates are designed to do something to a piece or pieces of data, and they're able to do it all at once. This is in contrast to a normal sequential computer processor which would instead iterate through that array of gates, doing one operation at a time. So: the parallel FPGA just takes a single step through the whole line of gates, while the sequential processor has to walk around the whole globe, going one turnstile at a time. The present is prime-time for parallel computation, so Intel's move makes all kinds of sense. (It had already been collaborating with Altera.) For one thing, FPGAs can handle a tremendous amount of input/output operations at once—handling sometimes thousands of input/output pins per single chip—which is good news for Internet of Things applications. It's also good news for processing large amounts of information, generally, and if the near future of computing promises anything it's lots and lots of data about absolutely everything, from stunning video game worlds to the finest structures of deep space.",,Vice,Why its acquisition of field-programmable chip-maker Altera matters.,Intel Bets $16.7 Billion on the Massively Parallel Future of Computing,https://www.vice.com/en_us/article/nz7vmd/intel-bets-167-billion-on-the-massively-parallel-future
Michael Byrne,"Last weekend I incurred the understandable wrath of many a programmer by suggesting that new coders learn coding ""the hard way"" via the C programming language rather than the language most of the universe wants them to learn, which is JavaScript. While a lot of readers wrote in to say that I was full of shit, I've also been getting a steady stream of emails asking: How do I learn C? Read more: Know Your Language: C Rules Everything Around Me  Good question! Given the completely overwhelming glut of online learn-to-code classes, it's not always obvious. I'll try to give some answers below.  Community college I started studying computer science at Portland Community College. My instructors were enthusiastic, interested in my success (as an individual), and almost all of them came from the software engineering industry. This is where I learned C. Results may vary, of course, but your local community college is worth investigating. My classes were next to free and were designed to be foundations for later CS learning. I wouldn't have done it any other way. edX CS50: Introduction to Computer Science This is Harvard University's Introduction to Computer Science and, in addition to the edX verified certificate ($90), there's a for-credit variation. Looking through the syllabus and reviews, this course isn't fucking around. Effort-wise, the CS50 packs in nine problem sets with each one requiring 10 to 20 hours of work, and a final project. If that seems like a lot (it shouldn't), it's because the course covers a lot. It starts with C and basic computer science stuff (algorithms, data structures, computer organization), but progresses into more on-the-ground topics like security, web development, and even artificial intelligence. C isn't the only language taught here either, and by the end you'll have a rough understanding of web and database languages too (HTML, JavaScript, SQL). Learn code the hard way So, yes, there is an actual C course that's just called Learn C the Hard Way . It would seem to live up to its name, offering a series of 52 exercises covering a vast field of knowledge from basic stuff like loops and control structures to designing your own TCP/IP client. It's meant for professionals relearning the C language, programmers experienced in other languages (especially a C-based language like C#), and anyone else who ""likes a challenge."" There are no cut corners here, and, given that's why we're learning C in the first place, this might be your best bet. This is from the LCTHW introduction, to give some idea: ""Our computer and the Operating System controlling it are the real tricksters. They conspire to hide their true inner workings from you so that you can never really know what is going on. The C programming language's only failing is giving you access to what is really there, and telling you the cold hard raw truth. C gives you the red pill. C pulls the curtain back to show you the wizard. C is truth."" Indeed. Become a maker The tiny computers powering the things of the Internet of Things and the clever machines of the Maker movement are largely programmed in C. For example, the ""Arduino language"" in reality is a more user-friendly wrapper above the C language—an additional layer of abstraction. Arduino boards and their more utilitarian kin are what have historically been known as embedded systems or real-time systems. Because they come with limited computing resources (especially memory), they rely on carefully optimized software with the capability of directly controlling computing hardware at low-levels. As much abstraction as possible must be peeled away and this is what the C language and the suite of language extensions known as Embedded C offer. So, build a robot or something. Eventually, you just won't have a choice but to learn C. Learn Unix C was invented to implement the Unix operating system in the 1970s and, while C has become a cross-platform force of nature, it remains tightly bound to Unix. C allows engineers under-the-hood operating system access in ways that can add up to very powerful programs. There's really no way to understand Unix (and operating systems generally) without understanding C, and you could probably argue the inverse as well. There are a billion other ways to learn C, of course, whether it's via other online classes or more self-directed approaches. The C for Dummies book gets a surprising amount of love, while CProgramming.com is both a standard reference and a good storehouse of tutorials. The C Programming Language , often known just as the ""K&R,"" remains the language's bible even four decades after its first publication. The best way to learn something is to just do it. You can read and watch videos all day and all night but it won't make much difference if you're not down in the shit yourself writing code. This is probably the best advice I can give about learning C or any other language.",,Vice,"The foundational programming language isn’t easy, but there are a billion resources to get you started.",How To Learn Programming in C,https://www.vice.com/en_us/article/yp3dzx/how-to-learn-programming-in-c
Michael Byrne,"Udacity and other online education platforms still face a lot of skepticism from within the technology industries they target. A common refrain is that they're unproven . There's some standardization as to what a college class or college degree means in terms of effort put forth and comprehension gained, but a Udacity ""nanodegree"" or Coursera specialization track is still a question mark for hiring managers. A sentiment I see on discussion boards is that MOOC coursework shows interest in a topic or field, but not necessarily proficiency.  Proficiency is what many MOOC programs are attempting, however. While the earliest MOOC-type programs, like Stanford's initial run of three courses in 2011, felt much more like organized self-learning than the formatted, rigid learning framework of a college class, things evolved quickly. As platforms like edX, Coursera, and Udacity emerged, so did a desire to directly challenge the establishment. Part of that desire was also to make money, e.g. provide an education product that people will want to buy.  That's about where we're at still. Udacity's latest effort at legitimizing its nanodegree programs, which offer credentials in various specialization suites in a way somewhat analogous to a college certificate program, is its just-unveiled ""job guarantee."" If the nanodegree-holder hasn't secured a job in six months after program completion, Udacity will refund the cost of the program ($1200 to $2400, depending on how long it takes the student to complete the program). It usually takes between six months and a year to complete a program.  The announcement that went out today reads:  Does this mean we will succeed? In the last year, we have quietly placed students in new jobs. Some of our graduates work at companies including Google, AT&T, Amazon, Nest, Goldman Sachs and others. Several of our graduates work for us now as global mentors, graders, or contract engineers. In addition, Google has invited our top graduates to their campus for an all-expenses-paid summit, and AT&T has reserved 100 new jobs for Udacity alumni. Based on our current placement data, we believe we can comfortably (and proudly!) offer this sweeping new guarantee.  I've taken several Udacity courses, and have fallen into a pattern of mirroring my college coursework with equivalent Udacity work, where it exists, as a complement. It's mostly really good! I don't think it quite compares to college coursework (it's easier and generally shallower), and I've found that the content/demands can be a bit soft compared to edX and Coursera (problems tend to feel less like problems than checkboxes), but someone driven enough should be able to get enough out of a program for it to count in the real world.  So I don't think a job guarantee is bullshit in the sense that the skills gained in the program aren't bullshit. The burden remains on Udacity, however, to convince its skeptics. This might just be a matter of time.",,Vice,MOOC hardball.,Udacity Is Now Offering a ‘Job Guarantee’ for Its Nanodegree Program Grads,https://www.vice.com/en_us/article/9a3yw3/udacity-is-now-offering-a-job-guarantee-for-its-nanodegree-program-grads
Michael Byrne,"One can argue that there's no such thing as an inherently shitty language. All languages have their frustrations and they all have their proper domains, which are sometimes more general than others. In other words, every language is a shitty language given the right context. It's also been said that there are two kinds of programming languages: ones that people hate and the ones that no one uses. The really, truly bad languages don't get properly loathed because really, truly bad languages are left to rust like the broken tools they are. That said, some programming languages see much more hate than others. Perl Perl is sometimes semi-jokingly referred to as a ""write-only"" language . That is, its syntax is so bizarre that code written in Perl is essentially unreadable by other programmers and thus not editable by other programmers. This sort of code is sometimes referred to as ""line noise."" Perl is a fairly old language with roots tracing back to 1987. It was designed to be a supercharged Unix scripting language, e.g. a language meant to string together Unix commands and pre-built software components into useful new pseudo-programs called scripts. A scripting language is often thought of as a glue useful for connecting different programs together or an automation tool. (You can read more about scripting vs. programming here .) Perl has been referred to as the ""duct tape of the internet"" because of its role in interfacing between web servers and web applications. Here's how Tim O'Reilly puts it in ""The Importance of Perl"" : A good scripting language is a high-level software development language that allows for quick and easy development of trivial tools while having the process flow and data organization necessary to also develop complex applications. It must be fast while executing. It must be efficient when calling system resources such as file operations, interprocess communications, and process control. A great scripting language runs on every popular operating system, is tuned for information processing (free form text) and yet is excellent at data processing (numbers and raw, binary data). It is embeddable, and extensible. Perl fits all of these criteria. So, yay Perl after all? Most people that actually use Perl like Perl. It has a steep learning curve as there are lots and lots and lots of different ways of doing things—things which may mean other things in different contexts—and it comes with some seriously anxiety-inducing syntactical idiosyncrasies. This partially has to do with the language's enthusiasm for what are known as sigils, which are just various symbols (like $, @, %, etc.) used to indicate a variable's data type or scope (what its meaning is within a certain context). What can make Perl really look like noise is its usage of regular expressions. This is a whole other huge topic, but a regular expression is basically a way of identifying a pattern in some text or a string of text. It's a deep, weird topic and it might just result in code that looks like this : sub deleteDoubleDots($) { while($_[0] =~ m/\.\./) { $_[0] =~ s/\/[^\/]*\/\.\.//; } } ... which looks a lot like Brainfuck code . C++ Somewhere back in my computer science education I had a real goofball of a professor, a climate change ""skeptic"" and a hater of the C++ programming language. This was early enough that I didn't even know C++ had such haters. I thought it was a great language, which, in retrospect, had a lot to do with my not knowing any other languages. I still think it's a great language, but I have a much better handle on what makes it so. Said professor had a popular joke posted on his website, and I'm fairly sure he didn't realize or believe that it was a joke ( he wouldn't be the only one ). It consists of a fake interview with C++ founder Bjarne Stroustrup in which Stroustrup admits that the existence of C++ is really just a big conspiracy-joke-thing, a means to create high-paying jobs for programmers. ""Well, one day, when I was sitting in my office, I thought of this little scheme, which would redress the balance a little,"" Stroustrup allegedly said. ""I thought 'I wonder what would happen, if there were a language so complicated, so difficult to learn, that nobody would ever be able to swamp the market with programmers.""  While the interview is a hoax, it touches on real complaints about C++. Many of those have to do with its divergences from C, including an abandonment of the system calls that nicely bind together C and the Unix operating system; the offspring language's more abstracted approach to system memory; and many things to do with C++'s handling of the programming abstractions known as objects. In the eyes of its detractors, C++ is needlessly complex, a nightmare to debug and even just read. And yet it also remains a very, very powerful language and one that still enjoys wide use. It's a relatively high-performance, efficient language which has made it a de facto language for everything from space probes to implementing other programming languages. C++ is biased toward system software, e.g. software the provides services to other software rather than directly to a user, and, as the Internet of Things continues to unfurl and populate the world with a superabundance of small computers of very limited resources, the C++ future is about as assured as anything else in computer science. Java My introduction to Java occurred via the same professor as above. While he hated Java too, I was perfectly capable of disliking it on my own. Java feels cheap from the second you start downloading its software development kit (SDK) from Oracle, the language's corporate parent since 2010. During this process, Oracle will try to sneak in downloads of the Ask.com toolbar and the Ask.com shopping toolbar, which have nothing to do with Java or software development nor are they anything you would ever willingly download as a software developer or just a regular consumer. They're just run of the mill foistware . And then there's Java itself. One of the most common complains is its verbosity. It's the opposite of elegant, a Ford Expedition to Python's Tesla. Java: class HelloWorld { public static void main(String[] args) { System.out.println(""Hello World!""); } } Python: print ""Hello World!"" To many, the verbosity of Java translates to code readability, which is fair enough, but it really depends on how it's being used. Java's syntactical bloat would have more place in a software mega-project, where many people are working on one thing together for an extended amount of time, than a quick script. Read more: The Slow-Flickering Star Death of Java There is also the Java Virtual Machine. Java code isn't compiled directly to machine instructions like C or C++, but is instead compiled into bytecode that is then fed into a freestanding program that then interacts with the machine. The relationship is a bit like how code runs in a web browser and it represents an thick additional layer of abstraction and, thus, computational overhead. The advantage, however, is cross-platform compatibility. Java will run more or less the same on the same virtual machine, whether it's running on OSX or Windows 10. Visual Basic .NET Visual Basic is one of the two core languages (the other being C#) behind Microsoft's .NET framework, an originally Windows-specific runtime environment allowing potentially many different languages to run on a single virtual machine (the Common Language Runtime or CLR) in a way similar to how Java runs on the Java Virtual Machine. VB .NET was originally intended to be a successor to Visual Basic, an earlier now-legacy Microsoft language that allowed programmers to create GUIs via drag and drop tools without having to worry about syntax/actual code. VB .NET offers completely bonkers syntax while being essentially the same under the hood as C#, which is so far a much more sane (and C-like, obviously) entrypoint into the CLR universe. Microsoft open-sourced .NET in 2014, including the VB and C# compilers, which means that both languages can now be developed cross-platform. This was a major limiting factor for VB, so we'll have to see how things evolve from here. PHP PHP is a scripting language meant to run on servers. It's responsible for a huge portion of what we now understand the web to be, which is a great big place of interactive web-resident applications. A place of data manipulation. Website interactivity now comes via a number of different languages and mechanisms, but PHP was there at just the right time to light the way. From a recent Know Your Language on PHP : It's not hard to find literature about ""why PHP sucks,"" but one of the key issues is inconsistency, both internally and with respect to other languages. It's unpredictable and inelegant. It's like slang, but without a formal, ""proper"" language underneath. Or a toolbox filled with esoteric and half-broken tools that will still be able to fix most things, but only in the most difficult and-or unpredictable manner possible. PHP is also incredibly insecure . PHP has done so much, but it would seem that it can only take us so far. Python I hated Python before I finally gave in and really liked it. Why? Two reasons. The first is that it ditches the curly braces employed in almost every other C-like programming language to group statements together. In C++ or Java (etc.), if you were to make it so that some stuff happens if some condition is true, you put all of that stuff inside of some curly braces after the condition to be evaluated. This makes sense . Python ditches the curly braces, and instead does the same thing with indentation. The stuff that's supposed to happen if the condition is true is all just indented after the conditional ( if , while , for , etc). No braces, just spaces. This also makes sense . Python has a lot of these sorts of twists, all of which exist to make code cleaner and more intuitive. It took some convincing, but, yeah, Python rules. There are rants about absolutely every programming language in existence out there, but these are the most common. Some will probably stick around on this list for many years to com, but as sure as new programming languages will be created, there will be more languages to hate.",,Vice,From Perl to Python to Java.,A Catalog of Programming Languages that Programmers Hate,https://www.vice.com/en_us/article/3daqq8/a-catalog-of-programming-languages-that-programmers-hate
Michael Byrne,"It's the new year, which means many of us are making resolutions to be better, healthier, less-bad, or just generally more valuable human beings. One popular resolution is (again) learning to code. With an ever-multiplying field of e-learning websites, web dev bootcamps, and other for-profit entities invested in the teaching of coding, this is a continuous, relentless year-round push, but the New Year is a magic time for the cultivation of casual learners. Learn to code: It's easy! From edX : ""Become the genius that designs, controls and powers the digital devices, websites, mobile apps and more that we interact with every day."" It's tempting, right? I've taken several computer science and programming courses from edX, usually to brush up on something or to supplement a college class (and rarely to completion), and they were pretty great. I've also taken courses from Coursera, Udacity, Stanford's Lagunita (based on the edX platform), and MIT OpenCourseWare (which is more of a passive open-ed platform). Those are great too and all can be had for the low price of $0 (though you can often pay money for upgrades, like certificates and ""nanodegrees"" ). So, it's all right there for the taking. Should you do it? Should you learn to code? No. At least no not in the usual sense of ""learning to code."" You should learn to code eventually and sooner rather than later, but the wrong course to start your coding education with is the introductory JavaScript course that will teach you some basics of syntax and how to use that syntax to do a few neat things to websites. There are a lot of these courses out there, and if you were to Google ""learn to code"" right now that's most of what would be returned: Intro to JavaScript. Don't take that class. If you want to make websites, learn JavaScript eventually, but don't start with it. Once you have some foundation in how code works, in how programming languages behave and interact with computers and the web, JS will be nothing. You might even choose to avoid it and front-end web development altogether. The difference between learning to code and learning about code is obviously pretty fuzzy. You need to use code to learn about it, but there are better and worse ways of doing this. The better ways are going to more difficult, less immediately applicable, and they are going to take more time. I'm sorry, but it will be worth it in the end. I learned to program in C at a community college and I wouldn't have done it any other way. As is common in undergraduate computer science programs, this introduction was spread across two consecutive courses in two consecutive academic quarters. I had little interest in computer science at the start—I was chasing an electrical engineering degree and the CS courses were requirements—but I wound up hooked, changing my track from EE to computer science and eventually transferring to a four-year school and, eventually, getting accepted into a computer science graduate program. If you want to make websites, learn JavaScript eventually, but don't start with it. I wasn't hooked on programming language syntax or manipulating programming libraries and frameworks. I wasn't hooked on jQuery or Bootstrap. I was hooked on problem solving and algorithms. This is a thing unique to programming: the immediate, nearly continuous rush of problem solving. To me, programming feels like solving puzzles (or, rather, it is solving puzzles), or a great boss fight in a video game. Eventually, you just start thinking a certain way to the point that it even invades your dreams. That's a weird one: dreaming about algorithms. To learn about coding, to understand coding, you should learn a language like C—ideally C itself, but C++, Java, Python, or another common general-purpose programming language would be chill too. The point is to learn programming as it is nakedly, minus as much gunk and fluff that can possibly be removed from the experience. That's what C is—a close layer affixed atop physical computing hardware, which still matters. You can break C pretty easily, particularly when it comes to managing computer memory. It's not foolproof by any means and many C-like programming languages exist expressly to avoid the errors made possible by C, such as buffer overruns. The thing is that this stuff doesn't go away if you're programming with a more user-friendly language, it's more just hidden. Eventually, as coding becomes less about learning and more about doing, that will be pretty helpful. But low-level things like memory and data types are fundamental to programming machines. (With the advance of the Internet of Things and the resource-limited computers behind it, dealing with memory and optimizing code will remain important in many practical senses, but that's another post.) The point is mostly that all other things in code are built from fundamental constructs: control structures, looping/repetition mechanisms, data structures. Even if your plans are limited to making the dopiest template websites, you should probably understand how to put together a linked list (a construct that doesn't exist in JavaScript because JavaScript eschews one of the most fundamental things in programming: pointers, or variables that point to specific locations in a computer's memory). To restate, you should learn first the ideas behind building things with code. Know that they're there. It doesn't have to be C; C just happens to be the canonical example of what I'm going for here. You could build a giant ass bridge with prefabricated materials and a set of detailed instructions without understanding a thing about the physics behind what makes a bridge not collapse, but I'm not sure I want to drive over that bridge. So, my general advice is to take an introduction to computer science course, from edX or wherever. It will probably be based on Python, which is fine (Python is a great language). You won't wind up with a cool website at the end. More likely, you'll have a command line program that can check if a number is prime or not. Something dry like that. Do that first and you'll be a thousand times better off in whatever code/programming endeavor you choose to pursue after. Just do it the hard way, if only at the beginning.",,Vice,At least relatively speaking.,In Defense of Learning Code the Hard Way,https://www.vice.com/en_us/article/53dpeq/in-defense-of-learning-code-the-hard-way
Michael Byrne,"Java is about the least buzzworthy programming language going. It is reviled by many, and rumors have swirled that its corporate parent, Oracle, isn't especially interested in the language's future. And yet, according to the TIOBE index's popularity ranking system , Java sits comfortably as the most popular programming language in all of the land. Not only that, but it's only become more popular, gaining about 6 percent share since 2014 and unseating C from the most-popular throne. The similar PYPL index also places Java at the top, but followed by Python instead of C. The ranking represents a Java resurgence. In the early-00s, the language held the top spot quite comfortably, only to be unseated in 2011 by C. From TIOBE: At first sight, it might seem surprising that an old language like Java wins this award. Especially if you take into consideration that Java won the same award exactly 10 years ago. On second thought, Java is currently number one in the enterprise back-end market and number one in the still growing mobile application development market (Android). Moreover, Java has become a language that integrates modern language features such as lambda expressions and streams. The future looks bright for Java. The TIOBE rankings are based on simple search engine statistics. 25 of the most popular search engines receive this query: +"" programming"" The rankings are assigned based on the number of hits each language returns. Simple enough. The PYPL index is a bit different. It's based on searches for programming language tutorials. The more a programming language tutorial is searched for, the more popular the language is taken to be. Again, simple enough. No other language even came close to Java in the TIOBE ranking growth-wise. Python, Visual Basic, and a Java-platform language called Groovy all increased by more than a single percentage point, but that's it. The only comparable change occurred in the other direction with a 5 percent fall in popularity for Objective-C, Apple's pre-Swift language for OSX and iOS development. This makes good sense as Objective-C has now been effectively replaced. So, is this a sign that you should be learning Java instead of Swift or JavaScript? Probably not, or at least not as your number-one learning focus. I can say that if you already know a general-purpose programming language like C++, you're more than halfway there. And Java is still taught as an introductory language at a lot of schools. I'm no Java expert, but I'm glad to have enough working knowledge that I could ease into it fairly painlessly.",,Vice,"After a brief reign, C is unseated. Meanwhile, Objective-C plummets.",2015's Most Popular Programming Language Was Good Old Java,https://www.vice.com/en_us/article/78kvv9/2015s-most-popular-programming-language-was-good-old-java
Leif Johnson,"Many people who play Minecraft limit themselves to using its building blocks more or less for their intended purpose, whether it's by building simple houses or creating sprawling, unforgettable underwater wonderlands. Yet YouTuber SethBling has repeatedly found them well-suited to higher callings. For his latest project, he used command blocks to cobble together a functional interpreter for the BASIC programming language that works within the confines of vanilla Minecraft's existing ruleset. It's not exactly fast, and it slows down with continued use. SethBling partly attributes the sluggishness to Minecraft's 20 Hz refresh clock, which limits the command blocks to operating 20 times per second. But it certainly works, as he demonstrates by running a program that prints out prime numbers on a gigantic whiteboard with the help of an onscreen keyboard. Later, he runs a script that allows a Minecraft ""turtle"" to mine a tunnel without his supervision (and without mods or add-ons, for that matter). He never shows it, but the turtle reportedly can place blocks as well. Minecraft players have been getting ""computers"" to work in the game for a while, but such efforts usually focus on the OpenComputer mod and the Lua programming language. Last November, in fact, we showed how one player had managed to control lights in the real world with an in-game switch. One major exception, though, was Laurens Weyn's massive "" Commodore 32 "" computer from 2014, which uses command blocks much like SethBlind's creation. In SethBling's case, programming the many, many command blocks involved took him around two weeks, and he wrote the code for each block by hand. If you're interested in trying it out for yourself, though, you won't have to put in that kind of time as the whole package is available for download via SethBling's website .",,Vice,YouTuber creates a BASIC interpreter in Minecraft with the help of command blocks,YouTuber Builds Working BASIC Programming in Minecraft,https://www.vice.com/en_us/article/pgkbm8/youtuber-builds-working-basic-programing-in-minecraft
Michael Byrne,"TrumpScript, tagline ""making Python great again,"" is Donald Trump in a programming language. The creation of four computer science students at Rice University—who put it together in a 36 hour hackathon—it's surely a novelty language, but it's also a very perfect novelty language, embodying the nonsense, bombasity, and even the frothing white nationalism of the actual presidential candidate. Here are the features of TrumpScript, from the TS GitHub page : No floating point numbers, only integers. America never does anything halfway. All numbers must be strictly greater than 1 million. The small stuff is inconsequential to us. There are no import statements allowed. All code has to be home-grown and American made. Instead of True and False, we have fact and lie. Only the most popular English words, Trump's favorite words, and current politician names can be used as variable names. Error messages are mostly quotes directly taken from Trump himself. All programs must end with ""America is great."" Our language will automatically correct Forbes' $4.5B to $10B. In its raw form, TrumpScript is not compatible with Windows, because Trump isn't the type of guy to believe in PC. The language is completely case insensitive.  There's some other stuff in the language syntax, such as replacing the equality (""="") operator with the words ""is"" and ""are."" Meanwhile, all of the arithmetic operators can be employed using natural language, e.g. substituting ""plus"" for ""+"" and ""minus"" for ""-."" As a result, you can wind up with fun programs like this: I is Donald Trump it time to tell you a fact for every american there is 100000000 immigrants trust me they are , there over there ; plan against us if, I are fact? ; : say they want to take our guns ! America is great. Under the hood, the language is Python. That is, compiling a file of Trump code with the ""TRUMP"" command will interpret the input file into proper Python and it will run, perhaps even when it shouldn't. Trump code, much like the man himself, isn't always honest. As the Trump documentation explains, ""Most importantly, Trump doesn't like to talk about his failures. So a lot of the time your code will fail, and it will do so silently. Just think of debugging as a fun little game."" Sam Shadwell and Chris Brown, two of the language's creators, explained a bit further in an interview with Inverse : What about TrumpScript makes you most proud? Sam: I kind of like the way that it works — if it actually worked, completely, which it…doesn't — Chris: There's some stuff — I'm working out some kinks. Sam: I'm most proud that the language itself is actually parsed and compiled in a way that sort of resembles how a real compiler would compile a real language. Chris: In what I would consider proper TrumpScript style: 80 percent of the words you write should probably get dropped by the compiler and be completely unnecessary to the program. As for the future, the group wasn't expecting quite this much attention and right now there's a queue full of amusing feature requests at the project's GH page. For example: ""TrumpScript should ship with Woman object with appropriate methods pre-defined."" Or: ""can I deport a module?"" This joke could probably go on for a while.",,Vice,A group of computer science students wants to make Python great again.,TrumpScript Is Donald Trump in a Programming Language,https://www.vice.com/en_us/article/3daqvj/donald-trump-in-a-programming-language
Michael Byrne,"The current search engine for programming language syntax is Google. Knowing how to search for information is a key skill in knowing how to program at all. You can know all of the algorithms and a half-dozen programming languages inside-out, but you will nonetheless be searching for how to do something at some point, whether it's related to some brand-new or super-obscure functionality or to how to translate some feature or another in one programming language to another language. In other words, knowing how to program has a lot to do with knowing how to access information—an acute awareness of how and when to learn. This learning might occur in hyperdrive if you're the sort of programmer that's either obsessively learning new things just for the sake of it—which is a whole lot of programmers—and-or has to learn new things to apply them to a new project or task. For a recent project, for example, I needed to use a machine learning framework that's implemented in a kind of obscure language called Lua, which is like a super-lightweight version of Python. I watched a couple of videos, but mostly I was inferring syntax from other Lua code and Googling things like ""Lua for loop break."" A computer engineering student at Queen's University in Ontario named Anthony Nguyen has released what he hopes will replace Google for the syntax searching needs of software developers. It's called SyntaxDB , which Nguyen hopes will, ""one day become the world's fastest programming reference."" I did some cursory searching and the SyntaxDB interface is pretty nice. Searching, say, ""Java iterator"" gives a few results for Java control flow structures and a sidebar menu for further exploration within the Java language. Those results are all internal to SyntaxDB and lead to reference materials written exclusively for the database. It's more like a multi-language quick reference manual than a search engine. As for those results, I didn't actually get a page for Java iterators, which seems bad and also seems like a limitation of relying on in-house content rather than trying to exploit the bottomless stockpiles of programming language documentation that already exist on the internet. Just writing your own reference materials seems easier in the short-term, but maybe not all that sustainable. The big advantage of SyntaxDB that I can see is in the brevity of the documentation. An entry on for-loops isn't going to give you every detail, or most any details at all, just what one looks like and its basic usage. And that's often all you're looking for—what a known concept or feature looks like in an unfamiliar programming language. SyntaxDB has an integration with the general internet search engine DuckDuckGo that seems more useful than using SyntaxDB directly. Here, you can enter a syntax search term in the normal search bar and DuckDuckGo will give you the quick and dirty SyntaxDB documentation, if it exists, along with the usual list of internet search results. If it doesn't exist, DuckDuckGo will find the next best thing. Will I use it? Mayyyybe. I'm already pretty good at cultivating quick and dirty answers from complicated documentation, and, like a lot of people that write code, I also often enough turn to the the vast stockpile of ad hoc documentation found in the programmer Q&A forums of StackOverflow. That's something Google can give me that I'm not sure could ever be replicated by SyntaxDB.",,Vice,Coding very often means Googling.,A Search Engine for Programming Language Syntax Is a Pretty Good Idea,https://www.vice.com/en_us/article/4xam5d/a-search-engine-for-programming-language-syntax-is-a-pretty-good-idea
Michael Byrne,"The internet is  overfull  with  polls and excitable blog posts about this month's most popular programming language omg , wherein the programming future is divined from metrics like internet search frequency, Github forks, and Stack Overflow appearances. What these always seem to miss is that selecting a programming language often isn't a real choice—language use is based less on naked preference than on questions like what language best fits a particular problem domain (C for embedded systems, say, or Javascript for web development), or what language an existing code base is written in, or what language some API supports. Languages are chosen by circumstance. But this is less so when programmers are off the clock. There aren't a whole lot of professions whose practitioners go home after work and do more work just for fun, but programming is a world of side projects—the job is the hobby and the hobby is the job. The appeal of side projects is, naturally, freedom. Off the clock, we can build whatever the hell we want, whether it's meant to be the seeds of a startup, a new electronic music instrument, or just a fun game. Off the clock, it's also more likely that we can pick and choose programming languages based on which ones we actually like. There are some of the same constraints, of course, but if we really, really wanted to, say, build a web application in C++, we could. It's with this in mind that Julia Silge, a data scientist at Stack Overflow, decided to chew through the site's vast data trove for programming language questions appearing only on the weekends vs. those appearing on weekdays (work days). The results are pretty interesting. Silge and colleagues looked at 10,451,274 questions posted to the site on weekdays and 2,132,073 questions posted on weekends. From this pool, they gleaned about 10,000 total tags in use and then conducted a frequency analysis on those tags. Results are below. The left, weekday side is dominated by, well, uncool technologies. These are often proprietary (closed-source) things likely to be in a big corporate environment. Sharepoint is a team collaboration tool; Oracle focuses on enterprise software (that is, software for big organizations); Excel is Microsoft's spreadsheet program; Internet Explorer is Internet Explorer. Five tags on the left are related to Microsoft. The right side is dominated by academic-friendly technologies and platforms well-suited to rapid prototyping, such as Heroku and Google App Engine. The leading tag on the right, Haskell, is a challenging functional language, a wicked satisfying programming paradigm that happens to be very in right now. Meanwhile, assembly is a generic term for the very low-level languages representing the last human-readable step before code becomes machine code. It's likely to be used in embedded environments where developers are writing code directly for hardware; one might write assembly code for an Arduino board, for example. OpenGL is a graphics programming interface used in making video games, VR, and other things you'd probably consider to be more fun than hacking on spreadsheets. Python is always a good time. Here's another perspective, which is how the weekend/weekday ratio has changed over time for certain technologies: Both Ruby and Rails and Scala have declined in weekend use and increased in weekday use, indicating that these technologies are bleeding through as more developers become interested in them and understand their ""real-world"" utility. On the flip side, ""If we look for the tags that have increased the most in weekend activity, we see the game engine Unity3D, as well as a number of tags used for building mobile apps,"" Silge writes. ""It looks like developers are designing more games and apps on the weekends now than in previous years.""",,Vice,A programming language popularity poll that matters.,The Most Popular Weekend Programming Languages,https://www.vice.com/en_us/article/ezmj5z/the-most-popular-weekend-programming-languages
Michael Byrne,"The condition of Turing completeness is almost always explained in terms of the Turing machine. Naturally. A Turing machine is simply a hypothetical black box that performs a small set of mathematical/logical operations on a strip of paper tape that is of potentially unlimited length. The box zips back and forth along the tape reading and writing data into discrete cells, eventually at some point in the future returning the answer to some question. This is what it is to be computable at all—an assurance that the machine won't run forever. Any true computer is just a simulation of this primitive abstraction. I've always had a hard time with the idea of the Turing machine. Part of the problem is that the Turing machine as described above is often described in terms of what it is —a box that does mathematical operations on cellular divisions of a strip of paper tape—and not what it means . And what the Turing machine means is that there is a condition of being able solve any computational problem given enough time and memory. For something to be Turing complete, it must be able to solve every problem solvable by any other computer that exists or that could be imagined to exist (such as a Turing machine). It may be easier to see this from the other side: a Turing incomplete computer is unable to solve some known class of problems solvable by another computer. Usually, when we talk about Turing completeness, we're talking about computers-as-programming languages, which are systems that describe how to solve different problems. Most programming languages you've heard of are Turing complete. There are no computational problems that can be solved by C and not by JavaScript or by Java and not Python or by Python and not Java. And so on. An interesting exception is SQL—the language implementing relational databases—which in its most basic form is generally understood to be Turing incomplete. This makes sense given that SQL doesn't actually exist for general-purpose computation and doesn't need features like loops and if-then conditional branching (which allows sections of code to be skipped under certain circumstances). Likewise, HTML isn't Turing complete, nor should we expect it to be. Its usage is descriptive and not computational. An interesting thing about Turing completeness is that it's not all that hard to achieve. In fact, it exists in the world all over the place just through accident. Bitcoin and darknet researcher Gwern Branwen has published a small catalog of ""surprisingly turing-complete"" constructs that lead to some curious and worrisome security implications. ""One might think that such universality as a system being smart enough to be able to run any program might be difficult or hard to achieve, but it turns out to be the opposite and it is difficult to write a useful system which does not immediately tip over into TC,"" Branwen writes. ""It turns out that given even a little control over input into something which transforms input to output, one can typically leverage that control into full-blown TC. This can be amusing, useful (although usually not), harmful, or extremely insecure [and] a cracker's delight."" Some examples include Magic the Gathering, CSS, and common musical notation. In the case of Magic, this is only true assuming an endless sequence of cards that force player moves, thus eliminating player choice. The mechanics get pretty deep . CSS—cascading style sheets, which add information about the appearance of webpages—meanwhile, becomes Turing complete when we include user clicks into the system and, thus, the ability to change the system's state. Otherwise, CSS, as a descriptive language, mostly just sits there, like HTML. With some slight tweaks, common musical notation can be converted into the esoteric, Turing-complete programming language Choon . A 2013 paper published by University of Cambridge computer scientist Stephen Dolan, and cited by Branwen, offers a contender for shortest computer science paper title ever: ""mov Is Turing Complete."" He's referring to an assembly language instruction—a hardware-level command, basically—that moves a unit of data from one memory address to another. It's one of a very long list of such assembly instructions, but what Dolan showed is that every other instruction can be reduced to this one action. It's wild. The security implication is maybe not obvious. Some large part of computer security, generally, is evading and defending against malicious code that might gain entry into your system and do unwanted things. Doing so requires being able to identify such code as code, and not, say, Pokemon and or human heart cells . Computation could be anywhere.",,Vice,"From Magic the Gathering decks to CSS, computers are everywhere. ",Turing Completeness Is Where You Least Expect It,https://www.vice.com/en_us/article/aeqg54/turing-completeness-is-where-you-least-expect-it
Michael Byrne,"According to survey results released earlier this month, software developers are on average a ""slightly happy"" group of workers. For employers, this is slightly good news as the happiness—or lack of unhappiness—of workers is naturally, obviously tied to productivity. The findings are described in a paper posted last month to the arXiv preprint server and are slated to be presented in June at the Evaluation and Assessment in Software Engineering Conference in Sweden.  ""A practice that has emerged recently is to promote ourishing happiness among workers in order to enact the happy-productive worker thesis,"" Daniel Graziotin and colleagues write in the aforementioned paper. ""Notable Silicon Valley companies and inuential startups are well known for their perks to developers. Recognizing the happiness of all stakeholders involved in producing software is essential to software company success.""  This is by now a cultural trope, of course: the coddled software engineer, toiling in between catered lunches and on-site massages while pulling in a comfortable six-figure salary. For talented and employable developers, it remains a seller's market. Indeed, happiness is crucial for retention in such a market, but the survey was a bit more interested in the idea of productivity—maximizing your HR investments in terms of both software quantity and quality. Graziotin and his team found their survey subjects via Github. Contact information was found by mining archived data for past public Github events, where email addresses are apparently more plentiful. They wound up with 33,200 records containing developer locations, contact information, and employers. They took a random sampling from this dataset and wound up with about 1,300 valid survey responses. (94 percent of respondents were male, which is bleak as fuck.) Survey responses were scored according to the SPANE-B metric, a standard tool used in psychology to assess ""affect,"" defined as total negative feelings subtracted from total positive feelings. It ranges from -24 to 24. The mean score found in the developer happiness survey was 9.05. Slightly happy. The minimum was -16, while the maximum was 24. So, even in the worst cases, employees weren't totally miserable, whereas in the best cases employees weren't miserable at all. That has to count for something. The reasons for programmer unhappiness are summarized in the table below. It's interesting that the first factor on the list is something that's just inherent to the job. Solving problems is the job, really. And, yeah, you get stuck. But getting unstuck—figuring a hard thing out—also happens to be a highlight of the job. If it wasn't, I don't think there would nearly the amount of talent in the industry just because smart people pursue challenges. So, no, programmers aren't miserable at all. ""This does not mean that software developers are happy to the point that there is no need to intervene on their unhappiness,"" Graziotin and co. conclude. ""On the contrary, we have shown that unhappiness is present, caused by various factors and some of them could easily be prevented. Our observations and other studies show that unhappiness has a negative effect both for developers personally and on development outcomes."" More massages, please.",,Vice,A new survey attempts to quantify the unhappiness of programmers.,Are Software Developers Miserable?,https://www.vice.com/en_us/article/53vb4z/are-software-developers-miserable
Michael Byrne,"For a while, people really loved complaining about Instagram filters. They make photos look different, but always in the same way. They encourage lazy photography. A cheap stylistic substitute for thoughtful composition. Etc. It was another whole big hand-wringing session about authenticity. Do people still tag things with ""#nofilter""? But to call image analogizing ""filtering"" is to sell the associated framework short. In the words of its New York University-based creators, it's ""processing images by example."" ""Rather than attempting to program individual filters by hand, we attempt to automatically learn filters from training data,"" the project's website explains . The idea: Give the Image Analogies framework three images and it will teach itself what makes the first two images similar, and then apply that similarity to the third image as a filter. The results are unpredictable and frequently very cool. Fortunately, you don't have to be a machine learning whiz to do it. That said, there is some set-up, which is the hardest part of making your own image analogy. I'll walk you through it below. We'll be using a Python implementation of the original NYU method written by programmer Adam Wentz, whose other projects include Huge Wall of Porn , gif hell , Oldstagramme , and more fun stuff. 0.0) Resources You don't need a GPU and a ton of memory to use Image Analogies, but it helps. Most of my experiments were done with Amazon EC2 instances that come with mostly all of the needed math/machine learning software preinstalled, and GPUs to run it on. (GPUs and their parallel processing abilities are key to the sorts of computations involved in machine learning.) Running on a remote machine also has the advantage of not completely inundating your own computer's processors, which can happen fast. All that said, I'm not going to explain the whole process of getting started with EC2 instances and interfacing with a remote shell via the command line. Maybe in another Hack This edition. Let's just assume for the sake of this tutorial that everything is being computed locally on the machine in front of you. That will work. 0.1) Software Image Analogies will work with either of the two big machine learning libraries TensorFlow and Theano. If you have a GPU to work with, you'll probably want the latter, while, if not, you'll need to be using TensorFlow. Another machine learning library called Keras is then needed to run on top of either Theano or TensorFlow. Yeah, I know: This is already getting to be pretty messy. But hold tight for a sec. Going forward, I'm going to assume that we're working with TensorFlow because there's actually quite a bit more to getting going with GPU support, mostly having to do with the installation of CUDA, which is the software/platform (yes, another one) that lets you use your Nvidia GPU for these kinds of computations in the first place. As for getting going with TensorFlow, you can follow the official guide here . It's not too bad and can be accomplished using pip like any other Python package. The Image Analogies framework itself can be installed using pip by the simple command: pip install neural-image-analogies . This installation should take care of all of the framework's dependencies, including Keras and TensorFlow. You might want to do this in a Python virtual environment , which will keep the Image Analogies installation from possibly breaking a dependency chain elsewhere on your system. 1.0) Weights With everything installed and theoretically working correctly, we can get to some actual machine learning. First, we need to download VGG16, which is a 16-layer convolutional neural network that's about the state-of-the-art in image recognition. This is what Image Analogies will use to make sense of our two input images. You can download a reduced form of VGG16 here . Note that it will need to be in the same directory from which you're running the Image Analogies script from. 2.0) Experiment We're basically ready to go. All that's left is picking some images. Remember, we're taking two images, comparing them, and then applying the results of that comparison to the third image. You can be pretty clever with this. From Wentz's Image Analogies Github page: Mostly, I've just been throwing images together and seeing what happens. You can wind up with some cool patterns, at least. There are also a million different options and parameters you can run this script with (see the Image Analogies Github ), which let you do everything from isolating specific layers of the machine learning model to tweaking detail levels and image scales. You could burn through an afternoon pretty easily just taking the ""throw stuff together"" approach (as below). Once Image Analogies is installed per the above instructions, it's launched with the following command: python make_image_analogy.py first-image second-image third-image filename-prefix-for-output . The filename prefix is what the script is going to stick onto the beginning of every filename that it saves to your computer. These images should be in whatever directory you're running the script from. If you wanted to save the output to a different directory, you'd just prepend that to the output filename prefix, like: /otherdirectory/outputprefix . By default, you'll get sample intermediate images saved to your computer as the algorithm chugs along. If you wind up getting sucky output images, it's easy enough just to cancel the script before it finishes. For me running this on a fairly standard-issue MacBook, each run takes about a half-hour. Running on a GPU-optimized Amazon instance knocks that down to five or 10 minutes. If you were interested in taking Image Analogies to the cloud, I'd suggest looking at the Go Deeper Amazon system image, which comes with all of the Image Analogies dependencies prebuilt and ready-to-go. It also has some pretty user-friendly documentation to get you started if, say, you have no idea what I mean by ""Amazon system image"" or EC2 or even GPU computing. Go Deeper even offers a remote desktop, which is pretty handy if you're not used to interacting with a remote computer via ssh . We're obviously just dipping a toe into something much, much bigger here. But, like Google's Deep Dream, it winds up being a good entry-point into the bigger thing that is visual processing and machine learning, generally. And it will be a good stepping off place for Hack This to go deeper too, at least sometimes. Read more Hack This .",,Vice,Getting creative with neural networks is easier than you think.,Hack This: Make a Photo Filter with Machine Learning,https://www.vice.com/en_us/article/ezp9xz/hack-this-make-a-photo-filter-with-machine-learning
Kaleigh Rogers,"When Reddit unleashed the internet on a blank canvas earlier this month , they shockingly filled it with collaborative art, rather than crudely drawn dicks. The results were so surprising , it compelled a programming student to find out the story behind the art pieces, so he found a way to preserve them all.  ""When you hear that we unleashed the entire internet on a blank canvas and they can draw anything they want, you think 'oh, this can't end well,'"" said Roland Rytz, a 24-year old computer science student at the Bern University of Applied Sciences. ""I was really impressed by the art and the degree of collaboration between everyone.""  Rytz told me he's always looking for new projects to expand his programming skills, so he built an interactive map of the canvas with annotations for each design. He calls it the r/place ""atlas,"" and it allows users to submit backstories for each piece of art on the canvas.  He published the atlas right after r/place stopped editing, and has seen received more than 2,000 entries. It not only gives an explanation for what each image represents, but also tells the history of how parts of the canvas were disputed territory, while others demonstrated collaboration.  One canvas was the center of a feud between r/France and r/de over which country's flag should take up the space. Eventually, according to Rytz's atlas, the two subreddits decided to compromise, and drew the flag of the European Union, with a dove of peace in the center.  The atlas also documents the history of the ""void:"" black pixels that took over various parts of the canvas. In one area, the void nearly won, but was replaced by the prism from Pink Floyd's Dark Side of the Moon album before r/place ended.  Rytz said he originally would fact-check every entry, but soon got overwhelmed with the information. Fortunately, he said the tool ends up getting checked by other users, similar to Wikipedia, so any wrong information is usually corrected quickly. He told me the site has drawn half a million visitors so far.  Rytz is now working on a tool that demonstrates what people with color blindness see when looking at colored images, and a 3D imaging tool that let's you view 2D MRI brain images as interactive 3D models.  ""I like to find projects that sound like fun or people can get some use out of it,"" Rytz said. ""It's just for fun.""  Subscribe to Science Solved It  , Motherboard's new show about the greatest mysteries that were solved by science.",,Vice,A programming student in Switzerland created a tool to collect all the backstories to the collaborative art pieces.,Here are All the Stories Behind Reddit’s Greatest Collaborative Project Ever,https://www.vice.com/en_us/article/3d334k/here-are-all-the-stories-behind-reddits-greatest-collaborative-project-ever
Michael Byrne,"This is the second part of a two-part Know Your Language entry on the Java programming language.   When we left off, Java was a deeply uncool programming language beyond its peak as a tool enabling early-days webpage interactivity and for developing enterprise software. We'd also left off with a promise to look closer at the language's rather utopian origins, which still matter a great deal to Java's present and future.  The Java past  Before 1995, software was very often built with C and C++. These are not idiot-proof programming languages. It's possible—easy even—to really muck things up at very low levels within a computer, such as corrupting physical memory.  In my first computer science classes, I spent who knows how many hours trying to debug memory access violations, or trying to figure out ""memory leaks,"" where some sliver of physical memory is claimed by a program but never returned to the operating system. If that program or piece of code is run iteratively (again and again), the effect is that an increasing amount of physical memory (RAM, generally) is never released back to the OS. This has potentially serious consequences when it comes to performance.  Memory leaks are sort of the canonical error associated with low-level programming. Java's answer was to add a feature called garbage collection. No longer would memory allocation and deallocation be the job of programmers. Instead, there would be another shadow program to keep tabs on things and properly dispose of no-longer-needed memory.  Where memory really matters is in systems that don't have a lot of it to spare: embedded systems. An embedded system is what we'd probably more likely call a thing in the Internet-of-Things. It's a small computer that controls some physical machine or interacts with the physical world in some specific way. The computer in a thermostat is an embedded system; an ATM is an embedded system; an Arduino board is an embedded system.  This was the original purpose of Java—controlling things like TVs and smart appliances. It began within Sun Microsystems (now Oracle) in 1991 as the Oak programming language. James Gosling, one of the language's original developers, wrote at the time: ""the goal was ... to build a system that would let us do a large, distributed, heterogeneous network of consumer electronic devices all talking to each other.""  The language's initial purpose didn't quite work out as the smart appliance dream (for a while, anyhow), so the language was retargeted at the web, where features like platform independence were still selling points. In 1995, a version of Netscape Navigator was released capable of running Java programs. You can read a lot more about Java Applets here , but for a good while these self-contained boxes running Java programs were the beginning and end of the interactive, dynamic web. Eventually, Applets would be mostly killed off by Javascript and CSS.  The Java future  The first part of this post was written in 2015 (sorry!). There have been some Java developments. To understand this, we need to understand something peculiar about Java that I haven't quite addressed. Recall that the central idea of Java code is that it actually runs on this intermediate entity called the Java Virtual Machine or JVM. This is like a ghost computer that lives between the language and the actual computer hardware below. This is where garbage collection is implemented, among other things.  The neat thing about the JVM is that it doesn't require Java. All it requires is Java bytecode. So, if you can make another programming language that can be converted into Java bytecode, then that language can run on the JVM and take advantage of its features.  Another quote from Gosling : ""Most people talk about Java the language, and this may sound odd coming from me, but I could hardly care less. At the core of the Java ecosystem is the JVM."" A short list of JVM languages that are not Java includes Groovy, Clojure, and Cotlin. Nearly every popular programming language has at least one implementation that runs on the JVM. So, there are versions of Python, Javascript, PHP, and Ruby that all run on the JVM.  But the JVM language I want to talk about is Scala.  Scala is weird and also about to be incredibly important. Its weirdness is in its functional programming features. I'll write a separate post soon about functional programming, but for most programmers used to languages like Python and C++ and even normal Java it's a mindfuck. It's really a rewrite of some of the fundamental mechanics of programming where every computation reduces to the evaluation of a mathematical function, a rule for mutating a data point x into some new data point y.    In practice, functional programming is a way of operating on very large amounts of data using very small, very simple code. A classic operation would be a map function, which takes in a list of data and a formula that specifies how to modify a single data point. The map takes that formula and applies it to every data point and then returns a list of new data points.  Something like this:  function addOne(number) = number + 1  newData = map(addOne, [1,2,3,4])  And then newData will equal [2,3,4,5]. Scala adds some features to Java that make it really good at this sort of thing. Scala works very well when it comes to doing operations on large amounts of data in relatively simple ways, which makes it a natural candidate for distributed computing. This is where computations are done on very large datasets across many different CPUs located in potentially many different locations. Because of the memory constraints of most computers, this is neccessary for a lot of big datasets. And as data grows and grows, it's becoming more neccessary. That's the problem solved by Apache Spark, a cluster-computing framework written in Scala and typically used within Scala applications. Spark is highly functional and a natural mate for Scala. Unlike fusty old Java, Spark and Scala are things that tend to get engineers excited in 2017. There's more to the Java future than Scala and distributed computing, of course. There's the Android operating system, which is built around a JVM core. That probably speaks for itself. Read more Know Your Language.",,Vice,Big Data offers Java a rebirth in functional programming and distributed computing.,Know Your Language: Java Still Matters (Part Two),https://www.vice.com/en_us/article/xwqb3q/know-your-language-java-still-matters-part-two
Michael Byrne,"According to the just-released Stack Overflow annual developer survey —a sampling of some 64,000 workers in the software industry from around the world—the proportion of new software devs entering the workforce with computer science degrees has dipped significantly. The decline is modest but notable: About 49 percent of developers entering the workforce now have an undergraduate CS degree, while about 55 percent of developers that have been in the workforce for 10 or more years have the degree. Survey respondents were recruited primarily through channels owned by Stack Overflow, such as banner ads placed on the site and blog posts. It's worth keeping in mind that the population of Stack Overflow users is an imperfect representation of the software development industry at-large. For one thing, it's pretty heavily tilted toward web developers.  A computer science degree is the classic educational background for software engineers. All in all, it's less of a strictly vocational experience than it is foundation-building. It's kind of a cliche, but I often say it's ""learning how to learn."" You probably won't write much Javascript in a college classroom, but you will learn core concepts that make it easy if not trivial to pick up new programming languages like Javascript or whatever the next cool thing is. (Among conceptually similar programming languages, the differences are after all mostly just syntax.)  So, full disclosure, I have a computer science degree and wouldn't have done it differently. But a lot of other entry-points have opened up in recent years, particularly coding bootcamps—strictly vocational programs that tend to focus on specific skillsets like frontend or fullstack web development—and massive online programs like Coursera and Udacity that offer ""nanodegrees"" and course completion certificates. Somewhere in there is self-teaching as well.  Somewhat unsurprisingly, the survey found that people with CS degrees were significantly less likely to leave the industry compared to those with other backgrounds.  Software development has itself changed over the years, which is reflected in these trends. Overwhelmingly, new developers are working on web applications rather than, say, embedded systems, databases, or desktop applications. In talking to other developers and engineers, it's hard not to think that there isn't a rift growing in the industry between web development and, well, the really hard stuff that's more likely to require a theoretical/conceptual background.  As demand spikes for engineers that can manage distributed systems, cloud architectures, data pipelines, and Internet-of-Things programming, the leakage of CS grads from the industry could have consequences. Part of the problem right now, perhaps, is that a CS degree doesn't offer too much of a payoff salary-wise. At least at the entry level, a systems or data engineer can expect to make about the same as a web developer. (That's just a personal observation, not a study finding.) The most common non-CS degrees held by those in the industry included natural sciences, math, psychology, and business.  Among the survey's other findings is this one: New developers are way into food. Like, as a job benefit.  ""New developers, those with less than 1 year of experience, are more likely to say that employer-provided meals like free lunch are important to them, as well as employer sponsorship of education, such as tuition reimbursement,"" a Stack Overflow blog post notes. ""These were options where new developers gave significantly different answers than others; new developers were 50 - 60 percent more likely to value free food and education than their more experienced colleagues. Developers with more experience are more likely to say that benefits like retirement contributions and the option to work remotely are important to them.""",,Vice,"New devs are skipping college, according to a new Stack Overflow survey.",Computer Science Graduates Are Disappearing From Software Development,https://www.vice.com/en_us/article/j5xb8p/computer-science-degrees-slowly-disappearing-from-software-dev
 ,"This is an opinion piece by Bryan Bethea, founder of Hip Hop Hacktivist, a free social programming event, that will take place on June 24 in New York, that uses the influence of hip hop culture to introduce tech industry opportunities to minority and underrepresented youths. When I came up with the idea of Hip Hop Hacktivist , my vision was to simply use what kids are familiar with to show them to something new. It's important that we speak to our young people in the language that they understand and delve into topics that resonate with them. The term ""Hacktivist""--a combination of the words ""Hacker"" and ""Activist""-- is meant to signify an activist who uses programming to impact social change.  Our event creates an educational ecosystem that has three layers of impact:  It provides an opportunity for experienced tech professionals to give back through mentorship It allows tech professionals who are new to the industry to pair program with experienced developers It introduces programming to students who have no experience  I want to use Hip Hop Hacktivist as a platform to change the negative perception and stigma of being a programmer that may exist to young people involved in hip hop culture. In low income communities, most young residents don't even think about becoming a programmer. Many, however, aspire to be rappers. Imagine the number of kids from the projects and streets of Brooklyn that you would have programming if they heard Jay-Z discuss the great entrepreneurial opportunity it offers. We will partner with members of the hip hop community to strengthen our efforts in advocating programming to youth.  We want kids to become producers instead of consumers. Most are unaware that opportunities in tech can change their circumstances tremendously.  In the future, my team and I hope to build a braintrust for Hip Hop Hacktivist that comprises key individuals who we feel understand our mission -- people who can help guide us and give us access to grow in a way that further bridges the gap between hip hop culture and technology. A few people in the music industry that we have already identified are Nas and his Queensbridge Ventures Partners firm, Will.i.am with his work over at Intel, and even Chamillionaire with all of the activity he has had in the tech space. My team and I think that they have vision and understand the opportunities that exist in tech. If we were able to connect with them, they would identify with Hip Hop Hacktivist's mission and help take it further.  Right now, Hip Hop Hacktivist revolves around introducing kids to programming and supplying them with resources for self learning. We hope it will become that resource as we expand and develop a hands-on curriculum to teach programming to youths. This is something that is already in the works.  We'd also like to develop a weekly Hip Hop Hacktivist show (something similar to what Desus and Mero do with their VICELAND show, but centered around tech and hip hop) to allow kids to see representations of themselves in positions in tech. It would also allow us to change the negative perception that some in our target demographic may have about working in tech.  We look forward to what the future has in store for Hip Hop Hacktivist. Anyone interested in the Hip Hop Hacktivist vision can contact the organization or register to attend .   Though Hip Hop Hacktivist is my vision, there are a few people who played very vital roles in making this happen. If it weren't for Kenny Durkin and Mary Cannon at Tumblr, Hip Hop Hacktivist would still just be a thought. Fortunately they were able to resonate with what I aim to accomplish and immediately jumped on board. Another key person is Akasha Archer, the Operations Manager. She's responsible for our visual identity along with website and promotional design. She also assisted me in managing day-to-day tasks and helped to keep things from becoming overwhelming. Then there's the wonderful group of mentors and organizers who generously agreed to donate their time and expertise to help the students. This chain of events and people involved are reflections of how special the NYC tech scene is.",,Vice,Activist Inspires Teens to Learn Coding With Hip-Hop Hackathon,Comment,https://www.vice.com/en_us/article/43yep3/activist-inspires-teens-to-learn-coding-with-hip-hop-hackathon
Michael Byrne,"It's hard to overstate the vastness and confusion of the online learning ecosystem circa 2017.  It's a realm that extends from online mirrors of university classes and even whole degree programs to niche tutorial subscriptions like Angular University to pioneers like Coursera. As someone who's done it, just approaching the Google search bar with a topic of interest is unlikely to yield a tutorial or course or program that's really ideal for the learner. There are too many variables: time commitment, workload, cost, interactivity, length, skill-level, prestige, certification (if any). And this is on top of all of the usual confounding search engine noise.  Part of the problem when it comes to programming and development skills is that there are many skills subsets (or stacks) and to newcomers it's not always clear how to gain those skills in an optimal way. It's actually really easy to find an extremely suboptimal learning path, by, say, trying to muddle through a course out of your depth or by focusing on a skill that's heading for obsolescence.   Surely there are busloads of would-be programmers that have just been turned off by the messiness of the whole thing: programming languages, transpiled programming languages, transpilers, programming language frameworks, web frameworks, HTML, compiled HTML, CSS, SASS, APIs, Amazon Web Services, containers services, reactive programming, functional programming, imperative programming, object-oriented programming, WebStorm, Atom, Sublime Text, Vim, and on and on and on. I could try and tell you a right way of navigating all of the skill trees involved in web development (or other sorts of development), but even if I came up with an optimal learning path, this stuff is changing all the time.  Enter Learn Anything . It's kind of a search engine. The basic idea is that you punch in a skillset you'd like to learn and it will return not a Google-like list of results, but a skill tree offering a clear way of navigating an optimized learning path. Included with that tree are links to curated learning resources. The content is all open-source and open to contributors, whose participation seems pretty neccessary to keeping Learn Anything useful.",,Vice,"There have never been so many online learning resources, but that has a downside.",Learn Anything Is a Graphical Search Engine to Tame the Online Learning Jungle,https://www.vice.com/en_us/article/kzzn8x/learn-anything-is-a-graphical-search-engine-to-tame-the-online-learning-jungle
Michael Byrne,"Blogger and programmer Matt Haggard makes the following argument in a Medium post published on Friday: ""If you're allowed to send me an email, I'm allowed to send you an email.""  What he's referring to is the explosion of ""no-reply"" email addresses, a phenomenon tightly coupled to the explosion in automated or programmatic email sending. Simply, if I'm writing a script to send an email—or one million emails—I'm most likely not writing that script to handle replies to those emails. This doesn't necessarily make me a spammer or a dick; instead, it might just mean that the normal ""reply"" interaction has been replaced with some other interaction, like, say, clicking a link within the email. Or it might mean that there is no interaction to be had in the first place, as in the case of spam or news alerts, etc. A quick search of my own email inbox for ""no-reply"" reveals that these are generally spam emails from companies like Groupon and notification emails telling me that such and such has now joined Quora or some shit is going down on Twitter. The complaint in these cases is really more that I'm getting spam emails at all than the fact that I can't reply to them. But there's some stuff in there that really should be reply-able, like kind of vague warnings that a key account needs to be updated. The absolute worst no-reply offenders are emailed notifications that I've received a message within some other messaging system, like the one used by my bank. And then sometimes you're just numb to it, like when a student loan servicer sends you a late notice in an email and then informs you that to follow-up on the notification you'll have to login into your student loan account and figure out how to send a message to customer service via some internal ""chat"" system that's really just a basic-ass contact form. Here's (most of) the rest of Haggard's post: You just sent me an email and I have a question. Don't make me hunt for a way to ask it. Email already has a built-in way to do that — reply. Whether it's good news or bad news, whether you're an established company or a startup, your customers will love you more if you let them reply to your emails. I think Haggard's complaint is really part of a much, much larger complaint about the hell that is trying to communicate with most any company in the year 2017. Every medium- to large-sized business is really on top of making sure that you can give them money as quickly and smoothly as possible, but when it comes to any other interaction, good luck. Like, I've spent full days of my life trying to get hundreds of dollars in erroneous charges refunded from a certain lodging industry disruptor. So, I'm not sure that it's the existence of no-reply email addresses that's the real problem. They're just a manifestation of a much larger thing, which is the no-reply corporation.",,Vice,"""If you’re allowed to send me an email, I’m allowed to send you an email.""",'No-Reply' Email Addresses Are a Plague,https://www.vice.com/en_us/article/vbbp5x/no-reply-email-addresses-are-a-plague
Michael Byrne,"Stack Overflow is far and away the reigning king of developer forums. It's more than just a handy resource or meeting ground for distressed tech professionals, it's a form of default documentation for programming technologies. As a result, the site has access to all kinds of interesting data on coding trends, emerging technologies, etc. Some of Stack Overflow's semi-regular data analyses are kind of silly— ""Developers Who Use Spaces Make More Money Than Those Who Use Tabs"" —but some offer unexpected insights into a highly dynamic industry. This week, Stack Overflow data scientist David Robinson published an interesting observation : There exists a small but meaningful divide between the programming technologies used in wealthy countries and those used in developing countries. To be sure, programmers everywhere tend to build things with the same tools, which makes sense because software is a global industry. In my own engineering side hustles I regularly work for and with people all over the world. But there are some curious exceptions. The first is in data science, which tends to employ the programming languages Python and R. ""Python is visited about twice as often in high-income countries as in the rest of the world, and R about three times as much,"" Robinson writes. ""We might also notice that among the smaller tags, many of the greatest shifts are in scientific Python and R packages such as pandas, numpy, matplotlib and ggplot2. This suggests that part of the income gap in these two languages may be due to their role in science and academic research. It makes sense these would be more common in wealthier industrialized nations, where scientific research makes up a larger portion of the economy and programmers are more likely to have advanced degrees."" C and C++ use is similarly skewed toward wealthy countries. This is likely for a similar reason. These are languages that are pushed in American universities. They also tend to be used in highly specialized/advanced programming fields like embedded software and firmware development where you're more likely to find engineers with advanced degrees. And then there's PHP. Waayyyyy at the bottom of the above chart is CodeIgniter, which is or is very close to the most disproportionately represented language in the analysis. It's a PHP framework. PHP is itself skewed toward lower-income countries, but CodeIgniter is deep. ""Further examination shows it is especially heavily visited in South/Southeast Asia (particularly India, Indonesia, Pakistan and the Philippines) while it has very little traffic from the US and Europe,"" Robinson suggests. ""It's possible that CodeIgniter is a common choice for outsourcing firms building websites."" I'm not sure there's a profound explanation for these disparities. You would find the same thing in most other globalized industries, surely. Software development likes to imagine itself as a uniquely egalitarian white-collar trade, but skillsets follow education and education follows money.",,Vice,But does it matter?,Coders In Wealthy and Developing Countries Lean on Different Programming Languages,https://www.vice.com/en_us/article/neekm8/coders-in-wealthy-and-developing-countries-lean-on-different-programming-languages
Michael Byrne,"Writing computer software involves a lot of cutting and pasting. It's just a part of normal workflow even―search Google for a problem, copy a kinda-sorta solution from Stack Overflow (or more proper documentation), and weld it into place in the new code. That last step naturally involves some amount of adaptation, rewriting, and debugging. (Sometimes it turns out that just writing your own solution in the first place would have been easier, but that's another story.) Earlier this month at the Association for Computing Machinery's Symposium on the Foundations of Software Engineering in Paderborn, Germany, a team of computer science researchers from MIT's CSAIL lab unveiled a new system for automatically transplanting code from one program into another. The tool, dubbed CodeCarbonCopy (CCC), works by comparing the execution of both the new software and the ""donor"" software, and then updating things like variable names and data representations in the donor code to the new host code. So, if the host program calls some variable x , CodeCarbonCopy will find the matching variable in the new code and rename it. That sounds kind of trivial, but it requires coming to some fundamental understanding about what each program actually does―like what a variable means given a certain program context and how that program uses it. Something similar is done to data representations within each program. It's an interesting problem. CCC solves it by feeding each program, the host and the donor, the same input file and watching each one do its thing. The result is a symbolic representation of every value that the two programs compute. In addition, CCC is able to identify functionality within the donor code that's used by the donor program but isn't actually useful in the new host. The MIT group did experiments with eight code transfers between six real-world programs, including VLC, mtPaint, and MPlayer. In seven transfers, functionality was successfully maintained through the transfer process. In the eighth transfer, from the program mtPaint to bmp2tiff, CCC just couldn't do the job thanks to some particularly quirky data structures. Anecdotally, there's some amount of building angst among developers about automation, that we will soon be outsourced to code-writing machines. CCC then might not be the best news, but for now we can take heart that the system still requires a considerable amount of human input. Crucially, the human programmer has to identify where in the host program the code is to be transplanted, any irrelevant data, and, of course, the actual functionality to be transplanted. CCC also takes a whole lot of time to do its magic, with the longest transplantation seen in the MIT group's experiments topping out at over 12 minutes.",,Vice,But take heart: It still requires human developers. ,New MIT Tool Automatically Rewrites Old Code for New Software,https://www.vice.com/en_us/article/3kavpy/new-mit-tool-automatically-rewrites-old-code-for-new-software
Michael Byrne,"There isn't much to Caenorhabditis elegans . At about 1 mm in length, it's the smallest of the roundworms. C elegans mostly consists of a primitive digestive tract and some clear gunk. It has a brain, but it's just about the minimum possible arrangement of neurons that could be considered as such. That's 302, while humans pack something like 86 billion neurons. Still, it's the only organism whose whole brain (or connectome) has been completely mapped. Approximating a human brain is maybe not even possible, but a primitive parasite is another story. It doesn't have to take much computing power at all. Those 302 neurons can be simulated on hardware as primitive as an Arduino Uno board, as demonstrated by Nathan Griffith's Nematoduino project, a robotic simulation of the aforementioned worm. The actual worms sense their world via chemicals. They have a great nose-type structure that lets them pick up on even subtle environmental cues. On the Nematoduino, sensation takes the form of a distance sensor. Locomotion, meanwhile, is at about worm level. The Nemtatoduino goes forward and backward, with the neurons governing each movement roughly correlated to the actual locomotion neurons of the worm. The simulation consumes about 40 percent of the Arduino board's program memory and about 42 percent of its SRAM. It can be extended, in other words. To that end, the Nematatoduino is open-source. As Griffith writes on the project's Github page : ""It's my hope that nematoduino will end up in the hands of a lot of students and neurorobotics hobbyists!""",,Vice,302 simulated roundworm neurons running on a cheap hobbyist hardware platform. ,Someone Programmed an Arduino Board with a Worm's Brain,https://www.vice.com/en_us/article/a3kqp4/someone-programmed-an-arduino-board-with-a-worms-brain
Michael Byrne,"Like precisely 100 percent of developers and programmers I have an opinion on coding bootcamps, a faddish alternative education model where participants learn specific real-world skills via concentrated coursework in usually in-personal classrooms. It’s a very expensive model, one in which students can easily spend five figures on even not very flashy programs. My opinion is pretty simple: You can do that and it may well get you a web development job that is a lot like other web development jobs, but you should at least consider community college.  Community college is not flashy and does not make promises about your future employability. You will also likely not learn current way-cool web development technologies like React and GraphQL. In terms of projects, you’re more likely to build software for organizing a professor’s DVD or textbook collection than you are responsive web apps. I would tell you that all of this is OK because in community college computer science classes you’re learning fundamentals, broad concepts like data structures, algorithmic complexity, and object-oriented programming.  You won’t learn any of those things as deeply as you would in a full-on university computer science program, but you’ll get pretty far. And community college is cheap, though that varies depending on where you are. Here in Portland, OR, the local community college network charges $104 per credit. Which means it’s possible to get a solid few semesters of computer science coursework down for a couple of grand. Which is actually amazing.  In a new piece published in the Communications of the ACM , Silicon Valley researchers Louise Ann Lyon and Jill Denner make the argument that community colleges have the potential to play a key role in increasing equity and inclusion in computer science education. If you haven’t heard, software engineering has a diversity problem. Access to education is a huge contributor to that, and Denner and Lyon see community college as something of a solution in plain sight.  “[T]here is a high participation of minorities in CS at CCs; more than half of CC students are non-white, and more than half of all Hispanic and Black undergraduates start at community college,” the authors write. “Efforts to retain students through transfer to completion of a bachelor’s degree would be a large step forward in helping diversify the field.”  “We argue that an understanding of the unique strengths and challenges of CC students is needed to strengthen efforts to broaden participation in CS.”  The commonly accepted education model in the US is that people graduate from high school and then start right away at a four-year college. This is the pipeline model. White middle-class parents take their kids to IKEA or Target and drop them off at a dormitory with a hug. And then there are the “other” people that go to community college and learn, like, auto repair.  Lyon and Denner note that the pipeline model has the general effect of suppressing diversity. They point to an alternative in which community college functions as a link between students of non-traditional backgrounds and traditional universities. Students spend a year or two in community college hammering out core classes and then transfer to a university. Which is a nice idea, but, as they explain, it’s mostly a nice idea in theory. Those first years get messy pretty quick, and students often wind up transferring a bunch between schools and-or being swallowed up and regurgitated by the workforce. It’s an easy path to get knocked off of.  “These twists and turns result in convoluted, individualized routes that can be full of detours and setbacks unexpected by students envisioning a simpler pipeline, and complicated by policies and restrictions at both CCs and four-year receiving institutions,” Lyon and Denner write. “For example, we found that students were delayed at CCs in preparation for transfer for many reasons including impacted CS classes, math anxiety or aversion, dropping and re-enrolling in classes in an effort to increase GPA, and family and financial responsibilities.”  So, the question is then of how do we make this more of a viable pathway. That’s what the authors are interested in. There’s a lot to this. Some of it involves providing advising and structure that’s better tailored to the unique situations of community college students, offering “extra support and flexibility in schedules, developing clear CS transfer pathways, and building knowledge of CS careers.” Some involves tailoring offerings that recognize the often different classroom experiences of different genders. Also: stronger partnerships between four-year schools and community colleges and increased industry participation in community college education. This is all pretty obvious stuff.  But I still find that community college in general is not always obvious, or at least it’s not talked about much within programmer circles. The polarity is almost always between bootcamps and four-year degrees.  I did the the community college to four-year school track and it was great. Unlike at my four-year school, the professors were all actively involved in industry and super-passionate about teaching. (Presumably there aren’t a whole lot of people with graduate CS educations teaching in community colleges for the money.) A backend engineer I worked with on a recent contract just did two years of community college and now he’s making engineering money in a high-end job market. This is a resource that already exists, just waiting to be fully exploited.",,Vice,It’s a solution hiding in plain sight.,"To Solve the Diversity Drought in Software Engineering, Look to Community Colleges",https://www.vice.com/en_us/article/kzgqyw/to-solve-the-diversity-drought-in-software-engineering-look-to-community-colleges
Jordan Pearson,"For all of the recent praise that’s been lavished upon Canada and our handsome Prime Minister Justin Trudeau for not being Donald Trump , it’s still a divided nation. There’s the geographical divide (despite being larger than the continental US, Canada has the population of California) but a digital one, too.  While populous areas have all of the internet infrastructure you’d expect in a modern country, many areas in Canada still have dial-up connections or worse . These underserved areas include rural and Northern regions, and crucially, Indigenous communities . Indigenous Peoples have been systematically marginalized socially, economically, and technologically in Canada for all of its 150 years in existence and even before—many communities still do not have clean water to drink—and the digital world is just one more area where they’ve been, in some cases, left to fall through the cracks.  Now, reconciliation with Indigenous Peoples is on the agenda for Canada as the federal government ostensibly seeks to right some past wrongs. During this moment of possible change, Denise Williams—the executive director of the First Nations Technology Council in British Columbia—sees a rare opportunity to make the digital playing field more equal for Indigenous Peoples in BC.  “The most exciting thing,” Williams told me over the phone, “is the fact that truth and reconciliation is in each of the Ministers’ mandates here in the province, and the way that the tech sector is trying to come to the table as being diverse and solutions-oriented. I think it’s very exciting to be at this intersection, at this time.”  WIlliams, 35, is Coast Salish from Cowichan Tribes on Vancouver Island. Before joining the technology council five years ago, she was involved with a range of Indigenous issues including community policing and education. But even then she was outspoken about how technology could be a part of reconciliation in Canada, too. After joining the First Nations Technology Council, Williams told me over the phone, she helped the organization pivot from trying to be a service provider and software developer itself to, as she put it, “be a convener” for stakeholders that have the resources to make things better.  Here’s what being a convener looks like: In 2018, after a successful pilot conducted in 2017, the council will fund a 12-week digital skills development program that will see more than 1,000 Indigenous people learn skills like web development and software testing. Every participant in the program will get a laptop paid for by a corporate partner that they can keep if they complete the course. “This is a common challenge,” Williams said over the phone. “Our Indigenous participants don't have access to their own laptop that can handle the software.” Crucially, these tech partnerships also entail paid internships for people who complete the skills development program.  “We want to make sure our partners are ready to welcome Indigenous interns, and they can take advantage of our reconciliation workshops and cultural sensitivity and awareness training,” Williams said. “That way, we can begin to shape the tech sector in BC in a way that’s never been done.”  The council also offers web design services to Indigenous communities in partnership with web services company Animikii Inc. It also offers community technology planning services for communities looking to upgrade their infrastructure. Indeed, one of the reasons that the council can play an advisory role in these endeavours, instead of directly taking them on, is that Indigenous communities are already doing it themselves.   Read More: Indigenous Peoples Will Shape a More Just and Sustainable Future for Canada  There are several regional Indigenous-owned and operated internet service providers in Canada, for example, and some communities have even gone so far as to erect their own infrastructure . But change is needed on a systemic scale, and that won’t come quickly or easily.  A main obstacle to better infrastructure in Indigenous communities over the next five years, Williams said, will be the large corporate incumbents that have an oligopoly over the cables that criss-cross Canada. An unfortunate reality is that these entities seek profit, and small communities getting wired up isn’t always a huge winner, revenues-wise.  “The goal isn’t that the cost is $10,000 a month to subscribe to a Telus service—that doesn’t seem quite right either,” Williams said. “So that’s why we need to be working collaboratively to find these solutions.”  Still, if anyone is up to the challenge, it’s Williams.  “Words that were spoken to me a few years ago from one of the Chiefs here was that Indigenous people are the original innovators on these territories,” WIlliams said, “having lived here for hundreds of thousands of years learning how to work responsibly with this environment, and how to innovate for the betterment of all.”  “Including Indigenous leadership and voices and worldviews in the tech sector at this really important growth stage...” she added, pausing. “We don’t even know what’s possible.”    Humans of the Year is a series about the people building a better future for everyone.  Follow along here .",,Vice,"As executive director of the First Nations Technology Council, Denise Williams wants to equalize the digital playing field for BC’s Indigenous communities. ",The Activist Fighting for High-Tech Reconciliation for Canada's Indigenous People,https://www.vice.com/en_us/article/qvwq7b/the-activist-fighting-for-high-tech-reconciliation-for-canadas-indigenous-people
William Turton,"A former YouTube engineer is speaking out against the company’s recommendation algorithm, saying that it is programmed in a way that could lead to the promotion of conspiracy theories. Guillaume Chaslot, who has a Ph.D in computer science and worked on the YouTube recommendation algorithm in 2010, says that when he worked at the company, YouTube was programming its algorithm to optimize for one key metric: keeping viewers on the site for as long as possible to maximize “watch time.”  Chaslot says that conspiracy videos, like those about flat earth or autism and vaccines, were more likely to be recommended in YouTube’s recommendation algorithm because of that focus on watch time. In a statement to VICE News, a YouTube spokesperson says the company still considers watch time in its algorithm, but that it now also factors on another metric: user satisfaction.  Chaslot has since created a tool, dubbed AlgoTransparency , that he says shows conspiracy videos on YouTube are still some of the most likely to be recommended. VICE News meet with Chaslot to discuss how YouTube's algorithm works, and how he plans to create another similar tool for the Facebook and Twitter algorithms. This segment originally aired on March 5, 2018, on VICE News Tonight on HBO.",,Vice,"YouTube is programmed in a way that could lead to the promotion of conspiracy theories, a former engineer says. ",How YouTube's algorithm prioritizes conspiracy theories,https://www.vice.com/en_us/article/d3w9ja/how-youtubes-algorithm-prioritizes-conspiracy-theories
Matthew Gault,"Programmer Matthew Earl needed level data from the original Super Mario Bros. for an upcoming project, so he decided to get that data the hard way. Earl wanted the background imagery for each distinct level—everything except the moving sprites and the HUD elements, such as the life total and coins.  There are many easy and well-worn ways to get that data, and people have been messing with the sprites from Super Mario Bros. for decades. As first spotted by Hackaday , Earl went the long way around. Instead of pulling the rendered assets out of the game, Earl dug into the source code itself and used an emulator in Python to extract the raw assets from the game and render it himself.  Nintendo has never released the official source code for the NES or any of its games, but industrious hackers have reverse engineered the code by pulling it apart and putting it back together on their own . Earl ran this code through a Python library called py65emu that emulates the NES machine’s assembly code. From there, he built a program that intercepts the visual data when it’s on its way from memory to to the picture processing unit and rendered it using Python. “And with that, we have managed to extract level imagery from SMB, purely in Python,” Earl wrote.  It seems like a labor intensive process, but Earl’s github post breaks it down and his work gives us a rare window into the inner workings of one of the most popular video games of all time.",,Vice,Hacker Matthew Earl used Python to extract raw visual data from Nintendo’s code.,How a Hacker Used Python to Extract the Source Code for ‘Super Mario Bros.’,https://www.vice.com/en_us/article/8xbvxp/how-a-hacker-used-python-to-extract-the-source-code-for-super-mario-bros
Michael Gaynor,"A few weeks ago, Alok Menghrajani, a security engineer at Square, set out to challenge himself. He wanted to fit a bootable CD-ROM, and a retro video game inside it, into a tweet.  The results are pretty cool. Within 280 characters, Menghrajani crafted code that creates a CD-ROM disk image, which can either be booted up in a virtual machine or burned to a physical disc. Inside that is the video game, which he described as a mixture of Tron and Snake. It took Menghrajani two weekends–between 50 and 100 hours–to complete, he estimated.  “I find it relaxing,” he said over the phone. “It’s a way to meditate for me.”  The process of fitting this all into something the size of a Tweet, Menghrajani says, is known as “code golfing.” Just like in golf, it’s a game of reaching your objective in as few steps as possible–in this case, writing code efficiently enough to do something with the least number of characters.  Menghrajani makes a hobby out of it. About five years ago, he code-golfed his way into embedding a QR code within a QR code . And a few years after that, he managed to fit a bootable floppy disk and a video game into a Tweet. Since then, Twitter has doubled the character limit for Tweets, which gave Menghrajani the space to try to do it with again a CD-ROM.  “It’s a fun activity,” Menghrajani said. “Once in a while I’ll come across as challenge that’s like, ‘Hey, this is a fun thing.’ I used to every month try to come up with something. Then I had a kid, so I haven't been writing as much.”  Menghrajani said that the CD-ROM idea was originally inspired by a conundrum he was facing at work. He started delving into research about how the CD format functions, and once he did, “I was like, ‘I can probably fit this is a tweet,’” he said.  For Menghrajani, whose day job involves doing a lot of code maintenance and upkeep, code golfing provides a different sort of satisfaction.  “It’s pure hacker spirit,” he said. “You just write the code and see what it does.”",,Vice,How to write a video game in 277 characters.,This Coder Fit a Bootable CD and Video Game Into a Tweet,https://www.vice.com/en_us/article/43pqnd/this-coder-fit-a-bootable-cd-and-video-game-into-a-tweet
Samantha Cole,"In an ideal world, no one would be shocked that Lyndsey Scott—an actress, Victoria’s Secret model, and the first Black woman to receive a contract with Calvin Klein at New York Fashion Week—is also a skilled coder.  But last week, Scott found herself needing to defend her programming prowess against trolls online.  When the meme account coding.engineer reposted an image about Scott’s work with the caption “CODING IS FOR ANYONE!” on Friday, the tone of the comments was far more negative than the original. People questioned her skills, implied that it was a “shame” that she chose to model, and said that she probably only knows how to print “Hello World” (the first lesson a coding student typically learns).   She decided to jump into the comment section herself: “I have 27481 points on StackOverflow,” she wrote—she’s also the lead iOS software engineer for app developer Rallybound, and holds a bachelor’s degree from Amherst where she double majored in computer science and theater.  “Looking at these comments I wonder why 41% of women in technical careers drop out because of a hostile work environment,” she wrote.  She was hesitant to comment at all, Scott told me over the phone. “But I realized that this is a persistent problem; I mean, it's been going on from the beginning of tech.” The resultant flood of stories about her comment—and about her standing up for herself against bigots online—was unexpected, she said. Even though she’s accustomed to people judging her for her appearance and discrediting her coding for it, she told me she was “shocked” at the scale of this particular story around the Instagram post.  Scott told me that she started coding on her own, specifically around iOS development, after college, when she needed supplemental income to support her acting dreams. She started answering questions on Stack Overflow, and wrote tutorials for other coding students. As her skills grew, so did her confidence.  “Finally I felt like people were taking me seriously,” she said. “There was a point where I'd walk into a room of programmers and they wouldn't believe I was one of them, or I'd try to contact someone for a job and they wouldn't take me seriously.”  “Knowing it's not an internet-only phenomenon, that this happens all across this industry, it made me want to speak up so that other women girls and minorities wouldn't have to face this sort of discrimination,” Scott said. “Maybe people would think twice before judging someone based on what they look like.”",,Vice,Instagram trolls tried to question Lyndsey Scott's coding skills—and showed how pervasive these biases are in tech.,Trolls Baffled by Victoria's Secret Model Who Codes,https://www.vice.com/en_us/article/qva4ap/lyndsey-scott-victorias-secret-coding-instagram
Michael Byrne,"According to the 56,033 developers surveyed by Stack Overflow as part of its annual user poll , JavaScript is again the most popular programming language. Other findings: developers are more into dogs than cats (except in Germany), are mostly self-taught, and are way more into Star Wars than Star Trek. Meanwhile, a mere 7 percent of adult developers would identify themselves as ""rockstars"" while another 10 percent consider themselves to be ""ninjas."" Would you even believe that 98.2 percent of them were also males? Males earning a mean annual income of $106,000, that is. So, while ""JavaScript is the most popular language can you even believe it"" makes for a nice headline, there are some significant caveats here. For one thing, looking a bit closer will reveal that respondents were almost entirely web developers of some sort. So, it may be a bit more reasonable to say that JavaScript is the most popular web development programming language, which is a pretty well, duh result. JavaScript has been at the top of the Stack Overflow annual survey since 2013, as one would expect given the extent to which JS has become omnipresent on the web. The other languages in the SO top 10 are mostly other web languages. These include SQL, which isn't quite a language in the same sense anyway, along with Angular.js and Node.js, which are both JavaScript frameworks. You will eventually find C and C++ way down on the list, as well as Python, but their showings are about what you'd expect given a survey of web developers. You will find a much better, much less bro-centric look into the ""most popular language"" question via IEEE Spectrum's ranking , which is based on 12 metrics from 10 different sources (including job postings, GitHub code, published papers, etc). Relatively speaking, it's a scientific look, with conclusions more representative of the whole spectrum of code. Here, JavaScript comes in at number eight, behind Java, C, C++, Python, and emerging data powerhouse R.",,Vice,For what it's worth.,Stack Overflow Survey: JavaScript Is the Most Popular Programming Language,https://www.vice.com/en_us/article/jpgpkk/stack-overflow-survey-javascript-is-the-most-popular-programming-language
Michael Byrne,"An ultra-common and generally bullshit theme that can be found across the internet from Business Insider to coder forums to anywhere else that aspiring programmers and coders may lurk is that of the ""most profitable"" programming language. Where should ""you,"" as the stereotypical case of just-anyone wanting to get into code to make better and easier money, be best off spending your limited attention and financial resources? It is a bogus question that gets at sickly heart of programming hype—a phenomenon that rests mostly on the notion that a few weeks of online learning or a code bootcamp will make someone into a coveted resource. I don't think I've ever seen an example of this ""most profitable"" theme more all-in than a post that landed in my alerts this morning, from the site ""Insider Monkey."" Here are the first two paragraphs: Computer programmers are in luck, since according to our list of 11 most profitable programming languages to learn in 2016, they can easily become wealthy. And if you are struggling with job options, maybe taking a course in programming languages isn't a bad idea, since the best of all is that you don't have to have a degree, just the knowledge. Since we live in the modern world, technology-related jobs are in high demand. Like that is not enough, they have also become very lucrative and are always at the top of best-paid jobs. If you manage to land a job in some tech-giant like Facebook, Google, Microsoft, Apple, only the sky is the limit. Good grief. What follows are Insider Monkey's 11 most profitable languages. The methodology involved taking the most popular programming languages, according to the monthly TIOBE index , and then ranking them according to the salaries associated with each one, according to data from GooRoo . The resulting list gives an average salary for each language and a sentence of information about the language that someone hurriedly extracted from a Google search. The idea behind the list is that programming is a world segmented by programming languages and that this is how people are hired into six-figure jobs—by their knowledge of a particular programming language. Which is ridiculous. Of course, employers want specific programming language skills, probably several of them. A bit more accurately, they want programmers who are skilled in programming and, thus, can learn new programming languages as they become useful for new problems. Which is the thing lists like this don't tell you: if you're skilled in programming/software engineering, learning most any programming language winds up being a reasonable, if not easy, task. I'm not trying to nuke the general ideas of in-demand or popular programming languages—because those are things that exist. But they exist within a much larger ecosystem of talent, ability, and, ultimately, employability. Here are a few things that need to be kept in mind as you chew through another ""most profitable"" programming language list. 0) Popular languages and in-demand languages != the future You'll see Java at the top of many ""most profitable"" lists and it is for sure an in-demand skill. Java is deeply sewn into the software world, particularly when it comes to business-focused or enterprise software. Java will be around for a while, but not so much by choice . The language's corporate parent, Oracle, has been rumored to be considering a Java planned obsolescence. Its decline will be slow but assured as new software systems are implemented in more modern languages. 0.5) People make money, not languages Also: if Java is topping salary lists, it might have something to do with it being a language much more likely to be employed by software engineers that learned it in college, where it is still often taught. In other words, Java can be expected to be a part of a much larger skill set than what we might normally think of ""coding,"" e.g. software engineering, systems programming, etc. A professional engineer whose work involves Java is going to have a boatload more qualifications than ""knowing Java,"" and this is likely to be to a greater extent than someone whose work primarily involves many if not most not-Java languages. 1) Programming languages are tools Programming languages are not programming . They are tools used to program. Programming is a set of skills that are mostly language-independent. ""Knowing Java"" implies to precisely no one that you are a competent programmer. Moreover, programming languages are rarely used in isolation. A project will likely require several of them working together. In a sense, saying a language is in-demand is a bit like saying that, among the tools used to build a house, a nail gun is in demand. I'm not sure a construction worker would go up to a foreman looking for work by saying that they are totally awesome with a hammer or screwdriver. These are things that are part of a toolkit employed by people that know how to build things and containing many tools for many different tasks. 2) Programming languages depend on problems Even very general languages like Java fit some domains better than others. Programming is always just solving problems and programming languages all exist to solve some subset of all problems. This is why there are so many languages—some are better at solving certain types of problems than others. C is well-suited to problems involving machines with very limited computing resources; JavaScript is best suited to web applications; PHP interacts with servers; R is most useful for statistics and graphics; and so forth. So, if not the ""most profitable,"" what language should you actually learn? Probably the one best suited to helping you learn other languages, e.g. the one that will teach you to actually program. That might be Python, or it could even be something with a very limited domain (or problem space), like Processing .",,Vice,Here's why that's a bullshit metric.,Don't Pick a Programming Language Because It's the 'Most Profitable',https://www.vice.com/en_us/article/d7y3gq/dont-pick-a-programming-language-because-its-the-most-profitable-java-javascript-python
Oscar Balderas & Nathaniel Janowitz,"An imprisoned Colombian hacker, Andrés Sepúlveda, claims he fraudulently helped Enrique Peña Nieto win Mexico's 2012 presidential election, as well as manipulate elections in eight additional countries across Latin America. Sepúlveda's interview with Bloomberg Businessweek caused a stir throughout Latin America, as well as the United States, particularly for the alleged involvement of Juan Jose Rendón, a Miami-based political consultant who, Bloomberg wrote, has been called the Karl Rove of Latin America for his dark influence on right-wing politics. And according to the campaign manager for the candidate whom Peña Nieto beat, cybercrimes of the sort Sepúlveda alleged are still happening in Mexican politics. The hacker claimed that Rendón hired him repeatedly to commit a wide variety of crimes to affect the outcomes of elections, including installing malware, hacking websites, creating fake profiles, and digitally spying on opposition candidates. Rendón emphatically denied these allegations to Bloomberg when contacted ahead of the article's publication, and reiterated the denial after its publication. He also said that he is talking to a ""leading US presidential campaign"" to go work for it once the primaries end and the general election campaign begins — but didn't specify which one. Related: Wife of Conservative Ex-Leader of Mexico Says She's Running for President in 2018 As for Sepúlveda, he is currently serving 10 years in a Colombian prison for use of malicious software, conspiracy to commit crime, and espionage related to Colombia's 2014 elections. But some of his most serious allegations regard the current Mexican president. He claimed when Enrique Peña Nieto was a presidential candidate, his campaign team hired him to spy on the private communications of his rivals, Andrés Manuel López Obrador and Josefina Vázquez Mota, and rig the election. Peña Nieto's Institutional Revolutionary Party or PRI paid him $600,000, Sepúlveda said. ""Sepúlveda's team installed malware in routers in the headquarters of the PRD candidate (leftist Andrés Manuel López Obrador), which let him tap the phones and computers of anyone using the phones and computers of anyone using the network, including the candidate,"" Bloomberg journalist Jordan Robertson wrote. ""He took similar steps against against PAN's Vázquez Mota. When the candidates' teams prepared policy speeches, Sepúlveda had the details as soon as a speechwriter's fingers hit the keyboard. Sepúlveda saw the opponents' upcoming meetings and campaign schedules before their own teams did,"" the article goes on. After Peña Nieto's victory, Sepúlveda's said he destroyed all evidence of his wrongdoing, including hard drives, to hide his digital traces. To hack the Mexican and other elections, he said he employed a team of experts from all over Latin America, from Argentinians to Mexicans themselves. Related: This Murder Has Exposed the Dark Side of Mexico's Hacking Community Ricardo Monreal, the coordinator for López Obrador 2012 campaign, when he came in second, told VICE News that the campaign had long suspected something of the sort, adding that the practice is still going on, even with the self-confessed main perpetrator imprisoned. ""Of course they spied and interfered in the social networks"" of the campaign, ""and they continue to do so now. Back then we publicly denounced the existence of a network of foreign publicists and bot operators,"" he said. ""This is a very serious and delicate accusation that Mexican authorities need to investigate. Political espionage is a common practice in our country, but the fact that it is so common does not make it legal, but one of the most unpunished crimes practiced and encouraged by public officials,"" Monreal, who now is the head of the Cuauhtémoc borough in Mexico City, added. Conversely, Ernesto Cordero, who was coordinator of Public Policies for Vázquez Mota's campaign, said he never felt spied on by his political opponents. Sepúlveda ""led a team of hackers that stole campaign strategies, manipulated social media to create false waves of enthusiasm and derision, and installed spyware in opposition offices, all to help Peña Nieto,"" the story continues. The alleged hacker said he created an estimated 30,000 fake Twitter profiles to shape discussion around Peña Nieto's favorite topics, like his plans to end drug violence. He made his admissions to Bloomberg because he is ""hoping to convince the public that he's rehabilitated – and gather support for a reduced sentence,"" the article read. Sepúlveda is under unusually heavy security even in prison; he sleeps under a bulletproof blanket, is constantly checked on by guards, and when he has to be moved, is driven in a convoy of armored vehicles with cell phone jammers intended to prevent the detonation of roadside bombs. Related: The DEA Is Tracking All Internet Traffic in Colombia, Hacked Email Shows The PRI, asked for comment by Bloomberg, maintains it had no knowledge of Sepúlveda or any member of his team having worked for the current president or other political campaigns. PRI leader César Camacho called the story ""absurd"" and dismissed it as a distraction. ""This is information that only distracts people. We have to focus on issues that are actually true,"" Camacho said, ""and not isolated statements that can in no way be backed."" Over an eight-year period, Sepúlveda said, he applied similar tactics in other elections in Colombia, Costa Rica, El Salvador, Guatemala, Honduras, Nicaragua, Panama, and Venezuela. He also said that he was offered work in Spain, but turned it down because he was too busy. And when asked whether the current US election was being tampered with in the same way that he allegedly did to elections in Latin America, Sepúlveda had no doubt: ""I'm 100 percent sure it is,"" he said. Related: Mexico Is Hacking Team's Biggest Paying Client – By Far Gabriela Gorbea and Alberto Riva contributed to this report. Follow Oscar Balderas Nathaniel Janowitz on Twitter: @oscarbalmen and @ngjanowitz",,Vice,"The campaign manager for the opposition party in the 2012 Mexican election told VICE News that they knew they were being spied on, and believe it continues to this day.","A Hacker Said He Rigged the 2012 Mexican Election, and It May Still Be Happening Today ",https://www.vice.com/en_us/article/59eqz5/hacker-mexico-latin-america-rendon-sepulveda-bloomberg-elections
 ,"The art gallery can be a rather austere space—maybe even brutalist on occasion. But with Berlin-based media artist Nils Völker's latest exhibition, Bits and Pieces , the gallery becomes a space of “poetic performance” through a choreographed dance of what the artist calls “ordinary objects.” For the exhibition, Völker suspends 108 motorized spheres that open and close in orchestrated rhythms. “The floating, colorful everyday objects merge into a single entity, an almost living organism that modulates, attempting to find a poetic balance between space, electricity and relationships,” writes NOME Gallery, which is holding the exhibition until April 16. “The movements generate a tension between the planned and the accidental, with each new variation opening up possibilities for interpretation. The sculptural presence draws attention away from the technical underpinnings of the finite object to a world of imaginative motional processes.” Völker tells The Creators Project the computing, technology and art that collide in Bits and Pieces were a sequence of many lucky personal coincidences. Before becoming an artist, he worked as a graphic designer. While researching drawing machines, Völker stumbled across a LEGO set with which one could build little robots—and apparently a drawing machine as well. “So I disassembled it and kept on building new robots and machines,” Völker says. “And as my ideas grew bigger and bigger at some point I switched to ‘real’ electronics and entered a whole new world of electronics and programming.” For Bits and Pieces , Völker similarly found the motorized spheres—known as Hoberman Spheres—by accident while browsing in a 99-cent store. He bought a few of them and quickly fashioned a test setup with some components he had lying around. “Although it was a really roughly made test with just a handful of spheres, it looked amazing and pretty promising,” Völker says. “At about the same time Luca Barbeni, the director of the NOME Gallery, approached me and we quite quickly decided that it could be pretty amazing to fill the whole space of his gallery with those moving toy balls.” “Each Hoberman Sphere is suspended on nylon strings with a lever mounted to a servo motor on the other end,” Völker explains. “Programs running on micro-controllers then make the motors move to the right angle at the right time. So it looks like the whole installation is almost moving in a very organic way, but in the end it’s just 108 spheres which stay in place and just grow and shrink at the right time.” Völker says that in general he is more into process than concepts. He likes to play with components and test materials without exactly knowing where things might end up. As a result, there is no complex theoretical concept behind Bits and Pieces . “But of course there are many possible interpretations when it comes to large amounts of mass-produced, almost identical objects doing all the same thing, or when things made from cheap and colorful plastic—the opposite of anything organic—start to behave pretty organically like a swarm or wave,” Völker muses. “But in general I prefer to leave any interpretation open to the visitor and really like to hear what others think instead of telling them what they should see in it.” Whatever viewers happen to see in Bits and Pieces , what cannot be missed is that Völker has transformed a gallery into a space for pure choreographed robotic poetry. That in and of itself is far from typical outside of roboticized factories. Bits and Pieces from Nils Völker on Vimeo .  Click here to see more of Nils Völker’s work. Related:  Meet the Domestic Robots Who Will Be Co-Parenting Your Kids  Oculus Rift Headset and Controllers Puts You Inside a Robot's Body  [Best of 2015] The Year in Robotics",,Vice,A Gallery Becomes a Floating Robotic Dance Orchestra,Entertainment,https://www.vice.com/en_us/article/78ejxx/floating-robotic-dance-orchestra
 ,"Algorithmic creatures dominate a lot of Raven Kwok ’s visual thinking. But when the Chinese artist, a.k.a., Guo Ruiwen premiered his video for Karma Fields’ song “Skyline,” he unveiled generative visual patterns based on the Voronoi tessellation , a mathematical model used in a wide range of fields, including art. Kwok puts these mathematical points to work again in his monumental, 45-minute generative video for Karma Field’s full album, New Age | Dark Age (Monstercat), though this time the tessellation is just one of a menagerie of algorithmic visuals. The world itself is monochromatic, as with much of Kwok’s work. The video is about an artificially-intelligent matrix of grids and shapes in constant motion and evolution. Kwok also plays with depth and flatness, often in the same frame, and brings in lyrics as he did in the recent video for “Stickup.” ""The concept behind this video is [to] build a synthetic world out of systems and algorithms that represent the symbiotic relationship between man and machine that Karma Fields portrays in this album,” Kwok tells The Creators Project. “This full-length video is an amalgamation of all previous work to create one fluid piece of visual art."" Spend your afternoon inside matrix with Raven Kwok and Karma Fields below:   Click here to see more of Raven Kwok’s work. Purchase Karma Fields' New Age | Dark Age via Monstercat here . Related:  Raven Kwok’s Latest Video Features a Code-Generated Talib Kweli  Lyrics Morph into Code in Karma Fields' Hypnotic Video  [Music Video] ""Skyline"" | Mathematic Models Become Creatures Driven by Code",,Vice,45 Minutes of Algorithmic Genius Compose Karma Fields' Full Album Music Video,Entertainment,https://www.vice.com/en_us/article/53waw5/raven-kwok-algorithmic-worlds-karma-fields-video-lp
 ,"Earlier this month in Milan, Jonas Lund's latest research shone through plexiglass panels in the Steve Turner booth at the Miart fair. Alongside a set of screens streaming online content seeking the next art market trend, and animations playing on obsolete LED monitors encased in concrete, Lund’s three illuminated digital paintings took center stage. The works were developed with an algorithm that wires a work of art for success. Through a set of parameters, the piece’s visual content is optimized for an art fair. In the artist’s words: “This series is based on a neural network that has been trained on all my previous works, to outsource the process of making new work to an artificial semi-intelligent program that 'thinks' like me.” Jonas Lund. New Now 3, 2016. UV print on plexiglass, metal frame and LED strip The paintings are in line with Lund’s longstanding preoccupations. His VIP (Viewer Improved Painting) from 2014 tested different compositions and colors while tracking the viewer’s gaze, ultimately determining the most compelling visual arrangement. That same year, the artist produced a series of oil paintings based on an algorithm that processed the most successful works at a Phillips auction. Earlier work, such as Paintshop , a factory and marketplace for collaborative art, already tackled questions around art production and the market. The work exhibited in Milan further the artist’s exploration of optimization practices, and is meant to offer more questions than answers. “What is an optimized artwork?” asks Lund. “A work that is set up for success to be liked by everyone, a work that stands out and creates diverging opinions, a work that sells, a work that asks the right questions at the right time, a work that gets 200 likes on Instagram, a work that makes you feel good?” Jonas Lund. miart, Milan, Installation view, Steve Turner, April 2016 There is, of course, no right or wrong answer. “Through most of my research I've reached the conclusion that there is no formula to determine what constitutes an successful work of art, and that's exactly the magic of a work of art—that it's not measurable by typical quantification methods, so the gesture of creating these pieces speaks more towards the obsessions of optimizations through streamlining quantification and a big data centric way of thinking rather than trying to find a fixed solution.” Appropriately, the intangible forms inside the paintings themselves seem to reflect a certain malleability, an unwillingness to fully mature into a final state. Illuminated with LED frames, they draw viewers near, but retain an air of mystery. Maybe keeping viewers guessing is one key to success. Jonas Lund. New Now 2, 2016. UV print on plexiglass, metal frame and LED strip To see more of Jonas Lund’s work, click here . Related:  Jonas Lund Has Created A Factory And Marketplace For Online Collaborative Art  Paint Your Pizza Lets You Paint A Pie And Order It  Look Closely: It's a Painting, Not an Inkjet Print",,Vice,These Digital Paintings Are Wired for Success—Literally,Entertainment,https://www.vice.com/en_us/article/3d5evw/digital-paintings-wired-for-success
Michael Byrne,"On Thursday, Bing, the yep-still-exists Microsoft search engine, together with partner HackerRank.com unveiled a new search capability geared toward coders. Simply: code-related queries will now bring up solutions in the form of real actual code snippets presented in a small live editor that can even run the code for you right on the spot. Googling code is a routine thing among programmers. There exists a vast ecosystem, mostly resident on Stack Overflow, of question-and-answer forums in which literally every code-related question has been answered five times, at least. It is a strange and sometimes disconcerting realm that has bailed my ass out on more occasions than I care to admit. It is also Bing's stated target. From a HackerRank.com blog post announcing the tool: Typically, engineers go to search engines to get answers on various sites like, Stackoverflow, Stackexchange and other blogs. Now, you have a streamlined alternative that will not only spit out the code solution you need but also edit the code and play with it in real-time. No IDE installation required. This will save you endless time you used to spend going back and forth from search to your code editor. So, I tried it on a handful of queries, some more realistic than others. I managed to get the code sandbox to actually pop up about one-quarter of the time, even when limited to reasonably basic searches. The rest of the time Bing just returned a usual list of search results. For example, it worked on a search for ""Python for loop""—returning a canonical Python for-loop example—but I got nothing for ""Python import module"" (that is, it just returned normal search results). ""Java initialize array to 0"" gave me nothing, as did ""Java print line"" and ""Java input string."" It did, however, spit out some code for ""Java for loop."" Based on the above, I didn't even bother trying to feed it any actual questions from Stack Overflow. Indeed, even very simple Stack Overflow questions aren't as atomic as this thing would seem to require. A big part of this is that they're usually not purely questions at all. Users are very frequently if not usually posting problems : ""My 2dGameEngine is very laggy""; ""MP3 metadata is not getting updated""; ""Path change ActiveXObject not working."" And those are just the questions actually posted on the site. Certainly, most of Stack Overflow's utility to programmers comes via Google searches for the simple reason that most everything has already been asked and it's just a matter of finding the answer. It may not be a direct answer either, but a related answer to a related question that points you in the right direction for a solution to your actual question. Eventually, you get pretty good at these searches. It does me no good to be able to run some textbook example of code in a tiny editor on a webpage. I mean, there are situations where that's useful—hence the popularity/utility of JSFiddle —but not really when I'm trying to figure a thing out by searching the internet. For starters, the thing I'm trying to figure out is invariably part of a much larger thing and it does me little good to see it functioning in isolation. That is, going back and forth from search to my code editor is a fact of life because the code editor is where the rest of my code/program resides. Continually updating, re-compiling, and just watching a program do its thing is, like, the entire workflow. Or it can seem like it, anyway. I guess I'm OK with that. I won't say this Bing thing is worthless, but it does kind of feel like a gimmick, at least at this stage.",,Vice,Textbook code snippets are not Stack Overflow answers.,Bing's Programming Search Tool Is Just Whatever,https://www.vice.com/en_us/article/53d9w5/bings-programming-search-tool-is-just-whatever
Michael Byrne,"Even to the more mathematically challenged among us, it's a reasonably easy task to parse a number with a decimal point. I can give you, say, 33.333 and it doesn't take any special mental contortions for you to make sense of it (not saying that you , in particular, are mathematically challenged, but play along). So, that's 33 things and then .33 of another thing, which is almost a third of a 34th thing. Easy enough. Computers have a funny, uneasy relationship to decimal numbers, however. Whole numbers are easy because it's easy to represent a whole number in our base-10 counting system as a binary (base-2) number. With 32 bits, or 32 digits, we can represent a huge range of whole numbers (integers or ints, in computer science), all the way up to 2147483647. Usually, that's more than enough. The problem when we start adding fractional values is that we have to have a way of encoding where exactly within a string of digits a decimal point should be located. It changes number by number. In computing, a number with a decimal point and corresponding fractional value is represented by the floating-point data type (a float). For a 32 bit floating-point number, it turns out that we really only get 23 binary digits to represent the numerical content of a number, with the rest reserved for representing the position of the decimal point within the number (as an exponent, as in scientific notion). The problem is less so a relatively limited range of possible values than a fundamental limitation on precision. You can see this easily enough by taking some arbitrary large number and scooting around a decimal point within it. The larger the number gets—e.g. with more digits on the left side of the decimal point—the less binary real estate we have to represent its fractional value (the right side). John Gustafson, a computer scientist specializing high-performance computing and the namesake behind Gustafson's Law , has proposed a new solution to this seemingly unavoidable source of error (read: imprecision). He calls the new format ""unum,"" for universal number. The key difference is that a unum allows for the various ""fields"" within a binary floating-point number representation to expand and contract according to required precision. The exponential values required to scale most decimal numbers (that is, determine their decimal point locations) are usually a lot less what can be represented by five bits, as are allocated in a 32-bit float, but the current standard tries to prepare for the worst case. If the exponent field is able to shrink depending on need, that leaves more digits that can represent actual numerical content, which is good. Here is Avogadro's number, 6.022×1023, two ways: ""A unum has three additional fields that make the number self-descriptive,"" Gustafson explains in an interview with the ACM's Ubiquity magazine . ""The 'ubit' that marks whether a number is exact or in between exact values, the size of the exponent in bits, and the size of the fraction in bits. So not only does the binary point float, the number of significant digits also floats."" There's a further approximation problem in floating-point numbers that's probably a bit harder to see, but Gustafson gives a good example. Imagine that we have two numbers that are very close to each other: 1,000,000.7 and 1,000,000.6. Subtracting these two numbers gives us .1000000. The problem is that with .1000000 it looks an awful lot like we have six digits of precision to the right of the 1 when we really don't have any idea what's in those digits notated as zeroes. They just appeared when we did the subtraction. In the original two numbers, that precision would be off somewhere to right of the 6 and 7 digits, which is uncharted territory. So what? The problem comes when we try to do a calculation with our new number. Like, if we add 1.0×10-12 to it we get .1000 001×10-6. Which seems right (that is, adding .1 with .000000000001 and getting .10000000001), but no . Because we never had any actual information about what was to the right of the 1 in .1. We just tricked the computer into thinking so. All we can safely do with the second, much smaller number is trash it. It's beyond our precision. This is the utility in having a field specifying whether or not our number is exact or if it's an approximation. If the computer knows it's exact, and those extra zeroes indeed mean zero and not don't-know , than we can do the calculation. If not, we can't. The Ubiquity interview is pretty deep, but worthwhile if you can stand more math and want to learn more about how computers (often poorly) represent information. The unum format, meanwhile, is finding some traction in the real world. ""I was just at the Supercomputing conference in Austin, TX, and I was amazed at the groundswell of interest and software efforts surrounding unum math,"" Gustafson said. ""The Python port is already done, and several groups are working on a C port. Because the Julia language is particularly good at adding new data types without sacrificing speed, several people jumped on making a port to Julia, including the MIT founders of that language. I learned of an effort to put unum math into Java and into D, the proposed successor to C++.""",,Vice,Farewell floats?,A New Number Format for Computers Could Nuke Approximation Errors for Good,https://www.vice.com/en_us/article/kb7zxa/a-new-number-format-for-computers-could-nuke-approximation-errors-for-good
Michael Byrne,"It's a funny time for software testing. As more and more software is replaced by web applications—the cloud, that is—software bugs have more and more come to mean security holes. That is, interacting with software now so often means exposing data, which means trusting the builders of said software to entirely new degrees. And, as builders, we really need to not fuck that up. Software testing—or debugging—is intense, tedious, and imperfect. Hence, software is full of bugs. Hence, software producers offer sometimes very large cash bounties to people that can find those bugs. A funny time. Computer scientists from MIT have developed a new automated tool that can quickly comb through many thousands of lines of code written using the popular web framework Ruby on Rails looking for security vulnerabilities. In testing 50 popular RoR web applications, the tool, which will be presented at the International Conference on Software Engineering in May and is known simply as Space, was able to come up with 23 previously undiagnosed vulnerabilities. The longest it took to debug any program was 64 seconds. And, as someone that does software testing on a semi-regular basis, I can say that 64 seconds essentially translates to 0 seconds. Performing a static analysis of code—where it's analyzed and inspected without actually running the program—at any kind of scale is a complicated, time-consuming ordeal. Things get even more difficult when we start talking about contemporary web applications because so much of the code behind them is pulled in from external libraries and frameworks. This was the problem faced by the MIT group: Even very simple functionality in Ruby on Rails applications, like assigning values to variables, tends to be defined in often-vast external libraries. When all of these external resources are drawn in, the resulting pile of code gets to be very large. ""The program under analysis is just huge,"" explains MIT computer science and engineering professor Daniel Jackson in a statement . ""Even if you wrote a small program, it sits atop a vast edifice of libraries and plug-ins and frameworks. So when you look at something like a Web application written in language like Ruby on Rails, if you try to do a conventional static analysis, you typically find yourself mired in this huge bog. And this makes it really infeasible in practice."" To solve the problem, the researchers attacked the RoR libraries themselves. The various operations defined within them were rewritten such that instead of doing actual computational operations, they returned symbolic expressions explaining what exactly those operations do. ""So we didn't revise the old code,"" Joe Near, now a postdoc researcher at the University of California, Berkeley and the lead researcher behind the tool, told me. ""For a subset of the APIs, we threw it out and replaced it. The new versions don't let you actually run the web application; they only let you analyze it."" The effect is that as code is fed into the Ruby on Rails interpreter, that interpreter offers the helpful line-by-line description of the program's functionality in very clear, precise terms. With this in hand, static analysis becomes a much more reasonable task. To make this reasonable task into an automated task, Near looked at the general nature of web applications and the various ways in which they allow users to have access to their data. He came up with seven different methods, and, for each, came up with a model describing what operations a user can perform on data. Using the rewritten libraries, he was able to develop a means of testing to see whether or not a given web app adheres to those models. When an app breaks the rules, there is likely to be a resulting security flaw. Even without complete access to their underlying code, Near was able to analyze 50 web apps using Space. For a programmer familiar with their own code (and with complete access to it, obviously), the tool should be no sweat.",,Vice,In 50 Ruby on Rails web apps it quickly busted 23 previously undiagnosed vulnerabilities. ,New MIT Tool Quickly Roots Out Hidden Web App Security Bugs,https://www.vice.com/en_us/article/kb7zpa/new-mit-tool-quickly-roots-out-lurking-web-app-security-bugs
Michael Byrne,"On Monday, Google and Oracle will officially enter the next stage in a six-year war over Google's usage of the Java programming language in constructing the Android operating system . Here, at the United States District Court in San Francisco, where the case began in 2011, a jury will hear arguments as to whether or not Google's usage of the Java API constitutes fair use. Oracle is seeking a staggering $9.3 billion in damages, or a bit less than twice what it paid in total for Sun Microsystems, the previous owner of the Java name, in 2010. In building Android back in the 2000s, Google used an implementation of the Java language that had not been officially sanctioned/licensed by Sun Microsystems. But Sun was much more chill about this stuff than Oracle is, so it was not such a big deal. Also: Android was not such a big deal at the time, or really a deal at all. Then, Oracle bought Sun and Java just as Android started to blow up into what it's become today. So, noting that the Android Java implementation was not actually licensed and that Google was a fantastically successful, wealthy corporation, Oracle sued. In round one , the same district court that will hear fair use arguments this week found that APIs cannot be copyrighted. Then, in round two, a federal appeals court found that, yes, APIs can totally be copyrighted, but Google's use of Java might be considered fair use. The appeals court then kicked the case back down to the district court to consider the fair use question. Round three. To understand what's going on here, and what the potentially quite grave implications are for software engineering in general, we really need to understand what an API actually is. It's kind of a tricky, albeit ubiquitous, concept that can mean a lot of things at once (or seem to). API stands for Application Programming Interface, which probably doesn't help much. The keyword here is ""interface,"" which we can imagine as an access point or endpoint that we are going to interact with via our code in some way. There are a few general API uses that each dictate what it is generally we're going to be interaction with. The API of Babel More often than not, an API is something that refers to some outside library or framework. By ""outside,"" I mean code that I myself have not written, but nonetheless am able to invoke via an API as if I had. This is an everyday part of programming in most any language. I don't care how P5 goes about calculating my number behind the scenes, just that it does. If, for example, I wanted to make some cool interactive graphics for my webpage, I would likely head over to P5.js, which is a JavaScript library based on the Processing programming language . (Note that p5.js is far from the only toolset for making cool visuals on the web, but it's the one I know pretty well.) Here I can find all sorts of predefined functions—discrete units of code meant to be reused that should ideally perform one and only one specific task—that will make drawing stuff in an HTML document much easier and more intuitive than just writing it all from scratch, which would be a pain in the ass and very error-prone. To illustrate, if I wanted to come up with a random number that fits a normal or Gaussian distribution, I can just refer to the P5 API, which conveniently has a function called randomGaussian(). And here's a key point about APIs: I don't care how P5 goes about calculating my number behind the scenes, just that it does. (You can look at everything that P5 does here . Keep in mind that this is even a fairly small library—which can be a good thing if you happen to be learning it.) randomGaussian() won't tell us how it does its magic for a couple of reasons. First, we don't need to know. We just need to know what it does, not how it does it. Second, the interface has no idea how it does what it does. It's almost certainly possible to get that same functionality through alternative implementations. Actually, I claim that any function written in any programming language can be rewritten (re-implemented) in at least one way, but I can't promise that that one way might not wind up being a really stupid and trivial way. Anyhow, this disconnect between implementation and function is what makes an interface an interface. It's a specification, not an implementation. So, with a standardized interface promising such and such functionality—it's helpful to imagine an interface as a promise of implementability—I can make my own software (write my own code) that will use interfaces/APIs to access that outside functionality. Again, this is just a routine thing in programming. APIs for hardware and data Web APIs are similar but often involve something more than just accessing predefined code. A web API is very often used for accessing the underlying functioning of the website as a whole. Just think of everything that websites do, from social media to stock trading to telling you what to buy. All of this (or most of it) can be accessed via a sort of API known as an endpoint. If I wanted to, say, use Twitter without ever having to see the Twitter homepage, I could do that via the Twitter API and some simple scripting. The API tools Twitter provides makes that really easy to do. For another example, if I wanted to write a web application to display the weather, it would make the most sense to grab the actual weather data from someone in the actual business of generating weather data, like Weather Underground. Weather Underground happens to have an API that I can use in my code to get Weather Underground weather data. I just have to follow the specifications given for how to actually do that. The idea of a specification is maybe the most important thing underlying the whole notion of an API: how to actually use it. APIs also provide access to hardware. You don't even want to know how your computer deals with actual physical hardware, and fortunately you don't have to. For one example, you might have already heard of OpenGL . This is an open-source API/library that provides a set of functions that can be used by developers to interact with a graphics-processing unit for the purpose of hardware-accelerated graphics rendering. OpenGL is very, very widely used. The Java API The Java API is interesting and unique. It's basically just a Library of Babel of different functions, classes, and data structures that the language provides to programmers to use in their code. It's huge . Specifications for all of it can be found in the Java documentation, which lists all of these functions and what they do and how to use them. What it doesn't list is how all of those functions work under the hood—that is, what their implementations are. We can as programmers just use Java's own implementations by referencing the Java core API, but we can also use someone else's implementation as well, or we can even write our own. We just have to make sure that the result meets the same specifications given by the Java API. This is what Google did, roughly: It used an implementation of the Java API not sanctioned by Oracle/Sun. So, the question is whether or not that constitutes copyright infringement. It should be clearer now as to what is at stake: the fundamental divide between interface and implementation. It so happens that this divide is what much of the open-source software world revolves around, the ability to separate functionality from implementation, to build the same things in new and better ways. Watch Google v Oracle closely.",,Vice,"It's a key question on the eve of Google vs. Oracle, round three.",What an API Is and Why It’s Worth Fighting For,https://www.vice.com/en_us/article/3dakwk/oracle-vs-google-what-an-api-is-and-why-its-worth-fighting-for
Samantha Cole,"Apple's AirPods are a tragedy . Ecologically, socially, economically—they're a capitalist disaster (or success story, depending how you look at capitalist endeavors in general). The batteries in the $160 wireless earbuds die within a year and a half, at which point they become useless.  The opposite of Airpods, then, is this extremely punk pair of DIY wireless earbuds that someone on Reddit hacked together using an old pair of wired Apple headphones and some hot glue.  ""I started this project roughly two months ago when my friend got a new pair of AirPods for his birthday and I thought to myself, 'that’s quite a lot of money for something I can make at home,'"" Sam Cashbook, who is 15, told me in a Reddit message.  Cashook started watching videos of people making their own AirPods, but mostly found people chopping the wires off of Apple headphones as a joke. He decided to take his own approach.   He bought a hands-free bone conduction headset from eBay (which transfers the sound vibrations from the bones outside your ear, to the inner ear), and took apart the casing to reveal the electronics. Then, he desoldered the wires from the original speaker in the headset, and connected his old Apple earbud speaker to the headset's printed circuit board.  ""I replaced the battery with something a little bigger and hot glued it all together (definitely not the best approach),"" he said. Voilà, AirPods. Maybe a little uglier, but the headphones work well, he said. The set has buttons for power, pausing music, volume controls and skipping tracks, and the battery is rechargeable.  ""This project was really fun and only cost me around four dollars, and helped me improve my soldering skills for smaller components,"" he said. ""I encourage people to make cool things like this!""  And I, for one, encourage people to hack and repair and repurpose their electronics so they may never again need to buy into a system of wasteful class symbolism and status posturing dictated by the biggest companies in the world. Just be careful not to zap your brain if you decide to try this yourself.",,Vice,A 15-year-old made a working pair of DIY AirPods using an old pair of headphones and a soldering iron.,How To Make Your Own AirPods for $4,https://www.vice.com/en_us/article/bj9y45/how-to-make-your-own-airpods-diy
Michael Byrne,"Until a week ago, I was pretty much buried up to my eyeballs in software testing—bug hunting, really. This was part of coursework for school, but the prey was real enough, with the stalking grounds including a popular open-source database software implementation as well as a janky version of the card game Dominion. In my years of studying computer science, I'm not sure if I've ever been presented with a comparable combination of thrill and drudgery. At the very least, I wound up with a new appreciation for the oft misunderstood realm of software testing. It turns out that testing involves so much more than just being annoying to software engineers—it's an art of intentionally breaking things. Pushing and pushing code until it finally snaps and every fuck-up is revealed in its fucked up glory. It's like being a test pilot, except with URL validators instead of space-planes. That's the thrill. The drudgery is burying yourself in someone else's code that is probably not as readable or well-documented as it should be, and then coming up with clever and exhaustive tests tailored to that code (fortunately many are or can be automated, but even then). I've been looking for a way into writing about software testing (and debugging/bug hunting) for a little while, and this week's announcement that the DoD is now shelling out big bucks in bug bounties seems as good as any. Bug hunting may sound like hacker sorcery, but no not really. It's just another wing of software engineering. Read more: Hack This: What to do when shit just won't work Bugs, especially bugs that wind up in finished software products or web apps, are usually (always?) really hard to find. They may only manifest once in a million runs of some software, or only in the most extreme edge cases, e.g. those situations so unlikely to occur in the execution of some piece of code that they are almost but not quite impossible. This means we might have to run the software a million times or more to catch it. But, if something bad is going to happen only once in a million runs, why are we even bothering? Because once in a million is in many cases a near-promise that something bad is in fact going to happen, maybe even many times. If our software is running, say, on one of the many, many computers under the hood of a modern car, that one in a million might be an airbag randomly deploying on a driver as they navigate rush-hour traffic. So, we have no choice but to look carefully and exhaustively at that one event. Maybe you can see how this quickly gets tedious. We start simply. The code is tested a few times with hand-picked inputs, just to see what happens. Probably, it executes as it should. This is called manual testing. Next, things escalate as we go from manually testing some code to writing our own automated tests. Now it gets tricky. Our task is to provide as much coverage as possible in our tests, which means that we need to (ideally) ensure that every line of code in the program is executed. This means following the program through its every possible branch and deviation and error condition, however unlikely it is that such an execution will happen IRL. Once in a million is in many cases a near-promise that something bad is in fact going to happen, maybe even many times. To do this, we need to be very careful about what inputs we give the program. If a section of code can conceivably execute, then we need to figure out how to ensure that the program actually reaches that point. An interesting thing is that it might turn out that a particular piece of code within a program will never run, no matter what we do. It just exists as a weird little bubble that maybe the software developer never go around to implementing or is maybe an artifact from some earlier version of the software that's no longer needed. It happens. As tedious as it can be, an automated testing suite can also be a highly satisfying thing. The point is basically the redline the program—or a discrete subdivision of the program, as in unit testing—and it feels kind of good just starting at the screen and knowing that our tests are at that moment ripping through it at a rate of maybe billions of instructions per second. A short while later and we'll have some results. Did it fail? Where? Why? A branch of automated testing is randomized testing, and this is where things get intense. Randomization is how we can really dig into the musty corners of some code. We can make up inputs ourselves forever, but when dealing with millions or more possible combinations, it probably makes more sense to just build a software tester that can itself generate random inputs across the entire allowed ranges of those inputs (and beyond, actually, because we have to make sure the program fails correctly on those bad inputs, right?). As an example, I recently had to figure out how to randomly test a function implemented in Java that is used to determine whether or not a URL is valid or not. The code doesn't just get to, like, paste it into Chrome—instead, the function breaks the URL up into its different constituent parts and then compares each one of those parts against valid patterns, which are themselves derived from the actual URL definition given by the Internet Engineering Task Force circa 1994. The problem was in coming up with not just random URLs, but also random URLs that I knew beforehand whether or not they were valid. My solution—or partial solution, really—was to just go out and grab a few thousand random but valid URLs from the internet, which I did using a Python script. I then came up with a few different ways of introducing random things into those URLs that would make them invalid, such as inserting a disallowed character or putting an extra slash where it wasn't supposed to be. So, the random tester created an input set by mixing valid URLs with URLs that it had purposefully, randomly fucked up. Crucially, the tester knew the difference beforehand so it could compare this prior knowledge against the URL checker's output. What I described above is more a canonical view of software testing than what bug bounty hunters are up to— ""Bug Bounties 101"" seems like a good place to start for that—but I wanted to give a sense of the problem, and most especially, what makes it an interesting and very hard problem. I'm not exactly debugging fly-by-wire systems on jet aircraft, but a boy can dream.",,Vice,There's a reason corporations pay big money for bugs.,The Agony and the Ecstasy of Debugging Code,https://www.vice.com/en_us/article/gv5vjq/hunting-bugs-junit-debugging-software-testing
